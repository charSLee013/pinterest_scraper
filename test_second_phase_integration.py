#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç¬¬äºŒé˜¶æ®µGraphQLé€»è¾‘é›†æˆæµ‹è¯•
æµ‹è¯•å®Œæ•´çš„ç¬¬äºŒé˜¶æ®µå·¥ä½œæµç¨‹ï¼šæ•°æ®åº“è¯»å– â†’ GraphQLé‡‡é›† â†’ æ•°æ®åº“å­˜å‚¨
"""

import sys
import os
import asyncio
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.smart_scraper import SmartScraper
from src.core.database.repository import SQLiteRepository
from loguru import logger
import uuid

async def test_complete_second_phase():
    """æµ‹è¯•å®Œæ•´çš„ç¬¬äºŒé˜¶æ®µå·¥ä½œæµç¨‹"""
    logger.info("=== æµ‹è¯•å®Œæ•´ç¬¬äºŒé˜¶æ®µå·¥ä½œæµç¨‹ ===")
    
    repo = SQLiteRepository(keyword="cat", output_dir="output")
    
    # 1. ä»æ•°æ®åº“è·å–çœŸå®çš„Pinï¼ˆæ’é™¤æµ‹è¯•æ•°æ®ï¼‰
    try:
        import sqlite3
        conn = sqlite3.connect("output/cat/pinterest.db")
        cursor = conn.cursor()
        
        cursor.execute("""
            SELECT id FROM pins 
            WHERE id NOT LIKE 'test_%'
               AND id NOT LIKE 'expansion_%'
               AND id NOT LIKE 'method_%'
               AND id NOT LIKE 'related_%'
               AND LENGTH(id) > 10
            ORDER BY created_at ASC 
            LIMIT 3
        """)
        
        results = cursor.fetchall()
        conn.close()
        
        if not results:
            logger.error("æ²¡æœ‰æ‰¾åˆ°çœŸå®çš„Pinæ•°æ®")
            return False
        
        pin_ids = [row[0] for row in results]
        logger.info(f"æ‰¾åˆ° {len(pin_ids)} ä¸ªçœŸå®Pin")
        
        # 2. åˆ›å»ºpin_setï¼ˆæ¨¡æ‹Ÿç¬¬äºŒé˜¶æ®µçš„æ•°æ®åº“é©±åŠ¨é€»è¾‘ï¼‰
        pin_set = set(pin_ids)
        logger.info(f"åˆå§‹pin_setå¤§å°: {len(pin_set)}")
        
        # 3. ä½¿ç”¨SmartScraperæµ‹è¯•ç¬¬äºŒé˜¶æ®µé€»è¾‘
        scraper = SmartScraper(debug=False)  # ç”Ÿäº§æ¨¡å¼
        session_id = f"integration_test_{uuid.uuid4().hex[:8]}"
        
        try:
            total_new_pins = 0
            total_rounds = 0
            
            # æ¨¡æ‹Ÿç¬¬äºŒé˜¶æ®µæ‰©å±•å¾ªç¯
            for round_num in range(2):  # æµ‹è¯•2è½®
                if not pin_set:
                    logger.info("pin_setä¸ºç©ºï¼Œåœæ­¢å¾ªç¯")
                    break
                
                # ä»setä¸­popä¸€ä¸ªpin
                pin_id = pin_set.pop()
                total_rounds += 1
                logger.info(f"ç¬¬{round_num + 1}è½®: å¤„ç†Pin {pin_id}")
                
                # ä½¿ç”¨GraphQLé€»è¾‘é‡‡é›†ç›¸å…³pins
                related_pins = await scraper._scrape_pin_detail_with_queue(pin_id, max_count=5)
                
                if related_pins:
                    logger.info(f"âœ… è·å–åˆ° {len(related_pins)} ä¸ªç›¸å…³Pin")
                    
                    # æ˜¾ç¤ºå‰2ä¸ªç›¸å…³Pinçš„ä¿¡æ¯
                    for i, pin in enumerate(related_pins[:2], 1):
                        logger.info(f"ç›¸å…³Pin {i}:")
                        logger.info(f"  ID: {pin.get('id', 'N/A')}")
                        logger.info(f"  æ ‡é¢˜: {pin.get('title', 'N/A')[:50]}...")
                        logger.info(f"  æœ‰å›¾ç‰‡URL: {bool(pin.get('largest_image_url'))}")
                    
                    # è¦†ç›–å¼æ‰¹é‡å­˜å‚¨åˆ°æ•°æ®åº“ï¼Œè·å–æ–°å¢pin id
                    new_pin_ids = repo.save_pins_and_get_new_ids(related_pins, "cat", session_id)
                    
                    if new_pin_ids:
                        # å°†æ–°pin idåŠ å…¥set
                        pin_set.update(new_pin_ids)
                        total_new_pins += len(new_pin_ids)
                        logger.info(f"æ•°æ®åº“æ–°å¢ {len(new_pin_ids)} ä¸ªPin")
                        logger.info(f"å½“å‰pin_setå¤§å°: {len(pin_set)}")
                        logger.info(f"æ–°å¢Pin IDsç¤ºä¾‹: {new_pin_ids[:2]}...")
                    else:
                        logger.info("æ‰€æœ‰ç›¸å…³Pinéƒ½æ˜¯é‡å¤çš„")
                else:
                    logger.warning("æœªè·å–åˆ°ç›¸å…³Pin")
            
            logger.info(f"\n=== ç¬¬äºŒé˜¶æ®µé›†æˆæµ‹è¯•æ€»ç»“ ===")
            logger.info(f"å¤„ç†è½®æ•°: {total_rounds}")
            logger.info(f"æœ€ç»ˆpin_setå¤§å°: {len(pin_set)}")
            logger.info(f"æ€»æ–°å¢Pinæ•°é‡: {total_new_pins}")
            
            if total_new_pins > 0:
                logger.info("âœ… ç¬¬äºŒé˜¶æ®µé›†æˆæµ‹è¯•æˆåŠŸï¼")
                return True
            else:
                logger.warning("âš ï¸ æœªè·å¾—æ–°å¢Pin")
                return False
                
        finally:
            await scraper.close()
            
    except Exception as e:
        logger.error(f"é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_database_integration():
    """æµ‹è¯•æ•°æ®åº“é›†æˆåŠŸèƒ½"""
    logger.info("\n=== æµ‹è¯•æ•°æ®åº“é›†æˆåŠŸèƒ½ ===")
    
    repo = SQLiteRepository(keyword="cat", output_dir="output")
    
    # æµ‹è¯•load_pins_with_imagesæ–¹æ³•
    pins_with_images = repo.load_pins_with_images("cat", limit=5)
    logger.info(f"æ•°æ®åº“ä¸­æœ‰å›¾ç‰‡çš„Pinæ•°é‡: {len(pins_with_images)}")
    
    if pins_with_images:
        logger.info("âœ… load_pins_with_imagesæ–¹æ³•æ­£å¸¸")
        
        # æµ‹è¯•save_pins_and_get_new_idsæ–¹æ³•
        test_pins = [
            {
                'id': f'db_integration_test_{uuid.uuid4().hex[:8]}',
                'title': 'æ•°æ®åº“é›†æˆæµ‹è¯•Pin',
                'description': 'æµ‹è¯•save_pins_and_get_new_idsæ–¹æ³•',
                'largest_image_url': 'https://example.com/integration_test.jpg',
                'image_urls': {'original': 'integration_test_url'}
            }
        ]
        
        session_id = f"db_integration_{uuid.uuid4().hex[:8]}"
        new_ids = repo.save_pins_and_get_new_ids(test_pins, "cat", session_id)
        
        if new_ids:
            logger.info(f"âœ… save_pins_and_get_new_idsæ–¹æ³•æ­£å¸¸ï¼Œæ–°å¢: {new_ids}")
            return True
        else:
            logger.info("â„¹ï¸ Pinå·²å­˜åœ¨ï¼Œå»é‡æ­£å¸¸")
            return True
    else:
        logger.warning("âš ï¸ æ•°æ®åº“ä¸­æ²¡æœ‰æœ‰å›¾ç‰‡çš„Pin")
        return False

async def test_graphql_data_extraction():
    """æµ‹è¯•GraphQLæ•°æ®æå–åŠŸèƒ½"""
    logger.info("\n=== æµ‹è¯•GraphQLæ•°æ®æå–åŠŸèƒ½ ===")
    
    # ä½¿ç”¨æˆåŠŸçš„ç¤ºä¾‹Pin ID
    test_pin_id = "801077852519350337"
    
    scraper = SmartScraper(debug=False)
    
    try:
        # æµ‹è¯•å•ä¸ªPinçš„GraphQLæ•°æ®æå–
        related_pins = await scraper._scrape_pin_detail_with_queue(test_pin_id, max_count=3)
        
        if related_pins:
            logger.info(f"âœ… GraphQLæ•°æ®æå–æˆåŠŸï¼Œè·å¾— {len(related_pins)} ä¸ªPin")
            
            # éªŒè¯æ•°æ®ç»“æ„
            for i, pin in enumerate(related_pins[:2], 1):
                pin_id = pin.get('id', '')
                title = pin.get('title', '')
                image_urls = pin.get('image_urls', {})
                largest_image = pin.get('largest_image_url', '')
                
                logger.info(f"Pin {i}:")
                logger.info(f"  ID: {pin_id}")
                logger.info(f"  æ ‡é¢˜: {title[:30]}...")
                logger.info(f"  å›¾ç‰‡URLæ•°é‡: {len(image_urls)}")
                logger.info(f"  æœ‰æœ€å¤§å›¾ç‰‡: {bool(largest_image)}")
                
                # éªŒè¯å¿…è¦å­—æ®µ
                if not pin_id:
                    logger.error(f"Pin {i} ç¼ºå°‘IDå­—æ®µ")
                    return False
                if not image_urls:
                    logger.error(f"Pin {i} ç¼ºå°‘å›¾ç‰‡URL")
                    return False
            
            logger.info("âœ… GraphQLæ•°æ®ç»“æ„éªŒè¯é€šè¿‡")
            return True
        else:
            logger.warning("âš ï¸ GraphQLæ•°æ®æå–å¤±è´¥")
            return False
            
    finally:
        await scraper.close()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # æµ‹è¯•1ï¼šæ•°æ®åº“é›†æˆåŠŸèƒ½
        db_success = await test_database_integration()
        
        # æµ‹è¯•2ï¼šGraphQLæ•°æ®æå–åŠŸèƒ½
        graphql_success = await test_graphql_data_extraction()
        
        # æµ‹è¯•3ï¼šå®Œæ•´çš„ç¬¬äºŒé˜¶æ®µå·¥ä½œæµç¨‹
        integration_success = await test_complete_second_phase()
        
        logger.info(f"\nğŸ‰ ç¬¬äºŒé˜¶æ®µé›†æˆæµ‹è¯•å®Œæˆ")
        logger.info(f"æ•°æ®åº“é›†æˆ: {'âœ… æˆåŠŸ' if db_success else 'âŒ å¤±è´¥'}")
        logger.info(f"GraphQLæ•°æ®æå–: {'âœ… æˆåŠŸ' if graphql_success else 'âŒ å¤±è´¥'}")
        logger.info(f"å®Œæ•´å·¥ä½œæµç¨‹: {'âœ… æˆåŠŸ' if integration_success else 'âŒ å¤±è´¥'}")
        
        if db_success and graphql_success and integration_success:
            logger.info("ğŸ¯ æ‰€æœ‰ç¬¬äºŒé˜¶æ®µé›†æˆæµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return False
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
