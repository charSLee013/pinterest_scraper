#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•
æµ‹è¯•ç¬¬ä¸€é˜¶æ®µ + ç¬¬äºŒé˜¶æ®µçš„å®Œæ•´é‡‡é›†æµç¨‹
"""

import sys
import os
import asyncio
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.core.smart_scraper import SmartScraper
from src.core.database.repository import SQLiteRepository
from loguru import logger
import uuid

async def test_full_scraping_workflow():
    """æµ‹è¯•å®Œæ•´çš„é‡‡é›†å·¥ä½œæµç¨‹"""
    logger.info("=== æµ‹è¯•å®Œæ•´é‡‡é›†å·¥ä½œæµç¨‹ ===")
    
    # ä½¿ç”¨ç‹¬ç«‹çš„æµ‹è¯•å…³é”®è¯é¿å…å¹²æ‰°
    test_keyword = f"test_workflow_{uuid.uuid4().hex[:8]}"
    repo = SQLiteRepository(keyword=test_keyword, output_dir="output")
    session_id = f"workflow_test_{uuid.uuid4().hex[:8]}"
    
    scraper = SmartScraper(
        repository=repo,
        session_id=session_id,
        debug=False
    )
    
    try:
        logger.info("å¼€å§‹å®Œæ•´é‡‡é›†æµç¨‹æµ‹è¯•...")
        
        # æ‰§è¡Œå®Œæ•´çš„é‡‡é›†æµç¨‹ï¼ˆåŒ…å«ç¬¬ä¸€é˜¶æ®µå’Œç¬¬äºŒé˜¶æ®µï¼‰
        pins = await scraper.scrape(
            query=test_keyword,
            target_count=15,  # å°æ•°é‡æµ‹è¯•
            repository=repo,
            session_id=session_id
        )
        
        logger.info(f"é‡‡é›†å®Œæˆï¼Œè·å¾— {len(pins)} ä¸ªPin")
        
        if pins:
            logger.info("âœ… å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ")
            
            # éªŒè¯æ•°æ®åº“ä¸­çš„æ•°æ®
            db_pins = repo.load_pins_with_images(test_keyword, limit=20)
            logger.info(f"æ•°æ®åº“ä¸­æœ‰å›¾ç‰‡çš„Pinæ•°é‡: {len(db_pins)}")
            
            # æ˜¾ç¤ºå‰3ä¸ªPinçš„ä¿¡æ¯
            for i, pin in enumerate(pins[:3], 1):
                logger.info(f"Pin {i}:")
                logger.info(f"  ID: {pin.get('id', 'N/A')}")
                logger.info(f"  æ ‡é¢˜: {pin.get('title', 'N/A')[:50]}...")
                logger.info(f"  æœ‰å›¾ç‰‡URL: {bool(pin.get('largest_image_url'))}")
            
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = scraper.get_stats()
            logger.info(f"é‡‡é›†ç»Ÿè®¡: {stats}")
            
            return True
        else:
            logger.warning("âš ï¸ æœªè·å–åˆ°Pinæ•°æ®")
            return False
            
    except Exception as e:
        logger.error(f"å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    finally:
        await scraper.close()

async def test_second_phase_expansion():
    """ä¸“é—¨æµ‹è¯•ç¬¬äºŒé˜¶æ®µæ‰©å±•é€»è¾‘"""
    logger.info("\n=== æµ‹è¯•ç¬¬äºŒé˜¶æ®µæ‰©å±•é€»è¾‘ ===")
    
    repo = SQLiteRepository(keyword="cat", output_dir="output")
    
    # è·å–æ•°æ®åº“ä¸­çš„çœŸå®Pin
    pins_with_images = repo.load_pins_with_images("cat", limit=2)
    
    if not pins_with_images:
        logger.warning("æ•°æ®åº“ä¸­æ²¡æœ‰æœ‰å›¾ç‰‡çš„Pinï¼Œè·³è¿‡ç¬¬äºŒé˜¶æ®µæµ‹è¯•")
        return True
    
    logger.info(f"ä»æ•°æ®åº“è·å–åˆ° {len(pins_with_images)} ä¸ªæœ‰å›¾ç‰‡çš„Pin")
    
    # åˆ›å»ºpin_set
    pin_set = set([pin['id'] for pin in pins_with_images])
    original_size = len(pin_set)
    
    scraper = SmartScraper(debug=False)
    session_id = f"expansion_test_{uuid.uuid4().hex[:8]}"
    
    try:
        expansion_successful = False
        
        # æµ‹è¯•æ‰©å±•å¾ªç¯
        for round_num in range(1):  # åªæµ‹è¯•1è½®
            if not pin_set:
                break
            
            pin_id = pin_set.pop()
            logger.info(f"æµ‹è¯•æ‰©å±•Pin: {pin_id}")
            
            # è·å–ç›¸å…³pins
            related_pins = await scraper._scrape_pin_detail_with_queue(pin_id, max_count=3)
            
            if related_pins:
                logger.info(f"è·å–åˆ° {len(related_pins)} ä¸ªç›¸å…³Pin")
                
                # ä¿å­˜å¹¶è·å–æ–°å¢pin id
                new_pin_ids = repo.save_pins_and_get_new_ids(related_pins, "cat", session_id)
                
                if new_pin_ids:
                    pin_set.update(new_pin_ids)
                    expansion_successful = True
                    logger.info(f"æˆåŠŸæ‰©å±• {len(new_pin_ids)} ä¸ªæ–°Pin")
                    logger.info(f"pin_setä» {original_size} æ‰©å±•åˆ° {len(pin_set)}")
                    break
        
        if expansion_successful:
            logger.info("âœ… ç¬¬äºŒé˜¶æ®µæ‰©å±•é€»è¾‘æµ‹è¯•æˆåŠŸ")
            return True
        else:
            logger.info("â„¹ï¸ ç¬¬äºŒé˜¶æ®µæ‰©å±•æœªè·å¾—æ–°Pinï¼ˆå¯èƒ½éƒ½æ˜¯é‡å¤çš„ï¼‰")
            return True  # è¿™ä¹Ÿæ˜¯æ­£å¸¸æƒ…å†µ
            
    finally:
        await scraper.close()

async def test_interrupt_and_resume():
    """æµ‹è¯•ä¸­æ–­å’Œæ¢å¤æœºåˆ¶"""
    logger.info("\n=== æµ‹è¯•ä¸­æ–­å’Œæ¢å¤æœºåˆ¶ ===")
    
    test_keyword = f"interrupt_test_{uuid.uuid4().hex[:8]}"
    repo = SQLiteRepository(keyword=test_keyword, output_dir="output")
    session_id = f"interrupt_test_{uuid.uuid4().hex[:8]}"
    
    scraper = SmartScraper(
        repository=repo,
        session_id=session_id,
        debug=False
    )
    
    try:
        # æ¨¡æ‹Ÿä¸­æ–­ï¼ˆè®¾ç½®è¾ƒå°çš„ç›®æ ‡æ•°é‡ï¼‰
        logger.info("å¼€å§‹é‡‡é›†ï¼ˆæ¨¡æ‹Ÿä¸­æ–­ï¼‰...")
        
        # å¯åŠ¨é‡‡é›†ä»»åŠ¡
        task = asyncio.create_task(scraper.scrape(
            query=test_keyword,
            target_count=5,
            repository=repo,
            session_id=session_id
        ))
        
        # ç­‰å¾…ä¸€æ®µæ—¶é—´åä¸­æ–­
        await asyncio.sleep(10)
        scraper.request_interrupt()
        
        try:
            pins = await task
            logger.info(f"ä¸­æ–­åè·å¾— {len(pins)} ä¸ªPin")
        except asyncio.CancelledError:
            logger.info("ä»»åŠ¡è¢«å–æ¶ˆ")
        
        # æ£€æŸ¥æ•°æ®åº“ä¸­çš„æ•°æ®ï¼ˆåº”è¯¥æœ‰éƒ¨åˆ†æ•°æ®è¢«ä¿å­˜ï¼‰
        db_pins = repo.load_pins_with_images(test_keyword, limit=10)
        logger.info(f"ä¸­æ–­åæ•°æ®åº“ä¸­ä¿å­˜äº† {len(db_pins)} ä¸ªPin")
        
        if len(db_pins) > 0:
            logger.info("âœ… ä¸­æ–­ä¿æŠ¤æœºåˆ¶æ­£å¸¸å·¥ä½œ")
            return True
        else:
            logger.warning("âš ï¸ ä¸­æ–­åæ•°æ®åº“ä¸­æ²¡æœ‰æ•°æ®")
            return False
            
    except Exception as e:
        logger.error(f"ä¸­æ–­æµ‹è¯•å¤±è´¥: {e}")
        return False
    
    finally:
        await scraper.close()

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    try:
        # æµ‹è¯•1ï¼šç¬¬äºŒé˜¶æ®µæ‰©å±•é€»è¾‘
        expansion_success = await test_second_phase_expansion()
        
        # æµ‹è¯•2ï¼šå®Œæ•´å·¥ä½œæµç¨‹
        workflow_success = await test_full_scraping_workflow()
        
        # æµ‹è¯•3ï¼šä¸­æ–­å’Œæ¢å¤æœºåˆ¶
        interrupt_success = await test_interrupt_and_resume()
        
        logger.info(f"\nğŸ‰ å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆ")
        logger.info(f"ç¬¬äºŒé˜¶æ®µæ‰©å±•: {'âœ… æˆåŠŸ' if expansion_success else 'âŒ å¤±è´¥'}")
        logger.info(f"å®Œæ•´å·¥ä½œæµç¨‹: {'âœ… æˆåŠŸ' if workflow_success else 'âŒ å¤±è´¥'}")
        logger.info(f"ä¸­æ–­æ¢å¤æœºåˆ¶: {'âœ… æˆåŠŸ' if interrupt_success else 'âŒ å¤±è´¥'}")
        
        if expansion_success and workflow_success and interrupt_success:
            logger.info("ğŸ¯ æ‰€æœ‰å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•é€šè¿‡ï¼")
            return True
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return False
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
