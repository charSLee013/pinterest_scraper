#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
é‡æ„åçš„--only-imageså·¥ä½œæµç¨‹

å®ç°å®Œå…¨ç‹¬ç«‹çš„å››é˜¶æ®µå¤„ç†ï¼š
1. æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹é˜¶æ®µï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤æŸåçš„æ•°æ®åº“æ–‡ä»¶
2. Base64ç¼–ç Pinè½¬æ¢é˜¶æ®µï¼šå°†base64ç¼–ç è½¬æ¢ä¸ºçœŸå®Pin ID
3. Pinè¯¦æƒ…æ•°æ®è¡¥å…¨é˜¶æ®µï¼šæ‰¹é‡è·å–ç¼ºå¤±çš„Pinè¯¦æƒ…ä¿¡æ¯
4. å›¾ç‰‡æ–‡ä»¶ä¸‹è½½é˜¶æ®µï¼šå¹¶å‘ä¸‹è½½ç¼ºå¤±çš„å›¾ç‰‡æ–‡ä»¶

æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥ç®¡ç†å’Œä¼˜é›…é€€å‡ºæœºåˆ¶ã€‚
"""

import os
import json
import time
from typing import Optional, Dict
from loguru import logger

from .stage_manager import WorkflowManager
from .stage_implementations import (
    DatabaseRepairStage,
    Base64ConversionStage,
    PinEnhancementStage,
    ImageDownloadStage
)


class OptimizedOnlyImagesWorkflow:
    """é‡æ„åçš„--only-imageså·¥ä½œæµç¨‹

    å®ç°å®Œå…¨ç‹¬ç«‹çš„å››é˜¶æ®µå¤„ç†é€»è¾‘ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µéƒ½æœ‰ç‹¬ç«‹çš„è¿æ¥ç®¡ç†å’Œä¼˜é›…é€€å‡ºæœºåˆ¶
    """

    def __init__(self, output_dir: str, max_concurrent: int = 15, proxy: Optional[str] = None):
        """åˆå§‹åŒ–å·¥ä½œæµç¨‹

        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            proxy: ä»£ç†è®¾ç½®
        """
        self.output_dir = output_dir
        self.max_concurrent = max_concurrent
        self.proxy = proxy

        # åˆ›å»ºå·¥ä½œæµç¨‹ç®¡ç†å™¨
        self.workflow_manager = WorkflowManager(output_dir)

        logger.info(f"ğŸš€ åˆå§‹åŒ–é‡æ„åçš„--only-imageså·¥ä½œæµç¨‹")
        logger.info(f"   - è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"   - æœ€å¤§å¹¶å‘: {max_concurrent}")
        logger.info(f"   - ä»£ç†è®¾ç½®: {proxy or 'æ— '}")
        logger.info(f"   - å››é˜¶æ®µç‹¬ç«‹å¤„ç†æ¨¡å¼")

        # å·¥ä½œæµç¨‹ç»Ÿè®¡
        self.workflow_stats = {
            "stage1_database_repair": {},
            "stage2_base64_conversion": {},
            "stage3_pin_enhancement": {},
            "stage4_image_download": {},
            "total_execution_time": 0
        }
    
    async def execute(self, target_keyword: Optional[str] = None) -> Dict:
        """æ‰§è¡Œä¼˜åŒ–åçš„ä¸‰é˜¶æ®µå·¥ä½œæµç¨‹
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å…³é”®è¯
            
        Returns:
            å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ
        """
        import time
        start_time = time.time()
        
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹")
        logger.info("=" * 60)
        
        try:
            # Phase 1: å®æ—¶Base64è½¬æ¢
            phase1_success = await self._execute_phase1_base64_conversion(target_keyword)
            if not phase1_success:
                logger.error("âŒ Phase 1 å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµç¨‹")
                return self._generate_failure_result("Phase 1 å¤±è´¥")
            
            # Phase 2: å…¨å±€Headerå‡†å¤‡
            phase2_success = await self._execute_phase2_header_preparation()
            if not phase2_success:
                logger.warning("âš ï¸ Phase 2 å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headersç»§ç»­")
            
            # Phase 3: æ™ºèƒ½ä¸‹è½½
            phase3_success = await self._execute_phase3_smart_download(target_keyword)
            if not phase3_success:
                logger.error("âŒ Phase 3 å¤±è´¥")
                return self._generate_failure_result("Phase 3 å¤±è´¥")
            
            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            self.workflow_stats["total_execution_time"] = time.time() - start_time
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
            
            return self._generate_success_result()
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return self._generate_failure_result(str(e))
    
    async def _execute_phase1_base64_conversion(self, target_keyword: Optional[str]) -> bool:
        """æ‰§è¡ŒPhase 1: å®æ—¶Base64è½¬æ¢
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯
            
        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸ”„ Phase 1: å®æ—¶Base64è½¬æ¢é˜¶æ®µ")
        logger.info("- é€ä¸ªæ£€æŸ¥æ•°æ®åº“ä¸­çš„base64ç¼–ç Pin")
        logger.info("- æ¯ä¸ªè½¬æ¢éƒ½æ˜¯åŸå­äº‹åŠ¡")
        logger.info("- æŒç»­å¤„ç†ç›´åˆ°æ²¡æœ‰base64ç¼–ç Pin")
        
        try:
            conversion_stats = await self.base64_converter.process_all_databases(target_keyword)
            self.workflow_stats["phase1_base64_conversion"] = conversion_stats
            
            if conversion_stats["total_converted"] > 0:
                logger.info(f"âœ… Phase 1 å®Œæˆ: è½¬æ¢äº† {conversion_stats['total_converted']} ä¸ªbase64ç¼–ç Pin")
            else:
                logger.info("âœ… Phase 1 å®Œæˆ: æ²¡æœ‰å‘ç°éœ€è¦è½¬æ¢çš„base64ç¼–ç Pin")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Phase 1 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    async def _execute_phase2_header_preparation(self) -> bool:
        """æ‰§è¡ŒPhase 2: å…¨å±€Headerå‡†å¤‡

        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸŒ Phase 2: å…¨å±€Headerå‡†å¤‡é˜¶æ®µ")
        logger.info("- å¯åŠ¨æµè§ˆå™¨è®¿é—®Pinterestå®˜ç½‘")
        logger.info("- è·å–å¹¶ç¼“å­˜æµè§ˆå™¨headers")
        logger.info("- ä¸ºæ•´ä¸ªä¼šè¯æä¾›å…¨å±€headers")

        try:
            # å¼ºåˆ¶è·å–æ–°çš„headers
            logger.info("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨è·å–å…¨å±€Headers...")
            headers_ready = await self.header_manager.ensure_headers_ready()

            header_info = self.header_manager.get_headers_info()
            self.workflow_stats["phase2_header_preparation"] = header_info

            if headers_ready:
                headers = self.header_manager.get_headers()
                logger.info(f"âœ… Phase 2 å®Œæˆ: Headerså‡†å¤‡å°±ç»ª")
                logger.info(f"   - Headersæ•°é‡: {len(headers)} ä¸ªå­—æ®µ")
                logger.info(f"   - åŒ…å«User-Agent: {'User-Agent' in headers}")
                logger.info(f"   - åŒ…å«Cookie: {'Cookie' in headers}")
                return True
            else:
                logger.warning("âš ï¸ Phase 2 å¤±è´¥: Headerså‡†å¤‡å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headers")
                return False

        except Exception as e:
            logger.error(f"âŒ Phase 2 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    async def _execute_phase3_smart_download(self, target_keyword: Optional[str]) -> bool:
        """æ‰§è¡ŒPhase 3: æ™ºèƒ½ä¸‹è½½
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯
            
        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸ“¥ Phase 3: æ™ºèƒ½ä¸‹è½½é˜¶æ®µ")
        logger.info("- æ£€æŸ¥æ¯ä¸ªPinæ˜¯å¦ç¼ºå°‘å›¾ç‰‡URL")
        logger.info("- æŒ‰éœ€è·å–Pinè¯¦æƒ…æ•°æ®")
        logger.info("- ç«‹å³æ›´æ–°æ•°æ®åº“åè¿›è¡Œå›¾ç‰‡ä¸‹è½½")
        logger.info("- ä½¿ç”¨å…¨å±€ç¼“å­˜çš„headers")
        
        try:
            # åˆ›å»ºæ™ºèƒ½å›¾ç‰‡ä¸‹è½½å™¨
            smart_downloader = SmartImageDownloader(
                output_dir=self.output_dir,
                max_concurrent=self.max_concurrent,
                proxy=self.proxy,
                header_manager=self.header_manager,
                pin_enhancer=self.pin_enhancer
            )
            
            # æ‰§è¡Œæ™ºèƒ½ä¸‹è½½
            if target_keyword:
                download_stats = await smart_downloader.download_keyword_images(target_keyword)
            else:
                download_stats = await smart_downloader.download_all_images()
            
            self.workflow_stats["phase3_smart_download"] = download_stats
            
            logger.info(f"âœ… Phase 3 å®Œæˆ: {download_stats}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Phase 3 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    def _generate_success_result(self) -> Dict:
        """ç”ŸæˆæˆåŠŸç»“æœ
        
        Returns:
            æˆåŠŸç»“æœå­—å…¸
        """
        return {
            "status": "success",
            "message": "ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ",
            "stats": self.workflow_stats,
            "phases": {
                "phase1_base64_conversion": "completed",
                "phase2_header_preparation": "completed",
                "phase3_smart_download": "completed"
            }
        }
    
    def _generate_failure_result(self, error_message: str) -> Dict:
        """ç”Ÿæˆå¤±è´¥ç»“æœ
        
        Args:
            error_message: é”™è¯¯æ¶ˆæ¯
            
        Returns:
            å¤±è´¥ç»“æœå­—å…¸
        """
        return {
            "status": "failed",
            "message": f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {error_message}",
            "stats": self.workflow_stats,
            "error": error_message
        }


class SmartImageDownloader:
    """æ™ºèƒ½å›¾ç‰‡ä¸‹è½½å™¨
    
    é›†æˆPinå¢å¼ºåŠŸèƒ½çš„å›¾ç‰‡ä¸‹è½½å™¨
    """
    
    def __init__(self, output_dir: str, max_concurrent: int, proxy: Optional[str], 
                 header_manager: GlobalHeaderManager, pin_enhancer: SmartPinEnhancer):
        """åˆå§‹åŒ–æ™ºèƒ½ä¸‹è½½å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            proxy: ä»£ç†è®¾ç½®
            header_manager: Headerç®¡ç†å™¨
            pin_enhancer: Pinå¢å¼ºå™¨
        """
        self.output_dir = output_dir
        self.max_concurrent = max_concurrent
        self.proxy = proxy
        self.header_manager = header_manager
        self.pin_enhancer = pin_enhancer
        
        # åˆ›å»ºä¼ ç»Ÿçš„å›¾ç‰‡ä¸‹è½½å™¨
        self.image_downloader = ImageDownloader(
            output_dir=output_dir,
            max_concurrent=max_concurrent,
            proxy=proxy,
            prefer_requests=True
        )
    
    async def download_keyword_images(self, keyword: str) -> Dict:
        """ä¸‹è½½æŒ‡å®šå…³é”®è¯çš„å›¾ç‰‡
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ™ºèƒ½ä¸‹è½½å…³é”®è¯: {keyword}")
        
        # ä½¿ç”¨å¢å¼ºçš„ä¸‹è½½é€»è¾‘
        return await self._download_with_enhancement(keyword)
    
    async def download_all_images(self) -> Dict:
        """ä¸‹è½½æ‰€æœ‰å…³é”®è¯çš„å›¾ç‰‡
        
        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("å¼€å§‹æ™ºèƒ½ä¸‹è½½æ‰€æœ‰å…³é”®è¯")
        
        # å‘ç°æ‰€æœ‰å…³é”®è¯
        keywords = self._discover_all_keywords()
        
        total_stats = {
            "keywords": len(keywords),
            "downloaded": 0,
            "failed": 0,
            "enhanced": 0
        }
        
        for keyword in keywords:
            keyword_stats = await self._download_with_enhancement(keyword)
            total_stats["downloaded"] += keyword_stats.get("downloaded", 0)
            total_stats["failed"] += keyword_stats.get("failed", 0)
            total_stats["enhanced"] += keyword_stats.get("enhanced", 0)
        
        return total_stats
    
    async def _download_with_enhancement(self, keyword: str) -> Dict:
        """ä½¿ç”¨å¢å¼ºé€»è¾‘ä¸‹è½½å›¾ç‰‡ - é‡æ„ä¸ºç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„

        Args:
            keyword: å…³é”®è¯

        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ™ºèƒ½ä¸‹è½½å…³é”®è¯: {keyword} (ä½¿ç”¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„)")

        # ä½¿ç”¨æ–°çš„ä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨
        from .pin_processing_queue import TaskQueueManager

        # åˆ›å»ºé˜Ÿåˆ—ç®¡ç†å™¨
        queue_manager = TaskQueueManager(
            max_workers=self.max_concurrent,
            queue_size=200  # é˜Ÿåˆ—å¤§å°ï¼Œé¿å…å†…å­˜çˆ†ç‚¸
        )

        try:
            # å¯åŠ¨ç”Ÿäº§è€…-æ¶ˆè´¹è€…å¤„ç†
            stats = await queue_manager.start_processing(
                keyword=keyword,
                output_dir=self.output_dir,
                pin_enhancer=self.pin_enhancer,
                header_manager=self.header_manager
            )

            # è½¬æ¢ç»Ÿè®¡æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            download_stats = {
                "keyword": keyword,
                "total_pins": stats.get("total_pins", 0),
                "enhanced_pins": stats.get("enhanced_pins", 0),
                "downloaded": stats.get("downloaded_pins", 0),
                "failed": stats.get("failed_pins", 0),
                "processing_time": stats.get("end_time", 0) - stats.get("start_time", 0) if stats.get("end_time") and stats.get("start_time") else 0
            }

            logger.info(f"ç”Ÿäº§è€…-æ¶ˆè´¹è€…å¤„ç†å®Œæˆ: {download_stats}")
            return download_stats

        except Exception as e:
            logger.error(f"ç”Ÿäº§è€…-æ¶ˆè´¹è€…å¤„ç†å¤±è´¥: {e}")
            # è¿”å›é”™è¯¯ç»Ÿè®¡
            return {
                "keyword": keyword,
                "total_pins": 0,
                "enhanced_pins": 0,
                "downloaded": 0,
                "failed": 0,
                "error": str(e)
            }
        finally:
            # ç¡®ä¿èµ„æºæ¸…ç†
            await queue_manager.stop_processing()



    async def _download_pin_images(self, pin: Dict, keyword: str, headers: Dict) -> bool:
        """ä¸‹è½½å•ä¸ªPinçš„å›¾ç‰‡

        Args:
            pin: Pinæ•°æ®
            keyword: å…³é”®è¯
            headers: HTTPè¯·æ±‚å¤´

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            pin_id = pin.get('id', '')
            if not pin_id:
                logger.debug("Pinç¼ºå°‘IDï¼Œè·³è¿‡ä¸‹è½½")
                return False

            # æå–å¯ä¸‹è½½çš„URL
            image_urls = self._extract_downloadable_urls(pin)
            if not image_urls:
                logger.debug(f"Pin {pin_id} æ²¡æœ‰å¯ä¸‹è½½çš„å›¾ç‰‡URL")
                return False

            # åˆ›å»ºå›¾ç‰‡ç›®å½•
            images_dir = os.path.join(self.output_dir, keyword, "images")
            os.makedirs(images_dir, exist_ok=True)

            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_extension = "jpg"  # é»˜è®¤æ‰©å±•å
            filename = f"{pin_id}.{file_extension}"
            output_path = os.path.join(images_dir, filename)

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                logger.debug(f"å›¾ç‰‡å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {pin_id}")
                return True

            # å°è¯•ä¸‹è½½ç¬¬ä¸€ä¸ªå¯ç”¨çš„URL
            for url in image_urls:
                try:
                    success = await self._download_single_image(url, output_path, headers)
                    if success:
                        logger.debug(f"å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {pin_id}")
                        return True
                except Exception as e:
                    logger.debug(f"ä¸‹è½½URLå¤±è´¥: {url[:100]}... é”™è¯¯: {e}")
                    continue

            logger.debug(f"Pin {pin_id} æ‰€æœ‰URLéƒ½ä¸‹è½½å¤±è´¥")
            return False

        except Exception as e:
            logger.error(f"ä¸‹è½½Pinå›¾ç‰‡å¤±è´¥: {pin.get('id', '')}, é”™è¯¯: {e}")
            return False

    async def _download_single_image(self, url: str, output_path: str, headers: Dict) -> bool:
        """ä¸‹è½½å•å¼ å›¾ç‰‡

        Args:
            url: å›¾ç‰‡URL
            output_path: è¾“å‡ºè·¯å¾„
            headers: HTTPè¯·æ±‚å¤´

        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        try:
            timeout = aiohttp.ClientTimeout(total=30)

            async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        # ç¡®ä¿ç›®å½•å­˜åœ¨
                        os.makedirs(os.path.dirname(output_path), exist_ok=True)

                        # å†™å…¥æ–‡ä»¶
                        async with aiofiles.open(output_path, 'wb') as f:
                            async for chunk in response.content.iter_chunked(8192):
                                await f.write(chunk)

                        # éªŒè¯æ–‡ä»¶å¤§å°
                        if os.path.exists(output_path) and os.path.getsize(output_path) > 1000:
                            return True
                        else:
                            # åˆ é™¤æ— æ•ˆæ–‡ä»¶
                            if os.path.exists(output_path):
                                os.remove(output_path)
                            return False
                    else:
                        logger.debug(f"HTTPé”™è¯¯: {response.status} for {url[:100]}...")
                        return False

        except Exception as e:
            logger.debug(f"ä¸‹è½½å›¾ç‰‡å¼‚å¸¸: {e}")
            return False

    def _extract_downloadable_urls(self, pin: Dict) -> list:
        """æå–å¯ä¸‹è½½çš„å›¾ç‰‡URL

        Args:
            pin: Pinæ•°æ®

        Returns:
            å¯ä¸‹è½½çš„URLåˆ—è¡¨
        """
        urls = []

        # æ£€æŸ¥largest_image_url
        largest_url = pin.get('largest_image_url', '').strip()
        if largest_url and largest_url.startswith('http'):
            urls.append(largest_url)

        # æ£€æŸ¥image_urlså­—å…¸
        image_urls = pin.get('image_urls', {})
        if isinstance(image_urls, str):
            try:
                image_urls = json.loads(image_urls)
            except:
                image_urls = {}

        if isinstance(image_urls, dict):
            for key, url in image_urls.items():
                if url and isinstance(url, str) and url.strip().startswith('http'):
                    if url not in urls:
                        urls.append(url)

        return urls
    
    def _discover_all_keywords(self) -> list:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        from pathlib import Path
        
        keywords = []
        output_path = Path(self.output_dir)
        
        if output_path.exists():
            for keyword_dir in output_path.iterdir():
                if keyword_dir.is_dir():
                    db_file = keyword_dir / "pinterest.db"
                    if db_file.exists():
                        keywords.append(keyword_dir.name)
        
        return keywords
