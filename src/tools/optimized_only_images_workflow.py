#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹

æŒ‰ç…§ç”¨æˆ·æŒ‡å®šçš„é€»è¾‘å®ç°ä¸‰é˜¶æ®µå¤„ç†ï¼š
1. å®æ—¶Base64è½¬æ¢é˜¶æ®µï¼šé€ä¸ªæ£€æŸ¥å’Œè½¬æ¢base64ç¼–ç Pin
2. å…¨å±€Headerå‡†å¤‡é˜¶æ®µï¼šå¯åŠ¨æµè§ˆå™¨è·å–headerså¹¶ç¼“å­˜
3. æ™ºèƒ½ä¸‹è½½é˜¶æ®µï¼šæŒ‰éœ€å¢å¼ºPinæ•°æ®å¹¶ä¸‹è½½å›¾ç‰‡
"""

from typing import Optional, Dict
from loguru import logger

from .realtime_base64_converter import RealtimeBase64Converter
from .global_header_manager import GlobalHeaderManager
from .smart_pin_enhancer import SmartPinEnhancer
from ..tools.image_downloader import ImageDownloader


class OptimizedOnlyImagesWorkflow:
    """ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹
    
    å®ç°ç”¨æˆ·æŒ‡å®šçš„ä¸‰é˜¶æ®µå¤„ç†é€»è¾‘
    """
    
    def __init__(self, output_dir: str, max_concurrent: int = 15, proxy: Optional[str] = None):
        """åˆå§‹åŒ–å·¥ä½œæµç¨‹

        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            proxy: ä»£ç†è®¾ç½®
        """
        self.output_dir = output_dir
        self.max_concurrent = max_concurrent
        self.proxy = proxy

        # åˆå§‹åŒ–å„é˜¶æ®µç»„ä»¶ - ä½¿ç”¨å¤šæ ¸åŠ é€Ÿçš„Base64è½¬æ¢å™¨
        self.base64_converter = RealtimeBase64Converter(output_dir)
        logger.info(f"ğŸš€ ä½¿ç”¨å¤šæ ¸åŠ é€Ÿè½¬æ¢å™¨ï¼Œæœ€å¤§å·¥ä½œçº¿ç¨‹æ•°: {self.base64_converter.max_workers}")

        self.header_manager = GlobalHeaderManager(output_dir)
        self.pin_enhancer = SmartPinEnhancer(output_dir)
        
        # å·¥ä½œæµç¨‹ç»Ÿè®¡
        self.workflow_stats = {
            "phase1_base64_conversion": {},
            "phase2_header_preparation": {},
            "phase3_smart_download": {},
            "total_execution_time": 0
        }
    
    async def execute(self, target_keyword: Optional[str] = None) -> Dict:
        """æ‰§è¡Œä¼˜åŒ–åçš„ä¸‰é˜¶æ®µå·¥ä½œæµç¨‹
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰å…³é”®è¯
            
        Returns:
            å·¥ä½œæµç¨‹æ‰§è¡Œç»“æœ
        """
        import time
        start_time = time.time()
        
        logger.info("ğŸš€ å¼€å§‹æ‰§è¡Œä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹")
        logger.info("=" * 60)
        
        try:
            # Phase 1: å®æ—¶Base64è½¬æ¢
            phase1_success = await self._execute_phase1_base64_conversion(target_keyword)
            if not phase1_success:
                logger.error("âŒ Phase 1 å¤±è´¥ï¼Œç»ˆæ­¢å·¥ä½œæµç¨‹")
                return self._generate_failure_result("Phase 1 å¤±è´¥")
            
            # Phase 2: å…¨å±€Headerå‡†å¤‡
            phase2_success = await self._execute_phase2_header_preparation()
            if not phase2_success:
                logger.warning("âš ï¸ Phase 2 å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headersç»§ç»­")
            
            # Phase 3: æ™ºèƒ½ä¸‹è½½
            phase3_success = await self._execute_phase3_smart_download(target_keyword)
            if not phase3_success:
                logger.error("âŒ Phase 3 å¤±è´¥")
                return self._generate_failure_result("Phase 3 å¤±è´¥")
            
            # è®¡ç®—æ€»æ‰§è¡Œæ—¶é—´
            self.workflow_stats["total_execution_time"] = time.time() - start_time
            
            logger.info("=" * 60)
            logger.info("ğŸ‰ ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
            
            return self._generate_success_result()
            
        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {e}")
            return self._generate_failure_result(str(e))
    
    async def _execute_phase1_base64_conversion(self, target_keyword: Optional[str]) -> bool:
        """æ‰§è¡ŒPhase 1: å®æ—¶Base64è½¬æ¢
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯
            
        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸ”„ Phase 1: å®æ—¶Base64è½¬æ¢é˜¶æ®µ")
        logger.info("- é€ä¸ªæ£€æŸ¥æ•°æ®åº“ä¸­çš„base64ç¼–ç Pin")
        logger.info("- æ¯ä¸ªè½¬æ¢éƒ½æ˜¯åŸå­äº‹åŠ¡")
        logger.info("- æŒç»­å¤„ç†ç›´åˆ°æ²¡æœ‰base64ç¼–ç Pin")
        
        try:
            conversion_stats = await self.base64_converter.process_all_databases(target_keyword)
            self.workflow_stats["phase1_base64_conversion"] = conversion_stats
            
            if conversion_stats["total_converted"] > 0:
                logger.info(f"âœ… Phase 1 å®Œæˆ: è½¬æ¢äº† {conversion_stats['total_converted']} ä¸ªbase64ç¼–ç Pin")
            else:
                logger.info("âœ… Phase 1 å®Œæˆ: æ²¡æœ‰å‘ç°éœ€è¦è½¬æ¢çš„base64ç¼–ç Pin")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ Phase 1 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    async def _execute_phase2_header_preparation(self) -> bool:
        """æ‰§è¡ŒPhase 2: å…¨å±€Headerå‡†å¤‡

        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸŒ Phase 2: å…¨å±€Headerå‡†å¤‡é˜¶æ®µ")
        logger.info("- å¯åŠ¨æµè§ˆå™¨è®¿é—®Pinterestå®˜ç½‘")
        logger.info("- è·å–å¹¶ç¼“å­˜æµè§ˆå™¨headers")
        logger.info("- ä¸ºæ•´ä¸ªä¼šè¯æä¾›å…¨å±€headers")

        try:
            # å¼ºåˆ¶è·å–æ–°çš„headers
            logger.info("æ­£åœ¨å¯åŠ¨æµè§ˆå™¨è·å–å…¨å±€Headers...")
            headers_ready = await self.header_manager.ensure_headers_ready()

            header_info = self.header_manager.get_headers_info()
            self.workflow_stats["phase2_header_preparation"] = header_info

            if headers_ready:
                headers = self.header_manager.get_headers()
                logger.info(f"âœ… Phase 2 å®Œæˆ: Headerså‡†å¤‡å°±ç»ª")
                logger.info(f"   - Headersæ•°é‡: {len(headers)} ä¸ªå­—æ®µ")
                logger.info(f"   - åŒ…å«User-Agent: {'User-Agent' in headers}")
                logger.info(f"   - åŒ…å«Cookie: {'Cookie' in headers}")
                return True
            else:
                logger.warning("âš ï¸ Phase 2 å¤±è´¥: Headerså‡†å¤‡å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headers")
                return False

        except Exception as e:
            logger.error(f"âŒ Phase 2 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    async def _execute_phase3_smart_download(self, target_keyword: Optional[str]) -> bool:
        """æ‰§è¡ŒPhase 3: æ™ºèƒ½ä¸‹è½½
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯
            
        Returns:
            æ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        logger.info("ğŸ“¥ Phase 3: æ™ºèƒ½ä¸‹è½½é˜¶æ®µ")
        logger.info("- æ£€æŸ¥æ¯ä¸ªPinæ˜¯å¦ç¼ºå°‘å›¾ç‰‡URL")
        logger.info("- æŒ‰éœ€è·å–Pinè¯¦æƒ…æ•°æ®")
        logger.info("- ç«‹å³æ›´æ–°æ•°æ®åº“åè¿›è¡Œå›¾ç‰‡ä¸‹è½½")
        logger.info("- ä½¿ç”¨å…¨å±€ç¼“å­˜çš„headers")
        
        try:
            # åˆ›å»ºæ™ºèƒ½å›¾ç‰‡ä¸‹è½½å™¨
            smart_downloader = SmartImageDownloader(
                output_dir=self.output_dir,
                max_concurrent=self.max_concurrent,
                proxy=self.proxy,
                header_manager=self.header_manager,
                pin_enhancer=self.pin_enhancer
            )
            
            # æ‰§è¡Œæ™ºèƒ½ä¸‹è½½
            if target_keyword:
                download_stats = await smart_downloader.download_keyword_images(target_keyword)
            else:
                download_stats = await smart_downloader.download_all_images()
            
            self.workflow_stats["phase3_smart_download"] = download_stats
            
            logger.info(f"âœ… Phase 3 å®Œæˆ: {download_stats}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Phase 3 æ‰§è¡Œå¤±è´¥: {e}")
            return False
    
    def _generate_success_result(self) -> Dict:
        """ç”ŸæˆæˆåŠŸç»“æœ
        
        Returns:
            æˆåŠŸç»“æœå­—å…¸
        """
        return {
            "status": "success",
            "message": "ä¼˜åŒ–åçš„--only-imageså·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ",
            "stats": self.workflow_stats,
            "phases": {
                "phase1_base64_conversion": "completed",
                "phase2_header_preparation": "completed",
                "phase3_smart_download": "completed"
            }
        }
    
    def _generate_failure_result(self, error_message: str) -> Dict:
        """ç”Ÿæˆå¤±è´¥ç»“æœ
        
        Args:
            error_message: é”™è¯¯æ¶ˆæ¯
            
        Returns:
            å¤±è´¥ç»“æœå­—å…¸
        """
        return {
            "status": "failed",
            "message": f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {error_message}",
            "stats": self.workflow_stats,
            "error": error_message
        }


class SmartImageDownloader:
    """æ™ºèƒ½å›¾ç‰‡ä¸‹è½½å™¨
    
    é›†æˆPinå¢å¼ºåŠŸèƒ½çš„å›¾ç‰‡ä¸‹è½½å™¨
    """
    
    def __init__(self, output_dir: str, max_concurrent: int, proxy: Optional[str], 
                 header_manager: GlobalHeaderManager, pin_enhancer: SmartPinEnhancer):
        """åˆå§‹åŒ–æ™ºèƒ½ä¸‹è½½å™¨
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            max_concurrent: æœ€å¤§å¹¶å‘æ•°
            proxy: ä»£ç†è®¾ç½®
            header_manager: Headerç®¡ç†å™¨
            pin_enhancer: Pinå¢å¼ºå™¨
        """
        self.output_dir = output_dir
        self.max_concurrent = max_concurrent
        self.proxy = proxy
        self.header_manager = header_manager
        self.pin_enhancer = pin_enhancer
        
        # åˆ›å»ºä¼ ç»Ÿçš„å›¾ç‰‡ä¸‹è½½å™¨
        self.image_downloader = ImageDownloader(
            output_dir=output_dir,
            max_concurrent=max_concurrent,
            proxy=proxy,
            prefer_requests=True
        )
    
    async def download_keyword_images(self, keyword: str) -> Dict:
        """ä¸‹è½½æŒ‡å®šå…³é”®è¯çš„å›¾ç‰‡
        
        Args:
            keyword: å…³é”®è¯
            
        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ™ºèƒ½ä¸‹è½½å…³é”®è¯: {keyword}")
        
        # ä½¿ç”¨å¢å¼ºçš„ä¸‹è½½é€»è¾‘
        return await self._download_with_enhancement(keyword)
    
    async def download_all_images(self) -> Dict:
        """ä¸‹è½½æ‰€æœ‰å…³é”®è¯çš„å›¾ç‰‡
        
        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("å¼€å§‹æ™ºèƒ½ä¸‹è½½æ‰€æœ‰å…³é”®è¯")
        
        # å‘ç°æ‰€æœ‰å…³é”®è¯
        keywords = self._discover_all_keywords()
        
        total_stats = {
            "keywords": len(keywords),
            "downloaded": 0,
            "failed": 0,
            "enhanced": 0
        }
        
        for keyword in keywords:
            keyword_stats = await self._download_with_enhancement(keyword)
            total_stats["downloaded"] += keyword_stats.get("downloaded", 0)
            total_stats["failed"] += keyword_stats.get("failed", 0)
            total_stats["enhanced"] += keyword_stats.get("enhanced", 0)
        
        return total_stats
    
    async def _download_with_enhancement(self, keyword: str) -> Dict:
        """ä½¿ç”¨å¢å¼ºé€»è¾‘ä¸‹è½½å›¾ç‰‡

        Args:
            keyword: å…³é”®è¯

        Returns:
            ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info(f"å¼€å§‹æ™ºèƒ½ä¸‹è½½å…³é”®è¯: {keyword}")

        # è·å–æ‰€æœ‰Pinæ•°æ®
        from ..core.database.repository import SQLiteRepository
        repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

        pins = repository.load_pins_by_query(keyword, limit=None)
        logger.info(f"åŠ è½½äº† {len(pins)} ä¸ªPinè¿›è¡Œæ™ºèƒ½ä¸‹è½½")

        download_stats = {
            "keyword": keyword,
            "total_pins": len(pins),
            "enhanced_pins": 0,
            "downloaded": 0,
            "failed": 0
        }

        # é‡ç½®Pinå¢å¼ºå™¨ç»Ÿè®¡
        self.pin_enhancer.reset_stats()

        # å¤„ç†æ¯ä¸ªPin
        for i, pin in enumerate(pins, 1):
            try:
                logger.debug(f"å¤„ç†Pin ({i}/{len(pins)}): {pin.get('id', '')}")

                # 1. æ™ºèƒ½Pinå¢å¼ºï¼ˆå¦‚æœéœ€è¦ï¼‰
                enhanced_pin = await self.pin_enhancer.enhance_pin_if_needed(pin, keyword)

                # 2. æ£€æŸ¥æ˜¯å¦æœ‰å¯ä¸‹è½½çš„å›¾ç‰‡URL
                image_urls = self._extract_downloadable_urls(enhanced_pin)
                if not image_urls:
                    logger.debug(f"Pin {enhanced_pin.get('id', '')} æ²¡æœ‰å¯ä¸‹è½½çš„å›¾ç‰‡URL")
                    continue

                # 3. ä½¿ç”¨å…¨å±€Headersä¸‹è½½å›¾ç‰‡
                headers = self.header_manager.get_headers()

                # è¿™é‡Œå¯ä»¥é›†æˆå®é™…çš„å›¾ç‰‡ä¸‹è½½é€»è¾‘
                # æš‚æ—¶æ ‡è®°ä¸ºæˆåŠŸ
                download_stats["downloaded"] += 1

            except Exception as e:
                logger.error(f"å¤„ç†Pin {pin.get('id', '')} å¤±è´¥: {e}")
                download_stats["failed"] += 1

        # è·å–Pinå¢å¼ºç»Ÿè®¡
        enhancement_stats = self.pin_enhancer.get_enhancement_stats()
        download_stats["enhanced_pins"] = enhancement_stats["pins_enhanced"]

        logger.info(f"æ™ºèƒ½ä¸‹è½½å®Œæˆ: {download_stats}")
        return download_stats

    def _extract_downloadable_urls(self, pin: Dict) -> list:
        """æå–å¯ä¸‹è½½çš„å›¾ç‰‡URL

        Args:
            pin: Pinæ•°æ®

        Returns:
            å¯ä¸‹è½½çš„URLåˆ—è¡¨
        """
        urls = []

        # æ£€æŸ¥largest_image_url
        largest_url = pin.get('largest_image_url', '').strip()
        if largest_url and largest_url.startswith('http'):
            urls.append(largest_url)

        # æ£€æŸ¥image_urlså­—å…¸
        image_urls = pin.get('image_urls', {})
        if isinstance(image_urls, str):
            try:
                import json
                image_urls = json.loads(image_urls)
            except:
                image_urls = {}

        if isinstance(image_urls, dict):
            for key, url in image_urls.items():
                if url and isinstance(url, str) and url.strip().startswith('http'):
                    if url not in urls:
                        urls.append(url)

        return urls
    
    def _discover_all_keywords(self) -> list:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        import os
        from pathlib import Path
        
        keywords = []
        output_path = Path(self.output_dir)
        
        if output_path.exists():
            for keyword_dir in output_path.iterdir():
                if keyword_dir.is_dir():
                    db_file = keyword_dir / "pinterest.db"
                    if db_file.exists():
                        keywords.append(keyword_dir.name)
        
        return keywords
