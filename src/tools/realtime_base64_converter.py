#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æ‰¹é‡-å¹¶è¡Œ-åŸå­åŒ–Base64è½¬æ¢å™¨

å…¨æ–°æ¶æ„è®¾è®¡ï¼Œå®Œå…¨è§£å†³æ•°æ®åº“æŸåé—®é¢˜çš„åŒæ—¶ä¿æŒé«˜æ€§èƒ½ï¼š

æ ¸å¿ƒæ¶æ„ï¼š
1. ã€æ•°æ®åº“æ“ä½œå•çº¿ç¨‹åŒ–ã€‘ï¼šæ‰¹é‡è¯»å– â†’ åŸå­å†™å…¥ï¼Œå®Œå…¨æ¶ˆé™¤å¹¶å‘å†™å…¥é£é™©
2. ã€è®¡ç®—ä»»åŠ¡å¤šçº¿ç¨‹åŒ–ã€‘ï¼šBase64è§£ç ç­‰CPUå¯†é›†å‹æ“ä½œå¹¶è¡Œå¤„ç†
3. ã€æ‰¹é‡åŸå­äº‹åŠ¡ã€‘ï¼šæ¯æ‰¹æ¬¡ä½œä¸ºä¸€ä¸ªå®Œæ•´äº‹åŠ¡ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
4. ã€ä¼˜é›…ä¸­æ–­æ”¯æŒã€‘ï¼šæ‰¹æ¬¡é—´æ£€æŸ¥ä¸­æ–­ä¿¡å·ï¼Œå½“å‰æ‰¹æ¬¡å®Œæˆåå®‰å…¨é€€å‡º
5. ã€æ€§èƒ½ä¼˜åŒ–ã€‘ï¼šæ‰¹æ¬¡å¤§å°å¯é…ç½®ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸CPUä¼˜åŠ¿

æŠ€æœ¯ç‰¹æ€§ï¼š
- æ•°æ®åº“å®‰å…¨ï¼šå•çº¿ç¨‹ä¸²è¡Œæ“ä½œSQLiteï¼Œé›¶æŸåé£é™©
- é«˜æ€§èƒ½è®¡ç®—ï¼šå¤šçº¿ç¨‹å¹¶è¡ŒBase64è§£ç å’Œæ•°æ®è½¬æ¢
- åŸå­æ€§ä¿è¯ï¼šæ‰¹æ¬¡çº§äº‹åŠ¡ï¼Œè¦ä¹ˆå…¨éƒ¨æˆåŠŸè¦ä¹ˆå…¨éƒ¨å¤±è´¥
- ä¸­æ–­å®‰å…¨ï¼šæ‰¹æ¬¡è¾¹ç•Œæ£€æŸ¥ä¸­æ–­ï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
- å†…å­˜æ§åˆ¶ï¼šæ‰¹æ¬¡å¤§å°é™åˆ¶ï¼Œé¿å…å†…å­˜æº¢å‡º
"""

import asyncio
import base64
import json
import hashlib
import time
import sys
import os
import signal
import threading
import tempfile
import shutil
import sqlite3
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Tuple, NamedTuple
from dataclasses import dataclass
from pathlib import Path
from loguru import logger

from ..core.database.repository import SQLiteRepository
from ..utils.progress_display import WindowsProgressDisplay


@dataclass
class ConversionBatch:
    """è½¬æ¢æ‰¹æ¬¡æ•°æ®ç»“æ„"""
    batch_id: int
    pins: List[Dict]
    keyword: str

    def __len__(self):
        return len(self.pins)


@dataclass
class ConversionResult:
    """å•ä¸ªPinè½¬æ¢ç»“æœ"""
    original_pin: Dict
    decoded_id: Optional[str]
    success: bool
    error_message: Optional[str] = None


class BatchAtomicBase64Converter:
    """æ‰¹é‡-å¹¶è¡Œ-åŸå­åŒ–Base64è½¬æ¢å™¨

    æ–°æ¶æ„ç‰¹ç‚¹ï¼š
    1. ã€æ•°æ®åº“å®‰å…¨ã€‘ï¼šå•çº¿ç¨‹æ‰¹é‡è¯»å–å’ŒåŸå­å†™å…¥ï¼Œé›¶æŸåé£é™©
    2. ã€é«˜æ€§èƒ½è®¡ç®—ã€‘ï¼šå¤šçº¿ç¨‹å¹¶è¡ŒBase64è§£ç ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸
    3. ã€æ‰¹é‡äº‹åŠ¡ã€‘ï¼šæ¯æ‰¹æ¬¡ä½œä¸ºåŸå­äº‹åŠ¡ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
    4. ã€ä¼˜é›…ä¸­æ–­ã€‘ï¼šæ‰¹æ¬¡è¾¹ç•Œæ£€æŸ¥ä¸­æ–­ï¼Œå®‰å…¨é€€å‡º
    5. ã€å†…å­˜æ§åˆ¶ã€‘ï¼šå¯é…ç½®æ‰¹æ¬¡å¤§å°ï¼Œé¿å…å†…å­˜æº¢å‡º
    6. ã€è¿›åº¦å¯è§†ã€‘ï¼šå®æ—¶æ˜¾ç¤ºæ‰¹æ¬¡å¤„ç†è¿›åº¦
    """

    def __init__(self, output_dir: str, batch_size: int = 4096, max_workers: int = None):
        """åˆå§‹åŒ–æ‰¹é‡åŸå­è½¬æ¢å™¨

        Args:
            output_dir: è¾“å‡ºç›®å½•
            batch_size: æ‰¹æ¬¡å¤§å°ï¼ˆæ¯æ‰¹å¤„ç†çš„Pinæ•°é‡ï¼‰
            max_workers: è®¡ç®—çº¿ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºè‡ªåŠ¨æ£€æµ‹CPUæ ¸å¿ƒæ•°
        """
        self.output_dir = output_dir
        # æ¿€è¿›ä¼˜åŒ–ï¼šå¤§å¹…å¢åŠ æ‰¹æ¬¡å¤§å°ä¸Šé™ä»¥å®ç°2å€æ€§èƒ½æå‡
        self.batch_size = max(1, min(batch_size, 8192))  # æå‡æ‰¹æ¬¡å¤§å°ä¸Šé™åˆ°8192
        # æ¿€è¿›ä¼˜åŒ–ï¼šå¤§å¹…æå‡å¹¶å‘æ•°ä»¥å®ç°2å€æ€§èƒ½æå‡
        cpu_cores = os.cpu_count() or 1
        # å¯¹äºBase64è§£ç è¿™ç§CPUå¯†é›†å‹ä»»åŠ¡ï¼Œä½¿ç”¨æ›´é«˜çš„å¹¶å‘æ•°
        aggressive_workers = min(32, cpu_cores * 4)  # æå‡åˆ°CPUæ ¸å¿ƒæ•°Ã—4ï¼Œä¸Šé™32
        self.max_workers = max_workers or aggressive_workers

        # ä½¿ç”¨å…¨å±€ä¸­æ–­ç®¡ç†å™¨ï¼Œä¸è®¾ç½®è‡ªå·±çš„ä¿¡å·å¤„ç†å™¨
        from .stage_manager import _global_interrupt_manager
        self.interrupt_manager = _global_interrupt_manager

        # ç»Ÿè®¡ä¿¡æ¯
        self.conversion_stats = {
            "total_converted": 0,
            "total_failed": 0,
            "total_batches": 0,
            "current_keyword": "",
            "keywords_processed": 0,
            "batch_size": self.batch_size,
            "max_workers": self.max_workers
        }

        logger.info(f"ğŸš€ åˆå§‹åŒ–æ‰¹é‡åŸå­è½¬æ¢å™¨ï¼ˆæ¿€è¿›æ€§èƒ½ä¼˜åŒ–ç‰ˆ - ç›®æ ‡2å€æ€§èƒ½æå‡ï¼‰")
        logger.info(f"   - æ‰¹æ¬¡å¤§å°: {self.batch_size} pins/batch (æ¿€è¿›ä¼˜åŒ–: é»˜è®¤4096, ä¸Šé™8192)")
        logger.info(f"   - è®¡ç®—çº¿ç¨‹: {self.max_workers} threads (æ¿€è¿›ä¼˜åŒ–: CPUæ ¸å¿ƒæ•°Ã—4, ä¸Šé™32)")
        logger.info(f"   - äº‹åŠ¡ä¼˜åŒ–: åŠ¨æ€æ‰¹é‡æäº¤ (500-1000ä¸ªPin/æ‰¹æ¬¡)")
        logger.info(f"   - æ•°æ®åº“ä¼˜åŒ–: SQLiteæ€§èƒ½å‚æ•°è°ƒä¼˜ (64MBç¼“å­˜, å†…å­˜æ˜ å°„)")
        logger.info(f"   - åŠ¨æ€è°ƒä¼˜: æ ¹æ®æ•°æ®é›†å¤§å°è‡ªåŠ¨ä¼˜åŒ–å‚æ•°")
        logger.info(f"   - æ¶æ„æ¨¡å¼: æ‰¹é‡è¯»å– â†’ é«˜å¹¶å‘è½¬æ¢ â†’ åŸå­å†™å…¥")
    

    
    async def process_all_databases(self, target_keyword: Optional[str] = None) -> Dict[str, int]:
        """å¤„ç†æ‰€æœ‰æ•°æ®åº“æˆ–æŒ‡å®šå…³é”®è¯æ•°æ®åº“
        
        Args:
            target_keyword: ç›®æ ‡å…³é”®è¯ï¼ŒNoneè¡¨ç¤ºå¤„ç†æ‰€æœ‰æ•°æ®åº“
            
        Returns:
            è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("ğŸš€ å¼€å§‹æ‰¹é‡-å¹¶è¡Œ-åŸå­åŒ–Base64è½¬æ¢")

        if target_keyword:
            # å¤„ç†æŒ‡å®šå…³é”®è¯
            await self._process_single_database_batch_atomic(target_keyword)
        else:
            # å¤„ç†æ‰€æœ‰å…³é”®è¯
            keywords = self._discover_all_keywords()
            for keyword in keywords:
                if self.interrupt_manager.is_interrupted():
                    logger.info("ğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œåœæ­¢å¤„ç†")
                    raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")
                await self._process_single_database_batch_atomic(keyword)
                self.conversion_stats["keywords_processed"] += 1

        logger.info(f"âœ… æ‰¹é‡åŸå­è½¬æ¢å®Œæˆ: {self.conversion_stats}")
        return self.conversion_stats
    
    async def _process_single_database_batch_atomic(self, keyword: str) -> bool:
        """æ‰¹é‡åŸå­å¤„ç†å•ä¸ªå…³é”®è¯æ•°æ®åº“

        æ–°æ¶æ„æµç¨‹ï¼š
        0. æ•°æ®åº“å¥åº·æ£€æŸ¥å’Œä¿®å¤
        1. å•çº¿ç¨‹æ‰¹é‡è¯»å–base64ç¼–ç Pin
        2. å¤šçº¿ç¨‹å¹¶è¡Œè½¬æ¢ï¼ˆçº¯è®¡ç®—ä»»åŠ¡ï¼‰
        3. å•çº¿ç¨‹åŸå­æ‰¹é‡å†™å…¥æ•°æ®åº“
        4. æ‰¹æ¬¡è¾¹ç•Œæ£€æŸ¥ä¸­æ–­ä¿¡å·

        Args:
            keyword: å…³é”®è¯

        Returns:
            æ˜¯å¦å¤„ç†æˆåŠŸ
        """
        try:
            logger.info(f"ï¿½ å¼€å§‹æ‰¹é‡åŸå­å¤„ç†å…³é”®è¯: {keyword}")
            self.conversion_stats["current_keyword"] = keyword

            # åˆ›å»ºRepositoryï¼ˆä»…ç”¨äºå•çº¿ç¨‹æ•°æ®åº“æ“ä½œï¼‰
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # ã€é˜¶æ®µ0ã€‘æ•°æ®åº“å¥åº·æ£€æŸ¥å’Œä¿®å¤
            if not await self._check_and_repair_database(repository, keyword):
                logger.error(f"âŒ æ•°æ®åº“å¥åº·æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡å…³é”®è¯: {keyword}")
                return False

            # ã€é˜¶æ®µ1ã€‘å•çº¿ç¨‹æ‰¹é‡è¯»å–æ‰€æœ‰base64ç¼–ç Pin
            all_base64_pins = self._get_all_base64_pins(repository)

            if not all_base64_pins:
                logger.info(f"âœ… å…³é”®è¯ {keyword}: æ²¡æœ‰å‘ç°base64ç¼–ç Pin")
                return True

            total_batches = (len(all_base64_pins) + self.batch_size - 1) // self.batch_size
            logger.info(f"ğŸ“Š å…³é”®è¯ {keyword}: å‘ç° {len(all_base64_pins)} ä¸ªbase64ç¼–ç Pin")
            logger.info(f"ğŸ“¦ å°†åˆ†ä¸º {total_batches} ä¸ªæ‰¹æ¬¡å¤„ç† (æ‰¹æ¬¡å¤§å°: {self.batch_size})")

            # ã€é˜¶æ®µ2ã€‘åˆ†æ‰¹æ¬¡å¤„ç†ï¼šæ‰¹é‡è¯»å– â†’ å¹¶è¡Œè½¬æ¢ â†’ åŸå­å†™å…¥
            total_converted = await self._process_batches_atomic(all_base64_pins, keyword, repository)

            logger.info(f"âœ… å…³é”®è¯ {keyword}: æ‰¹é‡åŸå­è½¬æ¢å®Œæˆ {total_converted}/{len(all_base64_pins)} ä¸ªPin")
            return True

        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡åŸå­å¤„ç†å…³é”®è¯ {keyword} å¤±è´¥: {e}")
            return False
    
    def _get_all_base64_pins(self, repository: SQLiteRepository) -> List[Dict]:
        """æ‰¹é‡è·å–æ‰€æœ‰base64ç¼–ç Pin
        
        Args:
            repository: æ•°æ®åº“ä»“åº“
            
        Returns:
            base64ç¼–ç Pinåˆ—è¡¨
        """
        try:
            with repository._get_session() as session:
                from src.core.database.schema import Pin
                
                # æ‰¹é‡æŸ¥è¯¢æ‰€æœ‰base64ç¼–ç Pin
                pin_records = session.query(Pin).filter(
                    Pin.id.like('UGlu%')  # base64ç¼–ç çš„Pin IDéƒ½ä»¥'UGlu'å¼€å¤´
                ).all()
                
                pins = []
                for pin_record in pin_records:
                    pins.append({
                        'id': pin_record.id,
                        'title': pin_record.title or '',
                        'description': pin_record.description or '',
                        'creator_name': pin_record.creator_name or '',
                        'creator_id': pin_record.creator_id or '',
                        'board_name': pin_record.board_name or '',
                        'board_id': pin_record.board_id or '',
                        'image_urls': pin_record.image_urls or '{}',
                        'largest_image_url': pin_record.largest_image_url or '',
                        'stats': pin_record.stats or '{}',
                        'raw_data': pin_record.raw_data or '{}',
                        'query': pin_record.query or ''
                    })
                
                logger.debug(f"ã€å•çº¿ç¨‹è¯»å–ã€‘æ‰¹é‡åŠ è½½äº† {len(pins)} ä¸ªbase64ç¼–ç Pin")
                return pins

        except Exception as e:
            logger.error(f"æ‰¹é‡è·å–base64ç¼–ç Pinå¤±è´¥: {e}")
            return []
    
    async def _process_pins_concurrently(self, pins: List[Dict], keyword: str, 
                                       repository: SQLiteRepository, 
                                       progress: WindowsProgressDisplay) -> int:
        """å¹¶å‘å¤„ç†Pinåˆ—è¡¨
        
        Args:
            pins: Pinåˆ—è¡¨
            keyword: å…³é”®è¯
            repository: æ•°æ®åº“ä»“åº“
            progress: è¿›åº¦æ˜¾ç¤ºå™¨
            
        Returns:
            æˆåŠŸè½¬æ¢çš„Pinæ•°é‡
        """
        conversion_count = 0
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†ï¼ˆæ•°æ®åº“I/Oå¯†é›†å‹ä»»åŠ¡é€‚åˆçº¿ç¨‹æ± ï¼‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_pin = {
                executor.submit(self._convert_single_pin_sync, pin, keyword): pin
                for pin in pins
            }

            try:
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_pin):
                    if self.interrupt_manager.is_interrupted():
                        logger.info("ğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨å–æ¶ˆå‰©ä½™ä»»åŠ¡...")

                        # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                        cancelled_count = 0
                        for f in future_to_pin:
                            if not f.done():
                                if f.cancel():
                                    cancelled_count += 1

                        logger.info(f"ğŸ›‘ å·²å–æ¶ˆ {cancelled_count} ä¸ªæœªå®Œæˆçš„ä»»åŠ¡")

                        # æŠ›å‡ºKeyboardInterruptä»¥ç»ˆæ­¢å¤„ç†
                        raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")

                    pin = future_to_pin[future]
                    try:
                        success = future.result()
                        if success:
                            conversion_count += 1
                            self.conversion_stats["total_converted"] += 1
                        else:
                            self.conversion_stats["total_failed"] += 1

                        # æ›´æ–°è¿›åº¦
                        progress.update(1)

                    except Exception as e:
                        logger.error(f"Pinè½¬æ¢ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
                        self.conversion_stats["total_failed"] += 1
                        progress.update(1)

            except KeyboardInterrupt:
                # å¤„ç†é¢å¤–çš„ä¸­æ–­ä¿¡å·
                logger.info("ğŸ›‘ æ¥æ”¶åˆ°é¢å¤–ä¸­æ–­ä¿¡å·ï¼Œç«‹å³åœæ­¢")
                raise  # é‡æ–°æŠ›å‡ºKeyboardInterrupt

        # å¦‚æœæ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå¼ºåˆ¶å…³é—­çº¿ç¨‹æ± 
        if self.interrupt_manager.is_interrupted():
            logger.info("ğŸ›‘ å¼ºåˆ¶å…³é—­çº¿ç¨‹æ± ï¼Œä¸ç­‰å¾…å‰©ä½™ä»»åŠ¡")
            # æ³¨æ„ï¼šè¿™é‡Œä¸èƒ½è°ƒç”¨executor.shutdown()ï¼Œå› ä¸ºå·²ç»é€€å‡ºwithè¯­å¥
        
        return conversion_count
    
    def _convert_single_pin_sync(self, pin: Dict, keyword: str) -> bool:
        """åŒæ­¥è½¬æ¢å•ä¸ªPinï¼ˆçº¿ç¨‹å®‰å…¨ç‰ˆæœ¬ï¼‰

        Args:
            pin: Pinæ•°æ®
            keyword: å…³é”®è¯

        Returns:
            æ˜¯å¦è½¬æ¢æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if self.interrupt_manager.is_interrupted():
                raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")

            # ä¸ºæ¯ä¸ªçº¿ç¨‹åˆ›å»ºç‹¬ç«‹çš„æ•°æ®åº“è¿æ¥
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # 1. è§£ç Pin ID
            decoded_id = self._decode_base64_pin_id(pin['id'])
            if not decoded_id:
                return False

            # å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if self.interrupt_manager.is_interrupted():
                raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")

            # 2. æ‰§è¡ŒåŸå­äº‹åŠ¡
            return self._atomic_pin_id_conversion_sync(pin, decoded_id, keyword, repository)

        except Exception as e:
            logger.error(f"è½¬æ¢Pin {pin['id']} å¤±è´¥: {e}")
            return False
    
    def _atomic_pin_id_conversion_sync(self, old_pin: Dict, new_pin_id: str, 
                                     keyword: str, repository: SQLiteRepository) -> bool:
        """åŒæ­¥åŸå­æ€§Pin IDè½¬æ¢æ“ä½œ
        
        Args:
            old_pin: æ—§Pinæ•°æ®
            new_pin_id: æ–°Pin ID
            keyword: å…³é”®è¯
            repository: æ•°æ®åº“ä»“åº“
            
        Returns:
            æ˜¯å¦è½¬æ¢æˆåŠŸ
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
            if self.interrupt_manager.is_interrupted():
                raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")

            with repository._get_session() as session:
                from src.core.database.schema import Pin

                # å¼€å§‹äº‹åŠ¡
                session.begin()

                try:
                    # å†æ¬¡æ£€æŸ¥æ˜¯å¦éœ€è¦åœæ­¢
                    if self.interrupt_manager.is_interrupted():
                        session.rollback()
                        raise KeyboardInterrupt("Base64è½¬æ¢è¢«ç”¨æˆ·ä¸­æ–­")

                    # 1. æ£€æŸ¥æ–°Pin IDæ˜¯å¦å·²å­˜åœ¨
                    existing_pin = session.query(Pin).filter_by(id=new_pin_id).first()
                    if existing_pin:
                        # å¦‚æœæ–°Pin IDå·²å­˜åœ¨ï¼Œåªåˆ é™¤æ—§çš„base64ç¼–ç Pin
                        deleted_count = session.query(Pin).filter_by(id=old_pin['id']).delete()
                        if deleted_count > 0:
                            session.commit()
                            return True
                        else:
                            session.rollback()
                            return False
                    
                    # 2. åˆ é™¤æ—§è®°å½•
                    deleted_count = session.query(Pin).filter_by(id=old_pin['id']).delete()
                    if deleted_count == 0:
                        session.rollback()
                        return False
                    
                    # 3. åˆ›å»ºæ–°Pinè®°å½•
                    pin_hash = hashlib.md5(f"{new_pin_id}_{keyword}".encode('utf-8')).hexdigest()
                    
                    new_pin = Pin(
                        id=new_pin_id,
                        pin_hash=pin_hash,
                        title=old_pin.get('title', ''),
                        description=old_pin.get('description', ''),
                        creator_name=old_pin.get('creator_name', ''),
                        creator_id=old_pin.get('creator_id', ''),
                        board_name=old_pin.get('board_name', ''),
                        board_id=old_pin.get('board_id', ''),
                        image_urls=old_pin.get('image_urls', '{}'),
                        largest_image_url=old_pin.get('largest_image_url', ''),
                        stats=old_pin.get('stats', '{}'),
                        raw_data=old_pin.get('raw_data', '{}'),
                        query=keyword
                    )
                    
                    # 4. æ’å…¥æ–°è®°å½•
                    session.add(new_pin)
                    
                    # 5. æäº¤äº‹åŠ¡
                    session.commit()
                    return True
                    
                except Exception as e:
                    session.rollback()
                    logger.error(f"Pin IDè½¬æ¢äº‹åŠ¡å¤±è´¥: {e}")
                    return False
                    
        except Exception as e:
            logger.error(f"åŸå­æ€§Pinæ›¿æ¢å¤±è´¥: {e}")
            return False
    
    def _decode_base64_pin_id(self, encoded_pin_id: str) -> Optional[str]:
        """è§£ç base64ç¼–ç çš„Pin ID
        
        Args:
            encoded_pin_id: base64ç¼–ç çš„Pin ID
            
        Returns:
            è§£ç åçš„æ•°å­—Pin IDï¼Œå¤±è´¥è¿”å›None
        """
        try:
            if encoded_pin_id.startswith('UGlu'):
                decoded = base64.b64decode(encoded_pin_id).decode('utf-8')
                if decoded.startswith('Pin:'):
                    return decoded[4:]
            return None
        except Exception as e:
            return None
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        # è¿™é‡Œå¤ç”¨åŸæœ‰çš„å‘ç°é€»è¾‘
        from pathlib import Path
        keywords = []
        output_path = Path(self.output_dir)
        
        if output_path.exists():
            for keyword_dir in output_path.iterdir():
                if keyword_dir.is_dir():
                    db_file = keyword_dir / "pinterest.db"
                    if db_file.exists():
                        keywords.append(keyword_dir.name)
        
        return keywords

    # ==================== æ–°æ¶æ„ï¼šæ‰¹é‡-å¹¶è¡Œ-åŸå­åŒ–æ–¹æ³• ====================

    def _optimize_batch_size_for_dataset(self, total_pins: int) -> int:
        """æ ¹æ®æ•°æ®é›†å¤§å°åŠ¨æ€ä¼˜åŒ–æ‰¹æ¬¡å¤§å°

        Args:
            total_pins: æ€»Pinæ•°é‡

        Returns:
            ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°
        """
        # æ¿€è¿›ä¼˜åŒ–ï¼šæ ¹æ®æ•°æ®é›†å¤§å°åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        if total_pins >= 10000:
            # å¤§æ•°æ®é›†ï¼šä½¿ç”¨æœ€å¤§æ‰¹æ¬¡å¤§å°
            optimized_size = min(8192, self.batch_size * 2)
        elif total_pins >= 5000:
            # ä¸­ç­‰æ•°æ®é›†ï¼šä½¿ç”¨è¾ƒå¤§æ‰¹æ¬¡å¤§å°
            optimized_size = min(6144, int(self.batch_size * 1.5))
        elif total_pins >= 1000:
            # å°æ•°æ®é›†ï¼šä½¿ç”¨æ ‡å‡†æ‰¹æ¬¡å¤§å°
            optimized_size = self.batch_size
        else:
            # æå°æ•°æ®é›†ï¼šä½¿ç”¨è¾ƒå°æ‰¹æ¬¡å¤§å°é¿å…å¼€é”€
            optimized_size = min(1024, self.batch_size)

        logger.debug(f"æ•°æ®é›†å¤§å° {total_pins} -> ä¼˜åŒ–æ‰¹æ¬¡å¤§å°: {optimized_size}")
        return optimized_size

    async def _process_batches_atomic(self, all_pins: List[Dict], keyword: str,
                                    repository: SQLiteRepository) -> int:
        """ã€æ ¸å¿ƒæ–¹æ³•ã€‘åˆ†æ‰¹æ¬¡åŸå­å¤„ç†Pinåˆ—è¡¨

        æ¶æ„æµç¨‹ï¼š
        1. å°†Pinåˆ—è¡¨åˆ†å‰²ä¸ºæ‰¹æ¬¡
        2. å¯¹æ¯ä¸ªæ‰¹æ¬¡ï¼šæ‰¹é‡è¯»å– â†’ å¹¶è¡Œè½¬æ¢ â†’ åŸå­å†™å…¥
        3. åœ¨æ‰¹æ¬¡è¾¹ç•Œæ£€æŸ¥ä¸­æ–­ä¿¡å·

        Args:
            all_pins: æ‰€æœ‰å¾…è½¬æ¢çš„Pinåˆ—è¡¨
            keyword: å…³é”®è¯
            repository: æ•°æ®åº“ä»“åº“

        Returns:
            æˆåŠŸè½¬æ¢çš„Pinæ€»æ•°
        """
        total_converted = 0

        # æ¿€è¿›ä¼˜åŒ–ï¼šæ ¹æ®æ•°æ®é›†å¤§å°åŠ¨æ€ä¼˜åŒ–æ‰¹æ¬¡å¤§å°
        optimized_batch_size = self._optimize_batch_size_for_dataset(len(all_pins))
        total_batches = (len(all_pins) + optimized_batch_size - 1) // optimized_batch_size

        # åˆ›å»ºè¿›åº¦æ¡
        with WindowsProgressDisplay(
            total=len(all_pins),
            desc=f"æ‰¹é‡åŸå­è½¬æ¢{keyword}",
            unit="pin"
        ) as progress:

            # åˆ†æ‰¹æ¬¡å¤„ç†
            for batch_id in range(total_batches):
                # æ£€æŸ¥ä¸­æ–­ä¿¡å·ï¼ˆåœ¨æ‰¹æ¬¡è¾¹ç•Œï¼‰
                if self.interrupt_manager.is_interrupted():
                    logger.info(f"ğŸ›‘ åœ¨æ‰¹æ¬¡ {batch_id + 1}/{total_batches} æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œå®‰å…¨é€€å‡º")
                    raise KeyboardInterrupt(f"Base64è½¬æ¢åœ¨æ‰¹æ¬¡ {batch_id + 1}/{total_batches} è¢«ç”¨æˆ·ä¸­æ–­")

                # è®¡ç®—å½“å‰æ‰¹æ¬¡èŒƒå›´ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„æ‰¹æ¬¡å¤§å°ï¼‰
                start_idx = batch_id * optimized_batch_size
                end_idx = min(start_idx + optimized_batch_size, len(all_pins))
                batch_pins = all_pins[start_idx:end_idx]

                # åˆ›å»ºæ‰¹æ¬¡å¯¹è±¡
                batch = ConversionBatch(
                    batch_id=batch_id,
                    pins=batch_pins,
                    keyword=keyword
                )

                logger.debug(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_id + 1}/{total_batches}: {len(batch_pins)} ä¸ªPin")

                # å¤„ç†å•ä¸ªæ‰¹æ¬¡
                batch_converted = await self._process_single_batch_atomic(batch, repository, progress)
                total_converted += batch_converted

                # æ›´æ–°ç»Ÿè®¡
                self.conversion_stats["total_batches"] += 1

                # æ‰¹æ¬¡é—´çŸ­æš‚ä¼‘æ¯ï¼Œè®©ç³»ç»Ÿæœ‰æœºä¼šå¤„ç†å…¶ä»–ä»»åŠ¡
                await asyncio.sleep(0.01)

        return total_converted

    async def _process_single_batch_atomic(self, batch: ConversionBatch,
                                         repository: SQLiteRepository,
                                         progress: WindowsProgressDisplay) -> int:
        """ã€åŸå­äº‹åŠ¡ã€‘å¤„ç†å•ä¸ªæ‰¹æ¬¡

        æµç¨‹ï¼š
        1. å¤šçº¿ç¨‹å¹¶è¡Œè½¬æ¢ï¼ˆçº¯è®¡ç®—ä»»åŠ¡ï¼‰
        2. å•çº¿ç¨‹åŸå­æ‰¹é‡å†™å…¥æ•°æ®åº“

        Args:
            batch: è½¬æ¢æ‰¹æ¬¡
            repository: æ•°æ®åº“ä»“åº“
            progress: è¿›åº¦æ˜¾ç¤ºå™¨

        Returns:
            æˆåŠŸè½¬æ¢çš„Pinæ•°é‡
        """
        try:
            # ã€æ­¥éª¤1ã€‘å¤šçº¿ç¨‹å¹¶è¡Œè½¬æ¢ï¼ˆçº¯è®¡ç®—ä»»åŠ¡ï¼Œä¸æ¶‰åŠæ•°æ®åº“ï¼‰
            conversion_results = await self._parallel_convert_batch(batch)

            # ã€æ­¥éª¤2ã€‘å•çº¿ç¨‹åŸå­æ‰¹é‡å†™å…¥æ•°æ®åº“
            success_count = self._atomic_batch_write(batch, conversion_results, repository)

            # æ›´æ–°è¿›åº¦æ¡
            progress.update(len(batch))

            # æ›´æ–°ç»Ÿè®¡
            self.conversion_stats["total_converted"] += success_count
            self.conversion_stats["total_failed"] += (len(batch) - success_count)

            logger.debug(f"ğŸ“¦ æ‰¹æ¬¡ {batch.batch_id}: æˆåŠŸè½¬æ¢ {success_count}/{len(batch)} ä¸ªPin")
            return success_count

        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch.batch_id} å¤„ç†å¤±è´¥: {e}")
            # æ›´æ–°è¿›åº¦æ¡ï¼ˆå³ä½¿å¤±è´¥ä¹Ÿè¦æ›´æ–°ï¼‰
            progress.update(len(batch))
            # æ›´æ–°å¤±è´¥ç»Ÿè®¡
            self.conversion_stats["total_failed"] += len(batch)
            return 0

    async def _parallel_convert_batch(self, batch: ConversionBatch) -> List[ConversionResult]:
        """ã€å¤šçº¿ç¨‹å¹¶è¡Œã€‘è½¬æ¢æ‰¹æ¬¡ä¸­çš„æ‰€æœ‰Pinï¼ˆçº¯è®¡ç®—ä»»åŠ¡ï¼‰

        Args:
            batch: è½¬æ¢æ‰¹æ¬¡

        Returns:
            è½¬æ¢ç»“æœåˆ—è¡¨
        """
        conversion_results = []

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¹¶è¡ŒBase64è§£ç ï¼ˆCPUå¯†é›†å‹ä»»åŠ¡ï¼‰
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰è½¬æ¢ä»»åŠ¡
            future_to_pin = {
                executor.submit(self._convert_single_pin_pure, pin): pin
                for pin in batch.pins
            }

            # æ”¶é›†è½¬æ¢ç»“æœ
            for future in as_completed(future_to_pin):
                pin = future_to_pin[future]
                try:
                    result = future.result()
                    conversion_results.append(result)
                except Exception as e:
                    # åˆ›å»ºå¤±è´¥ç»“æœ
                    error_result = ConversionResult(
                        original_pin=pin,
                        decoded_id=None,
                        success=False,
                        error_message=str(e)
                    )
                    conversion_results.append(error_result)

        return conversion_results

    def _convert_single_pin_pure(self, pin: Dict) -> ConversionResult:
        """ã€çº¯è®¡ç®—ã€‘è½¬æ¢å•ä¸ªPinï¼ˆä¸æ¶‰åŠæ•°æ®åº“æ“ä½œï¼‰

        Args:
            pin: Pinæ•°æ®

        Returns:
            è½¬æ¢ç»“æœ
        """
        try:
            # Base64è§£ç 
            decoded_id = self._decode_base64_pin_id(pin['id'])

            if decoded_id:
                return ConversionResult(
                    original_pin=pin,
                    decoded_id=decoded_id,
                    success=True
                )
            else:
                return ConversionResult(
                    original_pin=pin,
                    decoded_id=None,
                    success=False,
                    error_message="Base64è§£ç å¤±è´¥"
                )

        except Exception as e:
            return ConversionResult(
                original_pin=pin,
                decoded_id=None,
                success=False,
                error_message=str(e)
            )

    def _atomic_batch_write(self, batch: ConversionBatch,
                          conversion_results: List[ConversionResult],
                          repository: SQLiteRepository) -> int:
        """ã€å•çº¿ç¨‹åŸå­äº‹åŠ¡ã€‘æ‰¹é‡å†™å…¥è½¬æ¢ç»“æœåˆ°æ•°æ®åº“

        Args:
            batch: è½¬æ¢æ‰¹æ¬¡
            conversion_results: è½¬æ¢ç»“æœåˆ—è¡¨
            repository: æ•°æ®åº“ä»“åº“

        Returns:
            æˆåŠŸå†™å…¥çš„Pinæ•°é‡
        """
        success_count = 0

        try:
            with repository._get_session() as session:
                from src.core.database.schema import Pin

                # å¼€å§‹äº‹åŠ¡
                session.begin()

                try:
                    # æ‰¹é‡å¤„ç†æ‰€æœ‰æˆåŠŸçš„è½¬æ¢ç»“æœ
                    for result in conversion_results:
                        if not result.success or not result.decoded_id:
                            continue

                        old_pin = result.original_pin
                        new_pin_id = result.decoded_id

                        # æ£€æŸ¥æ–°Pin IDæ˜¯å¦å·²å­˜åœ¨
                        existing_pin = session.query(Pin).filter_by(id=new_pin_id).first()
                        if existing_pin:
                            # å¦‚æœæ–°Pin IDå·²å­˜åœ¨ï¼Œåªåˆ é™¤æ—§çš„base64ç¼–ç Pin
                            deleted_count = session.query(Pin).filter_by(id=old_pin['id']).delete()
                            if deleted_count > 0:
                                success_count += 1
                            continue

                        # åˆ é™¤æ—§è®°å½•
                        deleted_count = session.query(Pin).filter_by(id=old_pin['id']).delete()
                        if deleted_count == 0:
                            continue  # æ—§è®°å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡

                        # åˆ›å»ºæ–°Pinè®°å½•
                        pin_hash = hashlib.md5(f"{new_pin_id}_{batch.keyword}".encode('utf-8')).hexdigest()

                        new_pin = Pin(
                            id=new_pin_id,
                            pin_hash=pin_hash,
                            title=old_pin.get('title', ''),
                            description=old_pin.get('description', ''),
                            creator_name=old_pin.get('creator_name', ''),
                            creator_id=old_pin.get('creator_id', ''),
                            board_name=old_pin.get('board_name', ''),
                            board_id=old_pin.get('board_id', ''),
                            image_urls=old_pin.get('image_urls', '{}'),
                            largest_image_url=old_pin.get('largest_image_url', ''),
                            stats=old_pin.get('stats', '{}'),
                            raw_data=old_pin.get('raw_data', '{}'),
                            query=batch.keyword
                        )

                        # æ’å…¥æ–°è®°å½•
                        session.add(new_pin)
                        success_count += 1

                    # æäº¤æ•´ä¸ªæ‰¹æ¬¡çš„äº‹åŠ¡
                    session.commit()
                    logger.debug(f"ã€åŸå­å†™å…¥ã€‘æ‰¹æ¬¡ {batch.batch_id}: æˆåŠŸå†™å…¥ {success_count} ä¸ªPin")

                except Exception as e:
                    # å›æ»šæ•´ä¸ªæ‰¹æ¬¡
                    session.rollback()
                    logger.error(f"âŒ æ‰¹æ¬¡ {batch.batch_id} åŸå­å†™å…¥å¤±è´¥ï¼Œå·²å›æ»š: {e}")
                    success_count = 0

        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch.batch_id} æ•°æ®åº“æ“ä½œå¤±è´¥: {e}")
            success_count = 0

        return success_count

    async def _check_and_repair_database(self, repository: SQLiteRepository, keyword: str) -> bool:
        """å¢å¼ºçš„æ•°æ®åº“å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤

        è‡ªåŠ¨æ£€æµ‹å¹¶ä¿®å¤ä»è¿è¡Œä¸­å¤åˆ¶çš„æ•°æ®åº“æ–‡ä»¶é—®é¢˜ï¼š
        1. WALæ–‡ä»¶çŠ¶æ€ä¸ä¸€è‡´
        2. æ–‡ä»¶é”å®šçŠ¶æ€
        3. äº‹åŠ¡çŠ¶æ€ä¸å®Œæ•´
        4. æ•°æ®åº“æŸåè‡ªåŠ¨æŠ¢æ•‘

        Args:
            repository: æ•°æ®åº“ä»“åº“
            keyword: å…³é”®è¯

        Returns:
            æ˜¯å¦ä¿®å¤æˆåŠŸ
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹å¢å¼ºæ•°æ®åº“å¥åº·æ£€æŸ¥: {keyword}")

            # æ­¥éª¤1ï¼šæ£€æµ‹æ•°æ®åº“æ–‡ä»¶çŠ¶æ€
            db_path = self._get_database_path(repository, keyword)
            if not self._detect_database_issues(db_path, keyword):
                logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ•°æ®åº“é—®é¢˜ï¼Œå¼€å§‹è‡ªåŠ¨ä¿®å¤: {keyword}")

                # è‡ªåŠ¨ä¿®å¤æŸåçš„æ•°æ®åº“
                success = await self._auto_repair_corrupted_database(db_path, keyword)
                if not success:
                    logger.error(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥: {keyword}")
                    return False

                logger.info(f"âœ… æ•°æ®åº“è‡ªåŠ¨ä¿®å¤æˆåŠŸ: {keyword}")

            # æ­¥éª¤2ï¼šå¼ºåˆ¶WALæ£€æŸ¥ç‚¹ï¼Œåˆå¹¶WALæ–‡ä»¶åˆ°ä¸»æ•°æ®åº“
            success = self._force_wal_checkpoint(repository, keyword)
            if not success:
                logger.warning(f"âš ï¸ WALæ£€æŸ¥ç‚¹å¤±è´¥ï¼Œå°è¯•æ·±åº¦ä¿®å¤: {keyword}")
                success = await self._auto_repair_corrupted_database(db_path, keyword)
                if not success:
                    return False

            # æ­¥éª¤3ï¼šæ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥
            success = self._integrity_check(repository, keyword)
            if not success:
                logger.warning(f"âš ï¸ å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ï¼Œå°è¯•æ•°æ®æŠ¢æ•‘: {keyword}")
                success = await self._auto_repair_corrupted_database(db_path, keyword)
                if not success:
                    return False

            # æ­¥éª¤4ï¼šä¼˜åŒ–æ•°æ®åº“ï¼ˆæ¸…ç†ç¢ç‰‡ï¼Œé‡å»ºç´¢å¼•ï¼‰
            success = self._optimize_database(repository, keyword)
            if not success:
                logger.warning(f"âš ï¸ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥ï¼Œä½†å¯ä»¥ç»§ç»­: {keyword}")

            logger.info(f"âœ… å¢å¼ºæ•°æ®åº“å¥åº·æ£€æŸ¥å®Œæˆ: {keyword}")
            return True

        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæ•°æ®åº“å¥åº·æ£€æŸ¥å¼‚å¸¸ {keyword}: {e}")
            # å°è¯•æœ€åçš„è‡ªåŠ¨ä¿®å¤
            try:
                db_path = self._get_database_path(repository, keyword)
                success = await self._auto_repair_corrupted_database(db_path, keyword)
                if success:
                    logger.info(f"âœ… å¼‚å¸¸æ¢å¤æˆåŠŸ: {keyword}")
                    return True
            except Exception as repair_e:
                logger.error(f"âŒ å¼‚å¸¸æ¢å¤ä¹Ÿå¤±è´¥ {keyword}: {repair_e}")

            return False

    def _get_database_path(self, repository: SQLiteRepository, keyword: str) -> str:
        """è·å–æ•°æ®åº“æ–‡ä»¶è·¯å¾„

        Args:
            repository: æ•°æ®åº“ä»“åº“
            keyword: å…³é”®è¯

        Returns:
            æ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        try:
            if repository.keyword and repository.output_dir:
                # ä½¿ç”¨å…³é”®è¯ç‰¹å®šçš„æ•°æ®åº“è·¯å¾„
                from ..core.database.manager_factory import DatabaseManagerFactory
                manager = DatabaseManagerFactory.get_manager(repository.keyword, repository.output_dir)
                return manager.db_path
            else:
                # å›é€€åˆ°é»˜è®¤è·¯å¾„æ„å»º
                import os
                return os.path.join(repository.output_dir or "./output", keyword, "pinterest.db")
        except Exception as e:
            logger.error(f"è·å–æ•°æ®åº“è·¯å¾„å¤±è´¥ {keyword}: {e}")
            # æœ€åçš„å›é€€æ–¹æ¡ˆ
            import os
            return os.path.join("./output", keyword, "pinterest.db")

    def _detect_database_issues(self, db_path: str, keyword: str) -> bool:
        """æ£€æµ‹æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨é—®é¢˜

        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            keyword: å…³é”®è¯

        Returns:
            Trueè¡¨ç¤ºæ•°æ®åº“æ­£å¸¸ï¼ŒFalseè¡¨ç¤ºå­˜åœ¨é—®é¢˜éœ€è¦ä¿®å¤
        """
        try:
            logger.debug(f"ğŸ” æ£€æµ‹æ•°æ®åº“æ–‡ä»¶çŠ¶æ€: {keyword}")

            # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(db_path):
                logger.warning(f"âš ï¸ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
                return True  # ä¸å­˜åœ¨å°±ä¸éœ€è¦ä¿®å¤

            # ã€æ–°å¢ã€‘æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ä¿®å¤å®Œæˆçš„æ–‡ä»¶
            repaired_ready_file = f"{db_path}.repaired_ready"
            if os.path.exists(repaired_ready_file):
                logger.info(f"âœ… æ£€æµ‹åˆ°å·²ä¿®å¤çš„æ•°æ®åº“æ–‡ä»¶: {keyword}")
                # å°è¯•ä½¿ç”¨ä¿®å¤å®Œæˆçš„æ–‡ä»¶æ›¿æ¢åŸæ–‡ä»¶
                try:
                    # å¤‡ä»½åŸæ–‡ä»¶
                    backup_path = f"{db_path}.replaced_backup"
                    if os.path.exists(backup_path):
                        os.remove(backup_path)
                    shutil.move(db_path, backup_path)

                    # ä½¿ç”¨ä¿®å¤å®Œæˆçš„æ–‡ä»¶
                    shutil.move(repaired_ready_file, db_path)
                    logger.info(f"âœ… å·²ä½¿ç”¨ä¿®å¤å®Œæˆçš„æ•°æ®åº“: {keyword}")
                    return True
                except Exception as e:
                    logger.warning(f"âš ï¸ ä½¿ç”¨ä¿®å¤æ–‡ä»¶å¤±è´¥: {e}")
                    # ç»§ç»­æ£€æŸ¥åŸæ–‡ä»¶

            # æ£€æŸ¥WALå’ŒSHMæ–‡ä»¶çŠ¶æ€
            wal_file = f"{db_path}-wal"
            shm_file = f"{db_path}-shm"

            wal_exists = os.path.exists(wal_file)
            shm_exists = os.path.exists(shm_file)

            if wal_exists or shm_exists:
                logger.debug(f"ğŸ” æ£€æµ‹åˆ°WAL/SHMæ–‡ä»¶: WAL={wal_exists}, SHM={shm_exists}")

                # æ£€æŸ¥WALæ–‡ä»¶å¤§å° - æé«˜é˜ˆå€¼ï¼Œé¿å…è¯¯åˆ¤
                if wal_exists:
                    wal_size = os.path.getsize(wal_file)
                    # æé«˜é˜ˆå€¼åˆ°10MBï¼Œé¿å…æ­£å¸¸ä½¿ç”¨ä¸­çš„WALæ–‡ä»¶è¢«è¯¯åˆ¤
                    if wal_size > 10 * 1024 * 1024:  # WALæ–‡ä»¶å¤§äº10MBæ‰è®¤ä¸ºæœ‰é—®é¢˜
                        logger.warning(f"âš ï¸ WALæ–‡ä»¶è¿‡å¤§: {wal_size:,} å­—èŠ‚")
                        return False
                    elif wal_size > 1024 * 1024:  # 1-10MBä¹‹é—´ç»™å‡ºæç¤ºä½†ä¸ä¿®å¤
                        logger.info(f"â„¹ï¸ WALæ–‡ä»¶è¾ƒå¤§ä½†æ­£å¸¸: {wal_size:,} å­—èŠ‚")

            # å°è¯•å¿«é€Ÿè¿æ¥æµ‹è¯•
            try:
                conn = sqlite3.connect(db_path, timeout=5.0)
                cursor = conn.cursor()

                # å¿«é€Ÿå®Œæ•´æ€§æ£€æŸ¥
                cursor.execute("PRAGMA quick_check")
                result = cursor.fetchone()

                conn.close()

                if result and result[0] == "ok":
                    logger.debug(f"âœ… æ•°æ®åº“å¿«é€Ÿæ£€æŸ¥é€šè¿‡: {keyword}")
                    return True
                else:
                    logger.warning(f"âš ï¸ æ•°æ®åº“å¿«é€Ÿæ£€æŸ¥å¤±è´¥: {result}")
                    return False

            except sqlite3.DatabaseError as e:
                logger.warning(f"âš ï¸ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
                return False
            except Exception as e:
                logger.warning(f"âš ï¸ æ•°æ®åº“æ£€æµ‹å¼‚å¸¸: {e}")
                return False

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“é—®é¢˜æ£€æµ‹å¼‚å¸¸ {keyword}: {e}")
            return False

    def _force_wal_checkpoint(self, repository: SQLiteRepository, keyword: str) -> bool:
        """å¼ºåˆ¶WALæ£€æŸ¥ç‚¹ï¼Œè§£å†³ä»è¿è¡Œä¸­å¤åˆ¶æ•°æ®åº“çš„WALçŠ¶æ€é—®é¢˜"""
        try:
            with repository._get_session() as session:
                # å¼ºåˆ¶WALæ£€æŸ¥ç‚¹ï¼Œå°†WALæ–‡ä»¶å†…å®¹åˆå¹¶åˆ°ä¸»æ•°æ®åº“
                from sqlalchemy import text
                result = session.execute(text("PRAGMA wal_checkpoint(FULL)"))
                checkpoint_result = result.fetchone()

                if checkpoint_result:
                    busy_count, log_size, checkpointed_size = checkpoint_result
                    logger.debug(f"ğŸ”§ WALæ£€æŸ¥ç‚¹å®Œæˆ {keyword}: busy={busy_count}, log_size={log_size}, checkpointed={checkpointed_size}")

                # ç¡®ä¿WALæ¨¡å¼ä»ç„¶å¯ç”¨
                session.execute(text("PRAGMA journal_mode=WAL"))

                return True

        except Exception as e:
            logger.error(f"âŒ WALæ£€æŸ¥ç‚¹å¤±è´¥ {keyword}: {e}")
            return False

    def _integrity_check(self, repository: SQLiteRepository, keyword: str) -> bool:
        """æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥"""
        try:
            with repository._get_session() as session:
                # å¿«é€Ÿå®Œæ•´æ€§æ£€æŸ¥
                from sqlalchemy import text
                result = session.execute(text("PRAGMA quick_check"))
                check_result = result.fetchone()

                if check_result and check_result[0] == "ok":
                    logger.debug(f"âœ… æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥é€šè¿‡: {keyword}")
                    return True
                else:
                    logger.error(f"âŒ æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¤±è´¥ {keyword}: {check_result}")

                    # å°è¯•å®Œæ•´æ£€æŸ¥è·å–æ›´å¤šä¿¡æ¯
                    result = session.execute(text("PRAGMA integrity_check"))
                    full_check = result.fetchall()
                    logger.error(f"å®Œæ•´æ€§æ£€æŸ¥è¯¦æƒ…: {full_check}")
                    return False

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“å®Œæ•´æ€§æ£€æŸ¥å¼‚å¸¸ {keyword}: {e}")
            return False

    def _optimize_database(self, repository: SQLiteRepository, keyword: str) -> bool:
        """ä¼˜åŒ–æ•°æ®åº“ï¼ˆæ¸…ç†ç¢ç‰‡ï¼Œé‡å»ºç´¢å¼•ï¼‰"""
        try:
            with repository._get_session() as session:
                # åˆ†ææ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
                from sqlalchemy import text
                session.execute(text("ANALYZE"))

                # æ¸…ç†æ•°æ®åº“ç¢ç‰‡ï¼ˆå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰
                logger.debug(f"ğŸ”§ ä¼˜åŒ–æ•°æ®åº“ç¢ç‰‡: {keyword}")
                session.execute(text("VACUUM"))

                logger.debug(f"âœ… æ•°æ®åº“ä¼˜åŒ–å®Œæˆ: {keyword}")
                return True

        except Exception as e:
            logger.warning(f"âš ï¸ æ•°æ®åº“ä¼˜åŒ–å¤±è´¥ {keyword}: {e}")
            # ä¼˜åŒ–å¤±è´¥ä¸æ˜¯è‡´å‘½é”™è¯¯ï¼Œå¯ä»¥ç»§ç»­å¤„ç†
            return False

    async def _auto_repair_corrupted_database(self, db_path: str, keyword: str) -> bool:
        """è‡ªåŠ¨ä¿®å¤æŸåçš„æ•°æ®åº“

        é›†æˆæ•°æ®æŠ¢æ•‘é€»è¾‘ï¼Œè‡ªåŠ¨ä¿®å¤æŸåçš„æ•°æ®åº“æ–‡ä»¶

        Args:
            db_path: æ•°æ®åº“æ–‡ä»¶è·¯å¾„
            keyword: å…³é”®è¯

        Returns:
            æ˜¯å¦ä¿®å¤æˆåŠŸ
        """
        try:
            logger.info(f"ğŸš‘ å¼€å§‹è‡ªåŠ¨ä¿®å¤æŸåçš„æ•°æ®åº“: {keyword}")
            logger.info(f"ğŸ“ æ•°æ®åº“è·¯å¾„: {db_path}")

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(db_path):
                logger.error(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
                return False

            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºä¿®å¤æ“ä½œ
            temp_dir = tempfile.mkdtemp(prefix=f"auto_repair_{keyword}_")
            logger.info(f"ğŸ“ ä¸´æ—¶ä¿®å¤ç›®å½•: {temp_dir}")

            try:
                # æ­¥éª¤0ï¼šå¼ºåˆ¶å…³é—­æ‰€æœ‰ç°æœ‰è¿æ¥
                await self._force_close_all_connections(keyword)

                # æ­¥éª¤1ï¼šå¤šé‡å¤‡ä»½æŸåçš„æ•°æ®åº“
                backup_success = self._create_multiple_backups(db_path, keyword)
                if not backup_success:
                    logger.error(f"âŒ å¤‡ä»½å¤±è´¥ï¼Œåœæ­¢ä¿®å¤: {keyword}")
                    return False

                # æ­¥éª¤2ï¼šè‡ªåŠ¨æ•°æ®æŠ¢æ•‘
                recovered_db_path = os.path.join(temp_dir, "recovered.db")
                rescued_count = await self._auto_rescue_data(db_path, recovered_db_path, keyword)

                if rescued_count == 0:
                    logger.error(f"âŒ æ²¡æœ‰æŠ¢æ•‘åˆ°ä»»ä½•æ•°æ®: {keyword}")
                    return False

                logger.info(f"âœ… æ•°æ®æŠ¢æ•‘æˆåŠŸ: {keyword}, æŠ¢æ•‘äº† {rescued_count:,} æ¡è®°å½•")

                # æ­¥éª¤3ï¼šå†æ¬¡å¼ºåˆ¶å…³é—­è¿æ¥
                await self._force_close_all_connections(keyword)

                # æ­¥éª¤4ï¼šå¯¹æŠ¢æ•‘çš„æ•°æ®æ‰§è¡ŒBase64è½¬æ¢
                logger.info(f"ğŸ”„ å¼€å§‹å¯¹æŠ¢æ•‘çš„æ•°æ®æ‰§è¡ŒBase64è½¬æ¢: {keyword}")
                converted_db_path = await self._convert_rescued_data_base64(recovered_db_path, keyword)

                if not converted_db_path:
                    logger.error(f"âŒ æŠ¢æ•‘æ•°æ®Base64è½¬æ¢å¤±è´¥: {keyword}")
                    return False

                # æ­¥éª¤5ï¼šåˆ›å»ºå¹²å‡€çš„æ–°æ•°æ®åº“
                new_db_path = f"{db_path}.repaired"
                create_success = self._create_clean_database_auto(converted_db_path, new_db_path, keyword)

                if not create_success:
                    logger.error(f"âŒ åˆ›å»ºæ–°æ•°æ®åº“å¤±è´¥: {keyword}")
                    return False

                # æ­¥éª¤5ï¼šæœ€ç»ˆå¼ºåˆ¶å…³é—­è¿æ¥
                await self._force_close_all_connections(keyword)

                # æ­¥éª¤6ï¼šå®‰å…¨æ›¿æ¢æ•°æ®åº“æ–‡ä»¶
                replace_success = await self._safe_replace_database(db_path, new_db_path, keyword)

                if not replace_success:
                    logger.error(f"âŒ æ•°æ®åº“æ›¿æ¢å¤±è´¥: {keyword}")
                    return False

                logger.info(f"âœ… æ•°æ®åº“è‡ªåŠ¨ä¿®å¤å®Œæˆ: {keyword}")

                # ã€æ–°å¢ã€‘æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿WALå’ŒSHMæ–‡ä»¶è¢«å®Œå…¨ç§»é™¤
                await self._final_cleanup_auxiliary_files(db_path, keyword)

                return True

            finally:
                # æ¸…ç†ä¸´æ—¶ç›®å½•
                try:
                    shutil.rmtree(temp_dir)
                    logger.debug(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•: {temp_dir}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶ç›®å½•å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨ä¿®å¤æ•°æ®åº“å¼‚å¸¸ {keyword}: {e}")
            return False

    def _create_multiple_backups(self, db_path: str, keyword: str) -> bool:
        """åˆ›å»ºå¤šé‡å¤‡ä»½ç¡®ä¿æ•°æ®å®‰å…¨"""
        try:
            logger.info(f"ğŸ’¾ åˆ›å»ºå¤šé‡å¤‡ä»½: {keyword}")

            # å¤‡ä»½1ï¼šæŸåæ•°æ®åº“å¤‡ä»½
            backup1_path = f"{db_path}.corrupted_backup"
            shutil.copy2(db_path, backup1_path)
            logger.info(f"ğŸ’¾ æŸåæ•°æ®åº“å¤‡ä»½: {backup1_path}")

            # å¤‡ä»½2ï¼šæ—¶é—´æˆ³å¤‡ä»½
            import datetime
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            backup2_path = f"{db_path}.backup_{timestamp}"
            shutil.copy2(db_path, backup2_path)
            logger.info(f"ğŸ’¾ æ—¶é—´æˆ³å¤‡ä»½: {backup2_path}")

            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥ {keyword}: {e}")
            return True

    async def _auto_rescue_data(self, corrupted_db_path: str, recovered_db_path: str, keyword: str) -> int:
        """è‡ªåŠ¨æŠ¢æ•‘æ•°æ®ï¼ˆå¸¦è¿›åº¦æ˜¾ç¤ºï¼‰

        Args:
            corrupted_db_path: æŸåçš„æ•°æ®åº“è·¯å¾„
            recovered_db_path: æ¢å¤çš„æ•°æ®åº“è·¯å¾„
            keyword: å…³é”®è¯

        Returns:
            æŠ¢æ•‘çš„è®°å½•æ•°é‡
        """
        try:
            logger.info(f"ğŸš‘ å¼€å§‹è‡ªåŠ¨æ•°æ®æŠ¢æ•‘: {keyword}")

            # åˆ›å»ºæ–°æ•°æ®åº“ç»“æ„
            recovered_conn = sqlite3.connect(recovered_db_path, timeout=60.0)
            recovered_cursor = recovered_conn.cursor()

            # åˆ›å»ºpinsè¡¨ç»“æ„
            recovered_cursor.execute("""
            CREATE TABLE IF NOT EXISTS pins (
                id TEXT PRIMARY KEY,
                pin_hash TEXT,
                title TEXT,
                description TEXT,
                creator_name TEXT,
                creator_id TEXT,
                board_name TEXT,
                board_id TEXT,
                image_urls TEXT,
                largest_image_url TEXT,
                stats TEXT,
                raw_data TEXT,
                query TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
            """)

            # å°è¯•ä»æŸåçš„æ•°æ®åº“ä¸­è¯»å–æ•°æ®
            try:
                corrupted_conn = sqlite3.connect(corrupted_db_path, timeout=30.0)
                corrupted_cursor = corrupted_conn.cursor()

                # å…ˆç»Ÿè®¡æ€»è®°å½•æ•°ï¼ˆç”¨äºè¿›åº¦æ˜¾ç¤ºï¼‰
                try:
                    corrupted_cursor.execute("SELECT COUNT(*) FROM pins")
                    total_count = corrupted_cursor.fetchone()[0]
                    logger.info(f"ğŸ“Š é¢„è®¡æŠ¢æ•‘è®°å½•æ•°: {total_count:,}")
                except:
                    total_count = 0
                    logger.warning("âš ï¸ æ— æ³•ç»Ÿè®¡æ€»è®°å½•æ•°ï¼Œä½¿ç”¨é€è¡ŒæŠ¢æ•‘æ¨¡å¼")

                # å¼€å§‹æŠ¢æ•‘æ•°æ®
                logger.info(f"ğŸš‘ å¼€å§‹é€è¡ŒæŠ¢æ•‘æ•°æ®: {keyword}")

                # å°è¯•è¯»å–pinsè¡¨çš„æ•°æ®
                corrupted_cursor.execute("SELECT * FROM pins")

                rescued_count = 0
                batch_size = 1000
                batch_data = []

                # åˆ›å»ºè¿›åº¦æ˜¾ç¤º
                if total_count > 0:
                    progress_desc = f"æŠ¢æ•‘æ•°æ®{keyword}"
                    with WindowsProgressDisplay(
                        total=total_count,
                        desc=progress_desc,
                        unit="record"
                    ) as progress:

                        while True:
                            try:
                                row = corrupted_cursor.fetchone()
                                if row is None:
                                    break

                                batch_data.append(row)
                                rescued_count += 1

                                # æ‰¹é‡æ’å…¥æé«˜æ€§èƒ½
                                if len(batch_data) >= batch_size:
                                    self._batch_insert_rescued_data(recovered_cursor, batch_data)
                                    batch_data = []
                                    recovered_conn.commit()
                                    progress.update(batch_size)

                            except Exception as e:
                                # è·³è¿‡æŸåçš„è¡Œ
                                logger.debug(f"è·³è¿‡æŸåçš„è¡Œ: {e}")
                                continue

                        # æ’å…¥å‰©ä½™æ•°æ®
                        if batch_data:
                            self._batch_insert_rescued_data(recovered_cursor, batch_data)
                            recovered_conn.commit()
                            progress.update(len(batch_data))
                else:
                    # æ— è¿›åº¦æ¡æ¨¡å¼
                    while True:
                        try:
                            row = corrupted_cursor.fetchone()
                            if row is None:
                                break

                            batch_data.append(row)
                            rescued_count += 1

                            # æ‰¹é‡æ’å…¥
                            if len(batch_data) >= batch_size:
                                self._batch_insert_rescued_data(recovered_cursor, batch_data)
                                batch_data = []
                                recovered_conn.commit()

                                if rescued_count % 5000 == 0:
                                    logger.info(f"ğŸ“Š å·²æŠ¢æ•‘ {rescued_count:,} æ¡è®°å½•...")

                        except Exception as e:
                            logger.debug(f"è·³è¿‡æŸåçš„è¡Œ: {e}")
                            continue

                    # æ’å…¥å‰©ä½™æ•°æ®
                    if batch_data:
                        self._batch_insert_rescued_data(recovered_cursor, batch_data)
                        recovered_conn.commit()

                corrupted_conn.close()
                recovered_conn.close()

                logger.info(f"âœ… æ•°æ®æŠ¢æ•‘å®Œæˆ: {keyword}, æˆåŠŸæŠ¢æ•‘ {rescued_count:,} æ¡è®°å½•")
                return rescued_count

            except Exception as e:
                logger.error(f"âŒ æ•°æ®æŠ¢æ•‘å¤±è´¥ {keyword}: {e}")
                recovered_conn.close()
                return 0

        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ•°æ®æŠ¢æ•‘å¼‚å¸¸ {keyword}: {e}")
            return 0

    def _batch_insert_rescued_data(self, cursor, batch_data: List) -> None:
        """æ‰¹é‡æ’å…¥æŠ¢æ•‘çš„æ•°æ®"""
        try:
            cursor.executemany("""
            INSERT OR REPLACE INTO pins VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, batch_data)
        except Exception as e:
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€è¡Œæ’å…¥
            logger.debug(f"æ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€è¡Œæ’å…¥: {e}")
            for row in batch_data:
                try:
                    cursor.execute("""
                    INSERT OR REPLACE INTO pins VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, row)
                except Exception as row_e:
                    logger.debug(f"è·³è¿‡æŸåçš„è¡Œ: {row_e}")
                    continue

    def _create_clean_database_auto(self, recovered_db_path: str, new_db_path: str, keyword: str) -> bool:
        """è‡ªåŠ¨åˆ›å»ºå¹²å‡€çš„æ–°æ•°æ®åº“"""
        try:
            logger.info(f"ğŸ”§ åˆ›å»ºå¹²å‡€çš„æ–°æ•°æ®åº“: {keyword}")

            # å¤åˆ¶æŠ¢æ•‘çš„æ•°æ®åº“
            shutil.copy2(recovered_db_path, new_db_path)

            # ä¼˜åŒ–æ–°æ•°æ®åº“
            conn = sqlite3.connect(new_db_path, timeout=60.0)
            cursor = conn.cursor()

            # è®¾ç½®å®‰å…¨çš„PRAGMA
            cursor.execute("PRAGMA journal_mode=WAL")
            cursor.execute("PRAGMA synchronous=FULL")
            cursor.execute("PRAGMA busy_timeout=30000")
            cursor.execute("PRAGMA wal_autocheckpoint=1000")

            # é‡å»ºç´¢å¼•
            cursor.execute("REINDEX")

            # åˆ†ææ•°æ®åº“
            cursor.execute("ANALYZE")

            # æ¸…ç†ç¢ç‰‡
            logger.info(f"ğŸ”§ æ¸…ç†æ•°æ®åº“ç¢ç‰‡: {keyword}")
            cursor.execute("VACUUM")

            conn.commit()
            conn.close()

            logger.info(f"âœ… å¹²å‡€æ•°æ®åº“åˆ›å»ºå®Œæˆ: {keyword}")
            return True

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºå¹²å‡€æ•°æ®åº“å¤±è´¥ {keyword}: {e}")
            return False

    async def _safe_replace_database(self, original_db_path: str, new_db_path: str, keyword: str) -> bool:
        """å®‰å…¨æ›¿æ¢æ•°æ®åº“æ–‡ä»¶ï¼ˆå¢å¼ºçš„Windowsæ–‡ä»¶é”å®šå¤„ç†ï¼‰"""
        try:
            logger.info(f"ğŸ”„ å®‰å…¨æ›¿æ¢æ•°æ®åº“æ–‡ä»¶: {keyword}")

            # å°è¯•å¤šæ¬¡æ›¿æ¢ï¼Œå¤„ç†æ–‡ä»¶é”å®šé—®é¢˜
            max_retries = 8  # å¢åŠ é‡è¯•æ¬¡æ•°
            retry_delay = 1.0

            for attempt in range(max_retries):
                try:
                    # å¼ºåˆ¶å…³é—­æ‰€æœ‰å¯èƒ½çš„è¿æ¥
                    await self._force_close_all_connections(keyword)

                    # åˆ é™¤WALå’ŒSHMæ–‡ä»¶
                    wal_file = f"{original_db_path}-wal"
                    shm_file = f"{original_db_path}-shm"
                    journal_file = f"{original_db_path}-journal"

                    for aux_file in [wal_file, shm_file, journal_file]:
                        if os.path.exists(aux_file):
                            try:
                                # å°è¯•å¤šæ¬¡åˆ é™¤è¾…åŠ©æ–‡ä»¶
                                for del_attempt in range(3):
                                    try:
                                        os.remove(aux_file)
                                        logger.debug(f"ğŸ—‘ï¸ åˆ é™¤è¾…åŠ©æ–‡ä»¶: {aux_file}")
                                        break
                                    except Exception as del_e:
                                        if del_attempt < 2:
                                            await asyncio.sleep(0.5)
                                        else:
                                            logger.debug(f"æ— æ³•åˆ é™¤è¾…åŠ©æ–‡ä»¶ {aux_file}: {del_e}")
                            except Exception as e:
                                logger.debug(f"åˆ é™¤è¾…åŠ©æ–‡ä»¶å¤±è´¥ {aux_file}: {e}")

                    # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    import gc
                    gc.collect()

                    # ç­‰å¾…æ–‡ä»¶é”é‡Šæ”¾
                    await asyncio.sleep(retry_delay)

                    # å°è¯•æµ‹è¯•æ–‡ä»¶æ˜¯å¦å¯ä»¥è®¿é—®
                    try:
                        # å°è¯•ä»¥ç‹¬å æ¨¡å¼æ‰“å¼€æ–‡ä»¶æ¥æµ‹è¯•é”å®šçŠ¶æ€
                        with open(original_db_path, 'r+b') as test_file:
                            pass
                        logger.debug(f"âœ… æ–‡ä»¶é”å®šæµ‹è¯•é€šè¿‡: {keyword}")
                    except Exception as lock_test_e:
                        logger.debug(f"æ–‡ä»¶ä»è¢«é”å®š: {lock_test_e}")
                        raise lock_test_e

                    # å¤‡ä»½åŸæ•°æ®åº“
                    backup_path = f"{original_db_path}.replaced_backup"
                    if os.path.exists(backup_path):
                        os.remove(backup_path)

                    shutil.move(original_db_path, backup_path)
                    logger.info(f"ğŸ’¾ åŸæ•°æ®åº“å·²å¤‡ä»½: {backup_path}")

                    # ç§»åŠ¨æ–°æ•°æ®åº“åˆ°åŸä½ç½®
                    shutil.move(new_db_path, original_db_path)
                    logger.info(f"âœ… æ•°æ®åº“æ›¿æ¢æˆåŠŸ: {keyword}")

                    # éªŒè¯æ›¿æ¢ç»“æœ
                    if os.path.exists(original_db_path):
                        file_size = os.path.getsize(original_db_path)
                        logger.info(f"ğŸ“Š æ–°æ•°æ®åº“æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
                        return True
                    else:
                        logger.error(f"âŒ æ›¿æ¢åæ–‡ä»¶ä¸å­˜åœ¨: {original_db_path}")
                        return False

                except Exception as e:
                    logger.warning(f"âš ï¸ æ›¿æ¢å°è¯• {attempt + 1}/{max_retries} å¤±è´¥: {e}")
                    if attempt < max_retries - 1:
                        logger.info(f"ğŸ”„ ç­‰å¾… {retry_delay} ç§’åé‡è¯•...")
                        await asyncio.sleep(retry_delay)
                        retry_delay = min(retry_delay * 1.5, 10.0)  # æŒ‡æ•°é€€é¿ï¼Œæœ€å¤§10ç§’
                    else:
                        logger.error(f"âŒ æ‰€æœ‰æ›¿æ¢å°è¯•éƒ½å¤±è´¥: {keyword}")
                        # æœ€åå°è¯•ï¼šå°†ä¿®å¤çš„æ•°æ®åº“ä¿å­˜ä¸ºå¤‡ç”¨æ–‡ä»¶
                        try:
                            fallback_path = f"{original_db_path}.repaired_ready"
                            shutil.copy2(new_db_path, fallback_path)
                            logger.info(f"ğŸ’¾ ä¿®å¤çš„æ•°æ®åº“å·²ä¿å­˜ä¸º: {fallback_path}")
                            logger.info(f"ğŸ”§ è¯·æ‰‹åŠ¨æ›¿æ¢æ•°æ®åº“æ–‡ä»¶æˆ–é‡å¯ç¨‹åº")
                        except Exception as fallback_e:
                            logger.error(f"âŒ ä¿å­˜å¤‡ç”¨æ–‡ä»¶ä¹Ÿå¤±è´¥: {fallback_e}")
                        return False

            return False

        except Exception as e:
            logger.error(f"âŒ å®‰å…¨æ›¿æ¢æ•°æ®åº“å¼‚å¸¸ {keyword}: {e}")
            return False

    async def _force_close_all_connections(self, keyword: str) -> None:
        """å¼ºåˆ¶å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥

        Args:
            keyword: å…³é”®è¯
        """
        try:
            logger.debug(f"ğŸ”’ å¼ºåˆ¶å…³é—­æ‰€æœ‰æ•°æ®åº“è¿æ¥: {keyword}")

            # 1. æ¸…ç†DatabaseManagerFactoryä¸­çš„ç¼“å­˜è¿æ¥
            try:
                from ..core.database.manager_factory import DatabaseManagerFactory

                # ä½¿ç”¨æ­£ç¡®çš„æ¸…ç†æ–¹æ³•
                cleanup_success = DatabaseManagerFactory.cleanup_manager(keyword, self.output_dir)
                if cleanup_success:
                    logger.debug(f"ğŸ”’ æ¸…ç†ç®¡ç†å™¨ç¼“å­˜æˆåŠŸ: {keyword}")
                else:
                    logger.debug(f"ğŸ”’ æ¸…ç†ç®¡ç†å™¨ç¼“å­˜å¤±è´¥: {keyword}")

            except Exception as e:
                logger.debug(f"æ¸…ç†ç®¡ç†å™¨ç¼“å­˜å¤±è´¥: {e}")

            # 2. å¼ºåˆ¶åƒåœ¾å›æ”¶ï¼Œé‡Šæ”¾æœªå…³é—­çš„è¿æ¥
            import gc
            gc.collect()

            # 3. å¼ºåˆ¶æ‰§è¡ŒWALæ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®å†™å…¥ä¸»æ–‡ä»¶
            try:
                db_path = os.path.join(self.output_dir, keyword, "pinterest.db")
                if os.path.exists(db_path):
                    # åˆ›å»ºä¸´æ—¶è¿æ¥æ‰§è¡Œæ£€æŸ¥ç‚¹
                    temp_conn = sqlite3.connect(db_path, timeout=5.0)
                    temp_cursor = temp_conn.cursor()
                    temp_cursor.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                    temp_conn.close()
                    logger.debug(f"ğŸ”„ å¼ºåˆ¶WALæ£€æŸ¥ç‚¹å®Œæˆ: {keyword}")
            except Exception as e:
                logger.debug(f"å¼ºåˆ¶WALæ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            # 4. ç­‰å¾…æ›´é•¿æ—¶é—´è®©è¿æ¥å®Œå…¨é‡Šæ”¾
            await asyncio.sleep(2.0)

            logger.debug(f"âœ… æ•°æ®åº“è¿æ¥å¼ºåˆ¶å…³é—­å®Œæˆ: {keyword}")

        except Exception as e:
            logger.warning(f"âš ï¸ å¼ºåˆ¶å…³é—­è¿æ¥æ—¶å‡ºé”™ {keyword}: {e}")

    async def _final_cleanup_auxiliary_files(self, db_path: str, keyword: str) -> None:
        """æœ€ç»ˆæ¸…ç†ï¼šç¡®ä¿WALå’ŒSHMæ–‡ä»¶è¢«å®Œå…¨ç§»é™¤ï¼Œé¿å…ä¸‹æ¬¡å¯åŠ¨è¯¯åˆ¤"""
        try:
            logger.info(f"ğŸ§¹ å¼€å§‹æœ€ç»ˆæ¸…ç†è¾…åŠ©æ–‡ä»¶: {keyword}")

            # å¼ºåˆ¶WALæ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å†™å…¥ä¸»æ–‡ä»¶
            try:
                conn = sqlite3.connect(db_path, timeout=10.0)
                cursor = conn.cursor()
                cursor.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                result = cursor.fetchone()
                if result:
                    busy_count, log_size, checkpointed_size = result
                    logger.debug(f"æœ€ç»ˆWALæ£€æŸ¥ç‚¹: busy={busy_count}, log_size={log_size}, checkpointed={checkpointed_size}")
                conn.close()
                logger.debug(f"âœ… æœ€ç»ˆWALæ£€æŸ¥ç‚¹å®Œæˆ: {keyword}")
            except Exception as e:
                logger.warning(f"âš ï¸ æœ€ç»ˆWALæ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            # ç­‰å¾…è¿æ¥å®Œå…¨é‡Šæ”¾
            await asyncio.sleep(1.0)

            # åˆ é™¤æ‰€æœ‰è¾…åŠ©æ–‡ä»¶
            auxiliary_files = [
                f"{db_path}-wal",
                f"{db_path}-shm",
                f"{db_path}-journal"
            ]

            for aux_file in auxiliary_files:
                if os.path.exists(aux_file):
                    try:
                        # å¤šæ¬¡å°è¯•åˆ é™¤
                        for attempt in range(5):
                            try:
                                os.remove(aux_file)
                                logger.info(f"ğŸ—‘ï¸ å·²åˆ é™¤è¾…åŠ©æ–‡ä»¶: {os.path.basename(aux_file)}")
                                break
                            except Exception as del_e:
                                if attempt < 4:
                                    await asyncio.sleep(0.5)
                                else:
                                    logger.warning(f"âš ï¸ æ— æ³•åˆ é™¤è¾…åŠ©æ–‡ä»¶ {aux_file}: {del_e}")
                    except Exception as e:
                        logger.warning(f"âš ï¸ åˆ é™¤è¾…åŠ©æ–‡ä»¶å¤±è´¥ {aux_file}: {e}")

            logger.info(f"âœ… æœ€ç»ˆæ¸…ç†å®Œæˆ: {keyword} - åªä¿ç•™ pinterest.db")

        except Exception as e:
            logger.warning(f"âš ï¸ æœ€ç»ˆæ¸…ç†è¾…åŠ©æ–‡ä»¶æ—¶å‡ºé”™ {keyword}: {e}")

    async def _convert_rescued_data_base64(self, rescued_db_path: str, keyword: str) -> str:
        """å¯¹æŠ¢æ•‘çš„æ•°æ®æ‰§è¡ŒBase64è½¬æ¢ï¼ˆä¿®å¤UNIQUE constrainté—®é¢˜ï¼‰

        Args:
            rescued_db_path: æŠ¢æ•‘çš„æ•°æ®åº“è·¯å¾„
            keyword: å…³é”®è¯

        Returns:
            è½¬æ¢åçš„æ•°æ®åº“è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        try:
            logger.info(f"ğŸ”„ å¼€å§‹Base64è½¬æ¢æŠ¢æ•‘çš„æ•°æ®: {keyword}")

            # è¿æ¥æŠ¢æ•‘çš„æ•°æ®åº“
            conn = sqlite3.connect(rescued_db_path, timeout=60.0)
            cursor = conn.cursor()

            # æ¿€è¿›ä¼˜åŒ–ï¼šè®¾ç½®SQLiteæ€§èƒ½ä¼˜åŒ–å‚æ•°ä»¥å®ç°2å€æ€§èƒ½æå‡
            cursor.execute("PRAGMA synchronous = NORMAL")      # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨æ€§
            cursor.execute("PRAGMA cache_size = -64000")       # 64MBç¼“å­˜
            cursor.execute("PRAGMA temp_store = MEMORY")       # ä¸´æ—¶è¡¨å­˜å‚¨åœ¨å†…å­˜
            cursor.execute("PRAGMA mmap_size = 268435456")     # 256MBå†…å­˜æ˜ å°„
            cursor.execute("PRAGMA optimize")                  # ä¼˜åŒ–æŸ¥è¯¢è®¡åˆ’

            # ç»Ÿè®¡éœ€è¦è½¬æ¢çš„base64ç¼–ç Pin
            cursor.execute("SELECT COUNT(*) FROM pins WHERE id LIKE 'UGlu%'")
            base64_count = cursor.fetchone()[0]

            if base64_count == 0:
                logger.info(f"âœ… æŠ¢æ•‘çš„æ•°æ®æ— éœ€Base64è½¬æ¢: {keyword}")
                conn.close()
                return rescued_db_path

            logger.info(f"ğŸ“Š å‘ç° {base64_count:,} ä¸ªbase64ç¼–ç Pinéœ€è¦è½¬æ¢")

            # é˜¶æ®µ1ï¼šåˆ†æå’Œå»é‡å†²çªçš„base64ç¼–ç 
            deduped_count = await self._deduplicate_base64_pins(cursor, keyword)
            logger.info(f"ğŸ”§ å»é‡å®Œæˆ: åˆ é™¤äº† {deduped_count} ä¸ªé‡å¤çš„base64ç¼–ç Pin")

            # é˜¶æ®µ2ï¼šæ‰§è¡Œå®‰å…¨çš„æ‰¹é‡è½¬æ¢
            converted_count = await self._safe_batch_conversion(cursor, keyword)

            # å¼ºåˆ¶å®Œæˆæ‰€æœ‰äº‹åŠ¡å¹¶å…³é—­è¿æ¥
            try:
                # ç¡®ä¿æ‰€æœ‰äº‹åŠ¡éƒ½å·²æäº¤
                cursor.execute("COMMIT")
            except:
                pass  # å¦‚æœæ²¡æœ‰æ´»åŠ¨äº‹åŠ¡ï¼Œå¿½ç•¥é”™è¯¯

            try:
                # æ‰§è¡ŒWALæ£€æŸ¥ç‚¹ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®å†™å…¥ä¸»æ•°æ®åº“æ–‡ä»¶
                cursor.execute("PRAGMA wal_checkpoint(FULL)")
                logger.debug(f"âœ… WALæ£€æŸ¥ç‚¹å®Œæˆ: {keyword}")
            except Exception as e:
                logger.debug(f"WALæ£€æŸ¥ç‚¹å¤±è´¥: {e}")

            # å…³é—­è¿æ¥
            conn.close()

            # é¢å¤–ç­‰å¾…ç¡®ä¿è¿æ¥å®Œå…¨é‡Šæ”¾
            await asyncio.sleep(0.5)

            logger.info(f"âœ… Base64è½¬æ¢å®Œæˆ: {keyword}, è½¬æ¢äº† {converted_count:,} ä¸ªPin")
            return rescued_db_path

        except Exception as e:
            logger.error(f"âŒ Base64è½¬æ¢æŠ¢æ•‘æ•°æ®å¤±è´¥ {keyword}: {e}")
            return None

    async def _deduplicate_base64_pins(self, cursor, keyword: str) -> int:
        """å»é‡å†²çªçš„base64ç¼–ç Pin

        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            keyword: å…³é”®è¯

        Returns:
            åˆ é™¤çš„é‡å¤è®°å½•æ•°é‡
        """
        try:
            logger.info(f"ğŸ”§ å¼€å§‹åˆ†æå’Œå»é‡base64ç¼–ç Pin: {keyword}")

            # è·å–æ‰€æœ‰base64ç¼–ç çš„Pin
            cursor.execute("SELECT id, pin_hash, created_at FROM pins WHERE id LIKE 'UGlu%' ORDER BY created_at DESC")
            base64_pins = cursor.fetchall()

            # åˆ†æè§£ç å†²çª
            from collections import defaultdict
            conflicts = defaultdict(list)

            for pin_id, pin_hash, created_at in base64_pins:
                try:
                    # è§£ç base64 Pin ID
                    decoded_bytes = base64.b64decode(pin_id)
                    decoded_str = decoded_bytes.decode('utf-8')

                    if decoded_str.startswith('Pin:'):
                        # æ­£ç¡®å¤„ç†ç©ºæ ¼å’Œæ ¼å¼é—®é¢˜
                        real_pin_id = decoded_str[4:].strip()
                        conflicts[real_pin_id].append((pin_id, pin_hash, created_at))

                except Exception as e:
                    logger.debug(f"è·³è¿‡æ— æ•ˆçš„base64 Pin: {pin_id}, é”™è¯¯: {e}")
                    continue

            # å¤„ç†å†²çªï¼šä¿ç•™æœ€æ–°çš„è®°å½•ï¼Œåˆ é™¤é‡å¤çš„
            deleted_count = 0
            for real_pin_id, pin_records in conflicts.items():
                if len(pin_records) > 1:
                    logger.warning(f"å‘ç°å†²çª: çœŸå®ID '{real_pin_id}' å¯¹åº” {len(pin_records)} ä¸ªbase64ç¼–ç ")

                    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
                    pin_records.sort(key=lambda x: x[2], reverse=True)  # æŒ‰created_até™åº
                    keep_record = pin_records[0]
                    delete_records = pin_records[1:]

                    logger.info(f"ä¿ç•™è®°å½•: {keep_record[0]} (æœ€æ–°)")

                    # åˆ é™¤é‡å¤è®°å½•
                    for pin_id, pin_hash, created_at in delete_records:
                        cursor.execute("DELETE FROM pins WHERE id = ?", (pin_id,))
                        deleted_count += 1
                        logger.debug(f"åˆ é™¤é‡å¤è®°å½•: {pin_id}")

            # æäº¤åˆ é™¤æ“ä½œ
            cursor.connection.commit()

            logger.info(f"âœ… å»é‡å®Œæˆ: åˆ é™¤äº† {deleted_count} ä¸ªé‡å¤è®°å½•")
            return deleted_count

        except Exception as e:
            logger.error(f"âŒ å»é‡å¤±è´¥ {keyword}: {e}")
            cursor.connection.rollback()
            return 0

    async def _safe_batch_conversion(self, cursor, keyword: str) -> int:
        """å®‰å…¨çš„æ‰¹é‡Base64è½¬æ¢

        Args:
            cursor: æ•°æ®åº“æ¸¸æ ‡
            keyword: å…³é”®è¯

        Returns:
            è½¬æ¢çš„è®°å½•æ•°é‡
        """
        try:
            logger.info(f"ğŸ”„ å¼€å§‹å®‰å…¨æ‰¹é‡è½¬æ¢: {keyword}")

            # é‡æ–°è·å–å»é‡åçš„base64ç¼–ç Pin
            cursor.execute("SELECT id, pin_hash FROM pins WHERE id LIKE 'UGlu%'")
            base64_pins = cursor.fetchall()

            if not base64_pins:
                logger.info(f"âœ… æ²¡æœ‰éœ€è¦è½¬æ¢çš„base64ç¼–ç Pin: {keyword}")
                return 0

            converted_count = 0
            # æ¿€è¿›ä¼˜åŒ–ï¼šæ ¹æ®æ•°æ®é›†å¤§å°åŠ¨æ€è°ƒæ•´æäº¤æ‰¹æ¬¡å¤§å°
            if len(base64_pins) >= 10000:
                commit_batch_size = 1000  # å¤§æ•°æ®é›†ï¼šæ¯1000ä¸ªPinæäº¤ä¸€æ¬¡
            elif len(base64_pins) >= 5000:
                commit_batch_size = 750   # ä¸­ç­‰æ•°æ®é›†ï¼šæ¯750ä¸ªPinæäº¤ä¸€æ¬¡
            else:
                commit_batch_size = 500   # å°æ•°æ®é›†ï¼šæ¯500ä¸ªPinæäº¤ä¸€æ¬¡
            pending_commits = 0

            with WindowsProgressDisplay(
                total=len(base64_pins),
                desc=f"å®‰å…¨è½¬æ¢{keyword}",
                unit="pin"
            ) as progress:

                # é€ä¸ªè½¬æ¢ä»¥é¿å…æ‰¹é‡å†²çª
                for pin_id, pin_hash in base64_pins:
                    try:
                        # è§£ç base64 Pin ID
                        decoded_bytes = base64.b64decode(pin_id)
                        decoded_str = decoded_bytes.decode('utf-8')

                        if decoded_str.startswith('Pin:'):
                            # æ­£ç¡®å¤„ç†ç©ºæ ¼å’Œæ ¼å¼é—®é¢˜
                            new_pin_id = decoded_str[4:].strip()
                            new_pin_hash = hashlib.sha256(new_pin_id.encode()).hexdigest()

                            # ä½¿ç”¨è¿æ¥çš„è‡ªåŠ¨äº‹åŠ¡ç®¡ç†
                            try:
                                # æ£€æŸ¥æ–°Pin IDæ˜¯å¦å·²å­˜åœ¨
                                cursor.execute("SELECT COUNT(*) FROM pins WHERE id = ?", (new_pin_id,))
                                exists = cursor.fetchone()[0] > 0

                                if exists:
                                    # å¦‚æœæ–°Pin IDå·²å­˜åœ¨ï¼Œåªåˆ é™¤æ—§çš„base64ç¼–ç Pin
                                    cursor.execute("DELETE FROM pins WHERE id = ?", (pin_id,))
                                    logger.debug(f"åˆ é™¤é‡å¤çš„base64 Pin: {pin_id} (ç›®æ ‡ID {new_pin_id} å·²å­˜åœ¨)")
                                else:
                                    # å¦‚æœæ–°Pin IDä¸å­˜åœ¨ï¼Œæ‰§è¡Œæ›´æ–°
                                    cursor.execute("""
                                    UPDATE pins SET id = ?, pin_hash = ? WHERE id = ?
                                    """, (new_pin_id, new_pin_hash, pin_id))
                                    logger.debug(f"è½¬æ¢æˆåŠŸ: {pin_id} -> {new_pin_id}")

                                # ä¼˜åŒ–ï¼šæ‰¹é‡æäº¤è€Œä¸æ˜¯æ¯ä¸ªPinéƒ½æäº¤
                                converted_count += 1
                                pending_commits += 1

                                # æ¯è¾¾åˆ°æ‰¹é‡å¤§å°å°±æäº¤ä¸€æ¬¡
                                if pending_commits >= commit_batch_size:
                                    cursor.connection.commit()
                                    pending_commits = 0

                            except Exception as inner_e:
                                # å›æ»šäº‹åŠ¡
                                cursor.connection.rollback()
                                raise inner_e

                    except sqlite3.IntegrityError as e:
                        logger.warning(f"è½¬æ¢å†²çª {pin_id}: {e}")
                        continue
                    except Exception as e:
                        logger.debug(f"è·³è¿‡æ— æ•ˆçš„base64 Pin: {pin_id}, é”™è¯¯: {e}")
                        continue

                    # æ›´æ–°è¿›åº¦
                    progress.update(1)

                # æäº¤ä»»ä½•å‰©ä½™çš„å¾…å¤„ç†äº‹åŠ¡
                if pending_commits > 0:
                    cursor.connection.commit()
                    logger.debug(f"æœ€ç»ˆæäº¤å‰©ä½™ {pending_commits} ä¸ªè½¬æ¢")

            logger.info(f"âœ… å®‰å…¨æ‰¹é‡è½¬æ¢å®Œæˆ: {keyword}, è½¬æ¢äº† {converted_count:,} ä¸ªPin")
            return converted_count

        except Exception as e:
            logger.error(f"âŒ å®‰å…¨æ‰¹é‡è½¬æ¢å¤±è´¥ {keyword}: {e}")
            cursor.connection.rollback()
            return 0


# ä¸ºäº†å‘åå…¼å®¹ï¼Œä¿ç•™åŸæœ‰çš„ç±»åä½œä¸ºåˆ«å
RealtimeBase64Converter = BatchAtomicBase64Converter
