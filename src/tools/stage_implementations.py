#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç‹¬ç«‹é˜¶æ®µå®ç°

å®ç°--only-imageså·¥ä½œæµç¨‹çš„å››ä¸ªç‹¬ç«‹é˜¶æ®µï¼š
1. æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹é˜¶æ®µ
2. Base64ç¼–ç Pinè½¬æ¢é˜¶æ®µ  
3. Pinè¯¦æƒ…æ•°æ®è¡¥å…¨é˜¶æ®µ
4. å›¾ç‰‡æ–‡ä»¶ä¸‹è½½é˜¶æ®µ
"""

import os
import sqlite3
import json
import asyncio
import random
from typing import Dict, Any, Optional, List
from loguru import logger

from .stage_manager import StageManager
from .realtime_base64_converter import BatchAtomicBase64Converter
from .global_header_manager import GlobalHeaderManager
from .smart_pin_enhancer import SmartPinEnhancer
from ..core.database.repository import SQLiteRepository
from ..core.browser_manager import BrowserManager
from ..utils.network_interceptor import NetworkInterceptor


async def fetch_pin_detail_with_browser(pin_id: str) -> Optional[Dict]:
    """ä½¿ç”¨æµè§ˆå™¨+NetworkInterceptorè·å–Pinè¯¦æƒ…æ•°æ®ï¼ˆæ­£ç¡®çš„å®ç°ï¼‰

    Args:
        pin_id: Pin ID

    Returns:
        Pinè¯¦æƒ…æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        pin_url = f"https://www.pinterest.com/pin/{pin_id}/"

        # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†
        async with NetworkInterceptor(max_cache_size=100, verbose=False, target_count=0) as interceptor:
            browser = BrowserManager(
                proxy=None,
                timeout=30,
                headless=True,
                enable_network_interception=True
            )

            try:
                if not await browser.start():
                    return None

                browser.add_request_handler(interceptor._handle_request)
                browser.add_response_handler(interceptor._handle_response)

                if not await browser.navigate(pin_url):
                    return None

                # é¡µé¢åŠ è½½åçš„äººç±»è¡Œä¸ºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(2.0, 4.0))

                # æ»šåŠ¨è·å–Pinæ•°æ®ï¼Œç›´åˆ°è¿ç»­3æ¬¡æ— æ–°æ•°æ®
                consecutive_no_new = 0
                max_consecutive = 3
                scroll_count = 0
                max_scrolls = 10  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°

                while (len(interceptor.extracted_pins) == 0 and
                       consecutive_no_new < max_consecutive and
                       scroll_count < max_scrolls):

                    pins_before = len(interceptor.extracted_pins)

                    # ä½¿ç”¨çœŸå®çš„PageDowné”®ç›˜äº‹ä»¶æ»šåŠ¨ï¼ˆæ¯”JavaScriptæ›´è‡ªç„¶ï¼‰
                    await browser.page.keyboard.press("PageDown")
                    # æ»šåŠ¨åçš„äººç±»è¡Œä¸ºå»¶è¿Ÿ
                    await asyncio.sleep(random.uniform(2.0, 4.0))
                    scroll_count += 1

                    # ç­‰å¾…é¡µé¢åŠ è½½
                    try:
                        await browser.page.wait_for_load_state('networkidle', timeout=3000)
                    except:
                        pass

                    pins_after = len(interceptor.extracted_pins)

                    if pins_after > pins_before:
                        consecutive_no_new = 0
                    else:
                        consecutive_no_new += 1

                # å¦‚æœæ‹¦æˆªåˆ°äº†Pinæ•°æ®ï¼ŒæŸ¥æ‰¾ç›®æ ‡PIN
                if interceptor.extracted_pins:
                    for pin in interceptor.extracted_pins:
                        if pin.get('id') == pin_id:
                            logger.debug(f"âœ… Pin {pin_id} è¯¦æƒ…è·å–æˆåŠŸ")
                            return pin

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡PINï¼Œè¿”å›ç¬¬ä¸€ä¸ªPINä½œä¸ºç¤ºä¾‹
                if interceptor.extracted_pins:
                    pin_data = list(interceptor.extracted_pins)[0]
                    logger.debug(f"âœ… Pin {pin_id} è¯¦æƒ…é¡µè·å–åˆ°ç›¸å…³Pinæ•°æ®")
                    return pin_data

                logger.debug(f"âš ï¸ Pin {pin_id} è¯¦æƒ…æå–å¤±è´¥æˆ–æ— æ•°æ®")
                return None

            except Exception as e:
                logger.debug(f"Pinè¯¦æƒ…é¡µé‡‡é›†å‡ºé”™: {e}")
                return None
            finally:
                await browser.stop()
                # NetworkInterceptorä¼šåœ¨async withé€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†

    except Exception as e:
        logger.debug(f"è·å–Pinè¯¦æƒ…å¤±è´¥: {pin_id}, é”™è¯¯: {e}")
        return None


class DatabaseRepairStage(StageManager):
    """é˜¶æ®µ1ï¼šæ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹"""
    
    def __init__(self, output_dir: str):
        super().__init__("æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹", output_dir)
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹"""
        logger.info("ğŸ”§ å¼€å§‹æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹é˜¶æ®µ")
        
        # åˆ›å»ºè½¬æ¢å™¨ï¼ˆä»…ç”¨äºä¿®å¤åŠŸèƒ½ï¼‰
        converter = BatchAtomicBase64Converter(self.output_dir)
        
        # å‘ç°éœ€è¦æ£€æŸ¥çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = converter._discover_all_keywords()
        
        repair_stats = {
            "keywords_checked": 0,
            "keywords_repaired": 0,
            "keywords_failed": 0,
            "repair_details": {}
        }
        
        for keyword in keywords:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()
                
            logger.info(f"ğŸ” æ£€æŸ¥å…³é”®è¯æ•°æ®åº“: {keyword}")
            repair_stats["keywords_checked"] += 1
            
            try:
                # åˆ›å»ºç‹¬ç«‹çš„repositoryè¿›è¡Œæ£€æŸ¥
                repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)
                
                # æ‰§è¡Œå¥åº·æ£€æŸ¥å’Œä¿®å¤
                repair_success = await converter._check_and_repair_database(repository, keyword)
                
                if repair_success:
                    logger.info(f"âœ… æ•°æ®åº“å¥åº·æ£€æŸ¥é€šè¿‡: {keyword}")
                    repair_stats["repair_details"][keyword] = "å¥åº·"
                else:
                    logger.warning(f"âš ï¸ æ•°æ®åº“ä¿®å¤å¤±è´¥: {keyword}")
                    repair_stats["keywords_failed"] += 1
                    repair_stats["repair_details"][keyword] = "ä¿®å¤å¤±è´¥"
                
                # å¼ºåˆ¶å…³é—­å½“å‰å…³é”®è¯çš„è¿æ¥
                await converter._force_close_all_connections(keyword)
                
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                repair_stats["keywords_failed"] += 1
                repair_stats["repair_details"][keyword] = f"é”™è¯¯: {str(e)}"
        
        logger.info(f"ğŸ”§ æ•°æ®åº“ä¿®å¤æ£€æµ‹å®Œæˆ: {repair_stats}")
        return self._generate_success_result({"repair_stats": repair_stats})
    
    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯æ•°æ®åº“ä¿®å¤é˜¶æ®µå®Œæ•´æ€§"""
        # éªŒè¯æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶éƒ½å¯ä»¥æ­£å¸¸è®¿é—®
        try:
            if hasattr(self, '_last_result') and self._last_result:
                repair_details = self._last_result.get("repair_stats", {}).get("repair_details", {})
                
                for keyword, status in repair_details.items():
                    if status == "ä¿®å¤å¤±è´¥":
                        logger.warning(f"âš ï¸ å…³é”®è¯ {keyword} ä¿®å¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
                
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¿®å¤é˜¶æ®µéªŒè¯å¤±è´¥: {e}")
            return False


class Base64ConversionStage(StageManager):
    """é˜¶æ®µ2ï¼šBase64ç¼–ç Pinè½¬æ¢"""
    
    def __init__(self, output_dir: str):
        super().__init__("Base64ç¼–ç Pinè½¬æ¢", output_dir)
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒBase64ç¼–ç Pinè½¬æ¢"""
        logger.info("ğŸ”„ å¼€å§‹Base64ç¼–ç Pinè½¬æ¢é˜¶æ®µ")
        
        # åˆ›å»ºç‹¬ç«‹çš„è½¬æ¢å™¨
        converter = BatchAtomicBase64Converter(self.output_dir)
        
        # æ‰§è¡Œè½¬æ¢
        conversion_stats = await converter.process_all_databases(target_keyword)
        
        logger.info(f"ğŸ”„ Base64è½¬æ¢å®Œæˆ: {conversion_stats}")
        return self._generate_success_result({"conversion_stats": conversion_stats})
    
    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯Base64è½¬æ¢é˜¶æ®µå®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰base64ç¼–ç çš„Pin
            keywords = self._discover_all_keywords()
            
            for keyword in keywords:
                # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
                self.check_interruption_and_raise()
                    
                db_path = os.path.join(self.output_dir, keyword, "pinterest.db")
                if not os.path.exists(db_path):
                    continue
                
                try:
                    conn = sqlite3.connect(db_path, timeout=10.0)
                    cursor = conn.cursor()
                    
                    cursor.execute("SELECT COUNT(*) FROM pins WHERE id LIKE 'UGlu%'")
                    base64_count = cursor.fetchone()[0]
                    
                    conn.close()
                    
                    if base64_count > 0:
                        logger.warning(f"âš ï¸ å…³é”®è¯ {keyword} ä»æœ‰ {base64_count} ä¸ªbase64ç¼–ç Pin")
                        return False
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ éªŒè¯å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info("âœ… Base64è½¬æ¢é˜¶æ®µéªŒè¯é€šè¿‡ï¼šæ²¡æœ‰å‘ç°base64ç¼–ç Pin")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Base64è½¬æ¢é˜¶æ®µéªŒè¯å¤±è´¥: {e}")
            return False
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords


class PinEnhancementStage(StageManager):
    """é˜¶æ®µ3ï¼šPinè¯¦æƒ…æ•°æ®è¡¥å…¨"""

    def __init__(self, output_dir: str, max_concurrent: int = 8):
        super().__init__("Pinè¯¦æƒ…æ•°æ®è¡¥å…¨", output_dir)
        self.max_concurrent = max(1, min(max_concurrent, 20))
        logger.info(f"Pinè¯¦æƒ…è¡¥å…¨å¹¶å‘æ•°: {self.max_concurrent}")

        # æ€§èƒ½ç›‘æ§
        self.start_time = None
        self.processing_stats = {
            "total_pins": 0,
            "processing_time": 0,
            "average_speed": 0
        }
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒPinè¯¦æƒ…æ•°æ®è¡¥å…¨ - æ‰¹é‡å¤„ç†ç‰ˆæœ¬"""
        logger.info("ğŸ“¥ å¼€å§‹Pinè¯¦æƒ…æ•°æ®è¡¥å…¨é˜¶æ®µï¼ˆæ‰¹é‡å¤„ç†æ¨¡å¼ï¼‰")

        # 1. æ£€æŸ¥headeræ˜¯å¦è¿‡æœŸï¼Œè¿‡æœŸåˆ™é‡æ–°è·å–
        header_manager = GlobalHeaderManager(self.output_dir)
        headers_ready = await header_manager.ensure_headers_ready()
        if not headers_ready:
            logger.warning("âš ï¸ Headerså‡†å¤‡å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headers")

        # è·å–å…±äº«headers
        shared_headers = header_manager.get_headers()
        logger.info(f"âœ… è·å–åˆ°å…±äº«Headersï¼ŒåŒ…å« {len(shared_headers)} ä¸ªå­—æ®µ")

        # å‘ç°éœ€è¦å¤„ç†çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = self._discover_all_keywords()

        enhancement_stats = {
            "keywords_processed": 0,
            "total_pins_checked": 0,
            "total_pins_enhanced": 0,
            "total_pins_failed": 0,
            "keyword_details": {}
        }
        
        for keyword in keywords:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()

            logger.info(f"ğŸ“¥ å¤„ç†å…³é”®è¯: {keyword}")
            enhancement_stats["keywords_processed"] += 1
            
            try:
                # 2. åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinï¼ˆæ‰¹æ¬¡å¤§å°=max_concurrentï¼‰
                offset = 0
                keyword_total_checked = 0
                keyword_total_enhanced = 0
                keyword_total_failed = 0

                while True:
                    # æ£€æŸ¥ä¸­æ–­çŠ¶æ€
                    self.check_interruption_and_raise()

                    # åˆ†é¡µè·å–Pinæ‰¹æ¬¡
                    pins_batch = await self._get_pins_batch_with_pagination(keyword, self.max_concurrent, offset)

                    if not pins_batch:
                        # æ²¡æœ‰æ›´å¤šæ•°æ®
                        break

                    logger.info(f"ğŸ“¦ å…³é”®è¯ {keyword}: å¤„ç†æ‰¹æ¬¡ offset={offset}, å¤§å°={len(pins_batch)}")
                    keyword_total_checked += len(pins_batch)

                    # 3. ä½¿ç”¨ThreadPoolExecutorå¹¶å‘è·å–Pinè¯¦æƒ…
                    batch_stats = await self._process_pins_batch_concurrent(pins_batch, keyword, shared_headers)

                    # æ›´æ–°ç»Ÿè®¡
                    keyword_total_enhanced += batch_stats.get("updated", 0)
                    keyword_total_failed += batch_stats.get("failed", 0)

                    logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: å¢å¼º {batch_stats.get('updated', 0)} ä¸ªï¼Œå¤±è´¥ {batch_stats.get('failed', 0)} ä¸ª")

                    # æ›´æ–°åç§»é‡
                    offset += len(pins_batch)

                    # å¦‚æœæ‰¹æ¬¡å¤§å°å°äºmax_concurrentï¼Œè¯´æ˜å·²ç»åˆ°è¾¾æœ«å°¾
                    if len(pins_batch) < self.max_concurrent:
                        break

                # æ›´æ–°å…³é”®è¯ç»Ÿè®¡
                enhancement_stats["total_pins_checked"] += keyword_total_checked
                enhancement_stats["total_pins_enhanced"] += keyword_total_enhanced
                enhancement_stats["total_pins_failed"] += keyword_total_failed
                enhancement_stats["keyword_details"][keyword] = {
                    "checked": keyword_total_checked,
                    "enhanced": keyword_total_enhanced,
                    "failed": keyword_total_failed
                }

                logger.info(f"âœ… å…³é”®è¯ {keyword} å®Œæˆ: æ£€æŸ¥ {keyword_total_checked} ä¸ªï¼Œå¢å¼º {keyword_total_enhanced} ä¸ªï¼Œå¤±è´¥ {keyword_total_failed} ä¸ª")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                enhancement_stats["keyword_details"][keyword] = {
                    "error": str(e)
                }
        
        logger.info(f"ğŸ“¥ Pinè¯¦æƒ…è¡¥å…¨å®Œæˆ: {enhancement_stats}")
        return self._generate_success_result({"enhancement_stats": enhancement_stats})
    
    async def _get_pins_needing_enhancement(self, keyword: str) -> List[Dict]:
        """è·å–éœ€è¦å¢å¼ºçš„Pinåˆ—è¡¨"""
        try:
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # æŸ¥è¯¢ç¼ºå°‘å›¾ç‰‡URLçš„Pinï¼Œæ’é™¤base64ç¼–ç çš„Pin
            with repository._get_session() as session:
                from ..core.database.schema import Pin

                pins = session.query(Pin).filter(
                    # åŸæœ‰æ¡ä»¶ï¼šç¼ºå°‘å›¾ç‰‡URL
                    ((Pin.largest_image_url == None) |
                     (Pin.largest_image_url == '') |
                     (Pin.image_urls == None) |
                     (Pin.image_urls == '')) &
                    # æ–°å¢æ¡ä»¶ï¼šæ’é™¤base64ç¼–ç Pinï¼ˆä»¥UGluå¼€å¤´ï¼‰
                    (~Pin.id.like('UGlu%'))
                ).all()

                return [pin.to_dict() for pin in pins]

        except Exception as e:
            logger.error(f"è·å–éœ€è¦å¢å¼ºçš„Pinå¤±è´¥ {keyword}: {e}")
            return []

    async def _get_pins_batch_with_pagination(self, keyword: str, batch_size: int, offset: int) -> List[Dict]:
        """åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinåˆ—è¡¨

        Args:
            keyword: å…³é”®è¯
            batch_size: æ‰¹æ¬¡å¤§å°
            offset: åç§»é‡

        Returns:
            Pinæ•°æ®åˆ—è¡¨
        """
        try:
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # æŸ¥è¯¢ç¼ºå°‘å›¾ç‰‡URLçš„Pinï¼Œæ”¯æŒåˆ†é¡µï¼Œæ’é™¤base64ç¼–ç çš„Pin
            with repository._get_session() as session:
                from ..core.database.schema import Pin

                pins = session.query(Pin).filter(
                    # åŸæœ‰æ¡ä»¶ï¼šç¼ºå°‘å›¾ç‰‡URL
                    ((Pin.largest_image_url == None) |
                     (Pin.largest_image_url == '') |
                     (Pin.image_urls == None) |
                     (Pin.image_urls == '')) &
                    # æ–°å¢æ¡ä»¶ï¼šæ’é™¤base64ç¼–ç Pinï¼ˆä»¥UGluå¼€å¤´ï¼‰
                    (~Pin.id.like('UGlu%'))
                ).offset(offset).limit(batch_size).all()

                return [pin.to_dict() for pin in pins]

        except Exception as e:
            logger.error(f"åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinå¤±è´¥ {keyword}: {e}")
            return []

    async def _update_enhanced_pins_batch(self, enhanced_results: List[tuple], keyword: str) -> Dict[str, int]:
        """æ‰¹é‡æ›´æ–°å¢å¼ºåçš„Pinæ•°æ®åˆ°æ•°æ®åº“

        Args:
            enhanced_results: å¢å¼ºç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(pin_id, enhanced_data_or_none), ...]
            keyword: å…³é”®è¯

        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"updated": 0, "failed": 0, "skipped": 0}

        # åˆ›å»ºSmartPinEnhancerå®ä¾‹æ¥å¤ç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°é€»è¾‘
        pin_enhancer = SmartPinEnhancer(self.output_dir)

        for pin_id, enhanced_data in enhanced_results:
            try:
                if enhanced_data is None:
                    # è·å–å¤±è´¥
                    stats["failed"] += 1
                    continue

                if not enhanced_data.get('image_urls'):
                    # æ²¡æœ‰è·å–åˆ°å›¾ç‰‡URL
                    stats["skipped"] += 1
                    continue

                # å¤ç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°é€»è¾‘
                success = await pin_enhancer._update_pin_in_database_atomic(enhanced_data, keyword)

                if success:
                    stats["updated"] += 1
                    logger.debug(f"âœ… Pin {pin_id} æ•°æ®åº“æ›´æ–°æˆåŠŸ")
                else:
                    stats["failed"] += 1
                    logger.debug(f"âŒ Pin {pin_id} æ•°æ®åº“æ›´æ–°å¤±è´¥")

            except Exception as e:
                logger.error(f"æ›´æ–°Pin {pin_id} æ—¶å‡ºé”™: {e}")
                stats["failed"] += 1

        return stats

    async def _process_pins_batch_concurrent(self, pins_batch: List[Dict], keyword: str, shared_headers: Dict[str, str]) -> Dict[str, int]:
        """å¹¶å‘å¤„ç†Pinæ‰¹æ¬¡

        Args:
            pins_batch: Pinæ‰¹æ¬¡æ•°æ®
            keyword: å…³é”®è¯
            shared_headers: å…±äº«headers

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        import concurrent.futures
        import threading
        from tqdm import tqdm

        # è¿‡æ»¤å‡ºéœ€è¦å¢å¼ºçš„Pinï¼ˆæ²¡æœ‰å›¾ç‰‡URLçš„ï¼‰
        pins_to_process = []
        for pin in pins_batch:
            if not self._has_valid_image_urls(pin):
                pins_to_process.append(pin)

        if not pins_to_process:
            logger.info("æ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦å¢å¼ºçš„Pin")
            return {"updated": 0, "failed": 0, "skipped": len(pins_batch)}

        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(pins_to_process)} ä¸ªPinï¼Œå¹¶å‘æ•°: {self.max_concurrent}")

        async def process_single_pin_with_browser(pin_data: Dict) -> tuple:
            """ä½¿ç”¨æµè§ˆå™¨å¤„ç†å•ä¸ªPin"""
            try:
                # æ£€æŸ¥å…¨å±€ä¸­æ–­çŠ¶æ€
                from .stage_manager import _global_interrupt_manager
                if _global_interrupt_manager.is_interrupted():
                    raise KeyboardInterrupt("ç”¨æˆ·ä¸­æ–­æ“ä½œ")

                pin_id = pin_data.get('id')

                # ä½¿ç”¨æµè§ˆå™¨è·å–Pinè¯¦æƒ…
                enhanced_data = await fetch_pin_detail_with_browser(pin_id)

                if enhanced_data and enhanced_data.get('image_urls'):
                    # åˆå¹¶Pinæ•°æ®ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
                    pin_enhancer = SmartPinEnhancer(self.output_dir)
                    merged_pin = pin_enhancer._merge_pin_data(pin_data, enhanced_data)
                    return (pin_id, merged_pin)
                else:
                    return (pin_id, None)

            except KeyboardInterrupt:
                raise  # é‡æ–°æŠ›å‡ºä¸­æ–­å¼‚å¸¸
            except Exception as e:
                logger.debug(f"å¤„ç†Pin {pin_data.get('id', 'unknown')} å¤±è´¥: {e}")
                return (pin_data.get('id'), None)

        # ä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†
        enhanced_results = []

        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(pin_data):
            async with semaphore:
                return await process_single_pin_with_browser(pin_data)

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [process_with_semaphore(pin) for pin in pins_to_process]

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        from tqdm.asyncio import tqdm as atqdm

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        try:
            enhanced_results = await atqdm.gather(*tasks, desc=f"è·å–{keyword}è¯¦æƒ…")
        except KeyboardInterrupt:
            logger.info("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å¹¶å‘å¤„ç†")
            raise

        # 4. æ‰¹é‡æ›´æ–°æ•°æ®åº“
        update_stats = await self._update_enhanced_pins_batch(enhanced_results, keyword)

        return update_stats

    def _has_valid_image_urls(self, pin: Dict) -> bool:
        """æ£€æŸ¥Pinæ˜¯å¦æœ‰æœ‰æ•ˆçš„å›¾ç‰‡URLï¼ˆå¤ç”¨SmartPinEnhancerçš„é€»è¾‘ï¼‰"""
        # æ£€æŸ¥largest_image_url
        largest_url = pin.get('largest_image_url', '').strip()
        if largest_url and largest_url.startswith('http'):
            return True

        # æ£€æŸ¥image_urlså­—å…¸
        image_urls = pin.get('image_urls', {})
        if isinstance(image_urls, str):
            try:
                import json
                image_urls = json.loads(image_urls)
            except:
                image_urls = {}

        if isinstance(image_urls, dict) and image_urls:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆçš„URL
            for key, url in image_urls.items():
                if url and isinstance(url, str) and url.strip().startswith('http'):
                    return True

        return False

    async def _enhance_pins_batch(self, pins: List[Dict], keyword: str,
                                pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """æ‰¹é‡å¢å¼ºPin - æ”¯æŒå¹¶å‘å’Œå•çº¿ç¨‹æ¨¡å¼"""
        if self.max_concurrent == 1:
            return await self._enhance_pins_sequential(pins, keyword, pin_enhancer)
        else:
            return await self._enhance_pins_concurrent(pins, keyword, pin_enhancer)

    async def _enhance_pins_sequential(self, pins: List[Dict], keyword: str,
                                     pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """å•çº¿ç¨‹é¡ºåºå¢å¼ºPinï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        stats = {"checked": len(pins), "enhanced": 0, "failed": 0}

        for pin in pins:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()

            try:
                enhanced = await pin_enhancer.enhance_pin_if_needed(pin, keyword)
                if enhanced:
                    stats["enhanced"] += 1

            except Exception as e:
                logger.debug(f"å¢å¼ºPin {pin.get('id', 'unknown')} å¤±è´¥: {e}")
                stats["failed"] += 1

        return stats

    async def _enhance_pins_concurrent(self, pins: List[Dict], keyword: str,
                                     pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """å¤šçº¿ç¨‹å¹¶å‘å¢å¼ºPin"""
        import concurrent.futures
        import threading
        import asyncio
        from tqdm import tqdm

        stats = {"checked": len(pins), "enhanced": 0, "failed": 0}
        stats_lock = threading.Lock()

        def process_single_pin(pin_data: Dict) -> bool:
            """å•çº¿ç¨‹å¤„ç†å•ä¸ªPin"""
            try:
                # æ£€æŸ¥å…¨å±€ä¸­æ–­çŠ¶æ€
                from .stage_manager import _global_interrupt_manager
                if _global_interrupt_manager.is_interrupted():
                    raise KeyboardInterrupt("ç”¨æˆ·ä¸­æ–­æ“ä½œ")

                # åˆ›å»ºçº¿ç¨‹ä¸“ç”¨çš„enhancerå®ä¾‹
                thread_enhancer = SmartPinEnhancer(self.output_dir)

                # åŒæ­¥è°ƒç”¨enhance_pin_if_needed
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    enhanced = loop.run_until_complete(
                        thread_enhancer.enhance_pin_if_needed(pin_data, keyword)
                    )
                    return enhanced is not None
                finally:
                    loop.close()

            except KeyboardInterrupt:
                raise  # é‡æ–°æŠ›å‡ºä¸­æ–­å¼‚å¸¸
            except Exception as e:
                logger.debug(f"çº¿ç¨‹å¤„ç†Pin {pin_data.get('id', 'unknown')} å¤±è´¥: {e}")
                return False

        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(pins)} ä¸ªPinï¼Œå¹¶å‘æ•°: {self.max_concurrent}")

        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            with tqdm(total=len(pins), desc=f"å¤„ç†{keyword}", unit="pin") as pbar:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_pin = {
                    executor.submit(process_single_pin, pin): pin
                    for pin in pins
                }

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_pin):
                    pin = future_to_pin[future]
                    try:
                        success = future.result()
                        with stats_lock:
                            if success:
                                stats["enhanced"] += 1
                            else:
                                stats["failed"] += 1
                        pbar.update(1)
                    except KeyboardInterrupt:
                        logger.info("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å¹¶å‘å¤„ç†")
                        # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_pin:
                            f.cancel()
                        raise
                    except Exception as e:
                        logger.error(f"Pin {pin.get('id', 'unknown')} å¤„ç†å¼‚å¸¸: {e}")
                        with stats_lock:
                            stats["failed"] += 1
                        pbar.update(1)

        logger.info(f"å¹¶å‘å¤„ç†å®Œæˆ: å¢å¼º {stats['enhanced']} ä¸ªï¼Œå¤±è´¥ {stats['failed']} ä¸ª")
        return stats

    def _get_thread_safe_headers(self, header_manager: GlobalHeaderManager) -> Dict[str, str]:
        """è·å–çº¿ç¨‹å®‰å…¨çš„Headerså‰¯æœ¬"""
        return header_manager.get_headers()

    def _validate_concurrent_setup(self) -> bool:
        """éªŒè¯å¹¶å‘è®¾ç½®æ˜¯å¦æ­£ç¡®"""
        if self.max_concurrent < 1 or self.max_concurrent > 50:
            logger.error(f"æ— æ•ˆçš„å¹¶å‘æ•°: {self.max_concurrent}")
            return False

        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        try:
            import psutil
            available_memory = psutil.virtual_memory().available / (1024**3)  # GB
            if available_memory < 1.0:
                logger.warning(f"å¯ç”¨å†…å­˜è¾ƒä½: {available_memory:.1f}GB")
        except ImportError:
            logger.debug("psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æ£€æŸ¥")

        return True

    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯Pinå¢å¼ºé˜¶æ®µå®Œæ•´æ€§"""
        # è¿™ä¸ªé˜¶æ®µçš„éªŒè¯æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶è¿”å›True
        # å®é™…åº”ç”¨ä¸­å¯ä»¥æ£€æŸ¥å…³é”®Pinæ˜¯å¦éƒ½æœ‰äº†å›¾ç‰‡URL
        return True
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords


class ImageDownloadStage(StageManager):
    """é˜¶æ®µ4ï¼šå›¾ç‰‡æ–‡ä»¶ä¸‹è½½"""
    
    def __init__(self, output_dir: str, max_concurrent: int = 15):
        super().__init__("å›¾ç‰‡æ–‡ä»¶ä¸‹è½½", output_dir)
        self.max_concurrent = max_concurrent
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå›¾ç‰‡æ–‡ä»¶ä¸‹è½½"""
        logger.info("ğŸ“¥ å¼€å§‹å›¾ç‰‡æ–‡ä»¶ä¸‹è½½é˜¶æ®µ")
        
        # åˆ›å»ºç‹¬ç«‹çš„ä¸‹è½½å™¨
        from .image_downloader import ImageDownloader
        downloader = ImageDownloader(
            output_dir=self.output_dir,
            max_concurrent=self.max_concurrent,
            prefer_requests=True
        )
        
        # å‘ç°éœ€è¦å¤„ç†çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = self._discover_all_keywords()
        
        download_stats = {
            "keywords_processed": 0,
            "total_downloaded": 0,
            "total_failed": 0,
            "keyword_details": {}
        }
        
        for keyword in keywords:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()

            logger.info(f"ğŸ“¥ ä¸‹è½½å…³é”®è¯å›¾ç‰‡: {keyword}")
            download_stats["keywords_processed"] += 1
            
            try:
                # æ‰§è¡Œå…³é”®è¯å›¾ç‰‡ä¸‹è½½
                keyword_results = await downloader.download_keyword_images(keyword)
                
                # ç»Ÿè®¡ç»“æœ
                downloaded = sum(1 for success, _ in keyword_results if success)
                failed = sum(1 for success, _ in keyword_results if not success)
                
                download_stats["total_downloaded"] += downloaded
                download_stats["total_failed"] += failed
                download_stats["keyword_details"][keyword] = {
                    "downloaded": downloaded,
                    "failed": failed,
                    "total": len(keyword_results)
                }
                
                logger.info(f"âœ… å…³é”®è¯ {keyword}: ä¸‹è½½ {downloaded} æˆåŠŸ, {failed} å¤±è´¥")
                
            except Exception as e:
                logger.error(f"âŒ ä¸‹è½½å…³é”®è¯ {keyword} å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                download_stats["keyword_details"][keyword] = {
                    "error": str(e)
                }
        
        # æ¸…ç†ä¸‹è½½å™¨èµ„æº
        await downloader.close()
        
        logger.info(f"ğŸ“¥ å›¾ç‰‡ä¸‹è½½å®Œæˆ: {download_stats}")
        return self._generate_success_result({"download_stats": download_stats})
    
    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯å›¾ç‰‡ä¸‹è½½é˜¶æ®µå®Œæ•´æ€§"""
        # å¯ä»¥æ£€æŸ¥å…³é”®å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å›True
        return True
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords
