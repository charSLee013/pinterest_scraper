#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç‹¬ç«‹é˜¶æ®µå®ç°

å®ç°--only-imageså·¥ä½œæµç¨‹çš„å››ä¸ªç‹¬ç«‹é˜¶æ®µï¼š
1. æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹é˜¶æ®µ
2. Base64ç¼–ç Pinè½¬æ¢é˜¶æ®µ  
3. Pinè¯¦æƒ…æ•°æ®è¡¥å…¨é˜¶æ®µ
4. å›¾ç‰‡æ–‡ä»¶ä¸‹è½½é˜¶æ®µ
"""

import os
import sqlite3
import json
import asyncio
import random
from typing import Dict, Any, Optional, List
from loguru import logger

from .stage_manager import StageManager
from .realtime_base64_converter import BatchAtomicBase64Converter
from .global_header_manager import GlobalHeaderManager
from .smart_pin_enhancer import SmartPinEnhancer
from ..core.database.repository import SQLiteRepository
from ..core.browser_manager import BrowserManager
from ..utils.network_interceptor import NetworkInterceptor


async def fetch_pin_detail_with_browser(pin_id: str) -> Optional[Dict]:
    """ä½¿ç”¨æµè§ˆå™¨+NetworkInterceptorè·å–Pinè¯¦æƒ…æ•°æ®ï¼ˆæ­£ç¡®çš„å®ç°ï¼‰

    Args:
        pin_id: Pin ID

    Returns:
        Pinè¯¦æƒ…æ•°æ®ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        pin_url = f"https://www.pinterest.com/pin/{pin_id}/"

        # ä½¿ç”¨å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®æ¸…ç†
        async with NetworkInterceptor(max_cache_size=100, verbose=False, target_count=0) as interceptor:
            browser = BrowserManager(
                proxy=None,
                timeout=30,
                headless=True,
                enable_network_interception=True
            )

            try:
                if not await browser.start():
                    return None

                browser.add_request_handler(interceptor._handle_request)
                browser.add_response_handler(interceptor._handle_response)

                if not await browser.navigate(pin_url):
                    return None

                # é¡µé¢åŠ è½½åçš„äººç±»è¡Œä¸ºå»¶è¿Ÿ
                await asyncio.sleep(random.uniform(2.0, 4.0))

                # æ»šåŠ¨è·å–Pinæ•°æ®ï¼Œç›´åˆ°è¿ç»­3æ¬¡æ— æ–°æ•°æ®
                consecutive_no_new = 0
                max_consecutive = 3
                scroll_count = 0
                max_scrolls = 10  # æœ€å¤§æ»šåŠ¨æ¬¡æ•°

                while (len(interceptor.extracted_pins) == 0 and
                       consecutive_no_new < max_consecutive and
                       scroll_count < max_scrolls):

                    pins_before = len(interceptor.extracted_pins)

                    # ä½¿ç”¨çœŸå®çš„PageDowné”®ç›˜äº‹ä»¶æ»šåŠ¨ï¼ˆæ¯”JavaScriptæ›´è‡ªç„¶ï¼‰
                    await browser.page.keyboard.press("PageDown")
                    # æ»šåŠ¨åçš„äººç±»è¡Œä¸ºå»¶è¿Ÿ
                    await asyncio.sleep(random.uniform(2.0, 4.0))
                    scroll_count += 1

                    # ç­‰å¾…é¡µé¢åŠ è½½
                    try:
                        await browser.page.wait_for_load_state('networkidle', timeout=3000)
                    except:
                        pass

                    pins_after = len(interceptor.extracted_pins)

                    if pins_after > pins_before:
                        consecutive_no_new = 0
                    else:
                        consecutive_no_new += 1

                # å¦‚æœæ‹¦æˆªåˆ°äº†Pinæ•°æ®ï¼ŒæŸ¥æ‰¾ç›®æ ‡PIN
                if interceptor.extracted_pins:
                    for pin in interceptor.extracted_pins:
                        if pin.get('id') == pin_id:
                            logger.debug(f"âœ… Pin {pin_id} è¯¦æƒ…è·å–æˆåŠŸ")
                            return pin

                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç›®æ ‡PINï¼Œè¿”å›ç¬¬ä¸€ä¸ªPINä½œä¸ºç¤ºä¾‹
                if interceptor.extracted_pins:
                    pin_data = list(interceptor.extracted_pins)[0]
                    logger.debug(f"âœ… Pin {pin_id} è¯¦æƒ…é¡µè·å–åˆ°ç›¸å…³Pinæ•°æ®")
                    return pin_data

                logger.debug(f"âš ï¸ Pin {pin_id} è¯¦æƒ…æå–å¤±è´¥æˆ–æ— æ•°æ®")
                return None

            except Exception as e:
                logger.debug(f"Pinè¯¦æƒ…é¡µé‡‡é›†å‡ºé”™: {e}")
                return None
            finally:
                await browser.stop()
                # NetworkInterceptorä¼šåœ¨async withé€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†

    except Exception as e:
        logger.debug(f"è·å–Pinè¯¦æƒ…å¤±è´¥: {pin_id}, é”™è¯¯: {e}")
        return None


class DatabaseRepairStage(StageManager):
    """é˜¶æ®µ1ï¼šæ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹"""
    
    def __init__(self, output_dir: str):
        super().__init__("æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹", output_dir)
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œæ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹"""
        logger.info("ğŸ”§ å¼€å§‹æ•°æ®åº“ä¿®å¤ä¸æ£€æµ‹é˜¶æ®µ")
        
        # åˆ›å»ºè½¬æ¢å™¨ï¼ˆä»…ç”¨äºä¿®å¤åŠŸèƒ½ï¼‰
        converter = BatchAtomicBase64Converter(self.output_dir)
        
        # å‘ç°éœ€è¦æ£€æŸ¥çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = converter._discover_all_keywords()
        
        repair_stats = {
            "keywords_checked": 0,
            "keywords_repaired": 0,
            "keywords_failed": 0,
            "repair_details": {}
        }
        
        for keyword in keywords:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()
                
            logger.info(f"ğŸ” æ£€æŸ¥å…³é”®è¯æ•°æ®åº“: {keyword}")
            repair_stats["keywords_checked"] += 1
            
            try:
                # åˆ›å»ºç‹¬ç«‹çš„repositoryè¿›è¡Œæ£€æŸ¥
                repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)
                
                # æ‰§è¡Œå¥åº·æ£€æŸ¥å’Œä¿®å¤
                repair_success = await converter._check_and_repair_database(repository, keyword)
                
                if repair_success:
                    logger.info(f"âœ… æ•°æ®åº“å¥åº·æ£€æŸ¥é€šè¿‡: {keyword}")
                    repair_stats["repair_details"][keyword] = "å¥åº·"
                else:
                    logger.warning(f"âš ï¸ æ•°æ®åº“ä¿®å¤å¤±è´¥: {keyword}")
                    repair_stats["keywords_failed"] += 1
                    repair_stats["repair_details"][keyword] = "ä¿®å¤å¤±è´¥"
                
                # å¼ºåˆ¶å…³é—­å½“å‰å…³é”®è¯çš„è¿æ¥
                await converter._force_close_all_connections(keyword)
                
            except Exception as e:
                logger.error(f"âŒ æ£€æŸ¥å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                repair_stats["keywords_failed"] += 1
                repair_stats["repair_details"][keyword] = f"é”™è¯¯: {str(e)}"
        
        logger.info(f"ğŸ”§ æ•°æ®åº“ä¿®å¤æ£€æµ‹å®Œæˆ: {repair_stats}")
        return self._generate_success_result({"repair_stats": repair_stats})
    
    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯æ•°æ®åº“ä¿®å¤é˜¶æ®µå®Œæ•´æ€§"""
        # éªŒè¯æ‰€æœ‰æ•°æ®åº“æ–‡ä»¶éƒ½å¯ä»¥æ­£å¸¸è®¿é—®
        try:
            if hasattr(self, '_last_result') and self._last_result:
                repair_details = self._last_result.get("repair_stats", {}).get("repair_details", {})
                
                for keyword, status in repair_details.items():
                    if status == "ä¿®å¤å¤±è´¥":
                        logger.warning(f"âš ï¸ å…³é”®è¯ {keyword} ä¿®å¤å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
                
            return True
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ä¿®å¤é˜¶æ®µéªŒè¯å¤±è´¥: {e}")
            return False


class Base64ConversionStage(StageManager):
    """é˜¶æ®µ2ï¼šBase64ç¼–ç Pinè½¬æ¢"""
    
    def __init__(self, output_dir: str):
        super().__init__("Base64ç¼–ç Pinè½¬æ¢", output_dir)
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒBase64ç¼–ç Pinè½¬æ¢"""
        logger.info("ğŸ”„ å¼€å§‹Base64ç¼–ç Pinè½¬æ¢é˜¶æ®µ")
        
        # åˆ›å»ºç‹¬ç«‹çš„è½¬æ¢å™¨
        converter = BatchAtomicBase64Converter(self.output_dir)
        
        # æ‰§è¡Œè½¬æ¢
        conversion_stats = await converter.process_all_databases(target_keyword)
        
        logger.info(f"ğŸ”„ Base64è½¬æ¢å®Œæˆ: {conversion_stats}")
        return self._generate_success_result({"conversion_stats": conversion_stats})
    
    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯Base64è½¬æ¢é˜¶æ®µå®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰base64ç¼–ç çš„Pin
            keywords = self._discover_all_keywords()
            
            for keyword in keywords:
                # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
                self.check_interruption_and_raise()
                    
                db_path = os.path.join(self.output_dir, keyword, "pinterest.db")
                if not os.path.exists(db_path):
                    continue
                
                try:
                    conn = sqlite3.connect(db_path, timeout=10.0)
                    cursor = conn.cursor()
                    
                    cursor.execute("SELECT COUNT(*) FROM pins WHERE id LIKE 'UGlu%'")
                    base64_count = cursor.fetchone()[0]
                    
                    conn.close()
                    
                    if base64_count > 0:
                        logger.warning(f"âš ï¸ å…³é”®è¯ {keyword} ä»æœ‰ {base64_count} ä¸ªbase64ç¼–ç Pin")
                        return False
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ éªŒè¯å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                    continue
            
            logger.info("âœ… Base64è½¬æ¢é˜¶æ®µéªŒè¯é€šè¿‡ï¼šæ²¡æœ‰å‘ç°base64ç¼–ç Pin")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Base64è½¬æ¢é˜¶æ®µéªŒè¯å¤±è´¥: {e}")
            return False
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords


class PinEnhancementStage(StageManager):
    """é˜¶æ®µ3ï¼šPinè¯¦æƒ…æ•°æ®è¡¥å…¨"""

    def __init__(self, output_dir: str, max_concurrent: int = 8):
        super().__init__("Pinè¯¦æƒ…æ•°æ®è¡¥å…¨", output_dir)
        self.max_concurrent = max(1, min(max_concurrent, 20))
        logger.info(f"Pinè¯¦æƒ…è¡¥å…¨å¹¶å‘æ•°: {self.max_concurrent}")

        # æ€§èƒ½ç›‘æ§
        self.start_time = None
        self.processing_stats = {
            "total_pins": 0,
            "processing_time": 0,
            "average_speed": 0
        }
    
    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡ŒPinè¯¦æƒ…æ•°æ®è¡¥å…¨ - æ‰¹é‡å¤„ç†ç‰ˆæœ¬"""
        logger.info("ğŸ“¥ å¼€å§‹Pinè¯¦æƒ…æ•°æ®è¡¥å…¨é˜¶æ®µï¼ˆæ‰¹é‡å¤„ç†æ¨¡å¼ï¼‰")

        # 1. æ£€æŸ¥headeræ˜¯å¦è¿‡æœŸï¼Œè¿‡æœŸåˆ™é‡æ–°è·å–
        header_manager = GlobalHeaderManager(self.output_dir)
        headers_ready = await header_manager.ensure_headers_ready()
        if not headers_ready:
            logger.warning("âš ï¸ Headerså‡†å¤‡å¤±è´¥ï¼Œå°†ä½¿ç”¨é»˜è®¤Headers")

        # è·å–å…±äº«headers
        shared_headers = header_manager.get_headers()
        logger.info(f"âœ… è·å–åˆ°å…±äº«Headersï¼ŒåŒ…å« {len(shared_headers)} ä¸ªå­—æ®µ")

        # å‘ç°éœ€è¦å¤„ç†çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = self._discover_all_keywords()

        enhancement_stats = {
            "keywords_processed": 0,
            "total_pins_checked": 0,
            "total_pins_enhanced": 0,
            "total_pins_failed": 0,
            "keyword_details": {}
        }
        
        for keyword in keywords:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()

            logger.info(f"ğŸ“¥ å¤„ç†å…³é”®è¯: {keyword}")
            enhancement_stats["keywords_processed"] += 1
            
            try:
                # 2. åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinï¼ˆæ‰¹æ¬¡å¤§å°=max_concurrentï¼‰
                offset = 0
                keyword_total_checked = 0
                keyword_total_enhanced = 0
                keyword_total_failed = 0

                while True:
                    # æ£€æŸ¥ä¸­æ–­çŠ¶æ€
                    self.check_interruption_and_raise()

                    # åˆ†é¡µè·å–Pinæ‰¹æ¬¡
                    pins_batch = await self._get_pins_batch_with_pagination(keyword, self.max_concurrent, offset)

                    if not pins_batch:
                        # æ²¡æœ‰æ›´å¤šæ•°æ®
                        break

                    logger.info(f"ğŸ“¦ å…³é”®è¯ {keyword}: å¤„ç†æ‰¹æ¬¡ offset={offset}, å¤§å°={len(pins_batch)}")
                    keyword_total_checked += len(pins_batch)

                    # 3. ä½¿ç”¨ThreadPoolExecutorå¹¶å‘è·å–Pinè¯¦æƒ…
                    batch_stats = await self._process_pins_batch_concurrent(pins_batch, keyword, shared_headers)

                    # æ›´æ–°ç»Ÿè®¡
                    keyword_total_enhanced += batch_stats.get("updated", 0)
                    keyword_total_failed += batch_stats.get("failed", 0)

                    logger.info(f"âœ… æ‰¹æ¬¡å®Œæˆ: å¢å¼º {batch_stats.get('updated', 0)} ä¸ªï¼Œå¤±è´¥ {batch_stats.get('failed', 0)} ä¸ª")

                    # æ›´æ–°åç§»é‡
                    offset += len(pins_batch)

                    # å¦‚æœæ‰¹æ¬¡å¤§å°å°äºmax_concurrentï¼Œè¯´æ˜å·²ç»åˆ°è¾¾æœ«å°¾
                    if len(pins_batch) < self.max_concurrent:
                        break

                # æ›´æ–°å…³é”®è¯ç»Ÿè®¡
                enhancement_stats["total_pins_checked"] += keyword_total_checked
                enhancement_stats["total_pins_enhanced"] += keyword_total_enhanced
                enhancement_stats["total_pins_failed"] += keyword_total_failed
                enhancement_stats["keyword_details"][keyword] = {
                    "checked": keyword_total_checked,
                    "enhanced": keyword_total_enhanced,
                    "failed": keyword_total_failed
                }

                logger.info(f"âœ… å…³é”®è¯ {keyword} å®Œæˆ: æ£€æŸ¥ {keyword_total_checked} ä¸ªï¼Œå¢å¼º {keyword_total_enhanced} ä¸ªï¼Œå¤±è´¥ {keyword_total_failed} ä¸ª")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
                enhancement_stats["keyword_details"][keyword] = {
                    "error": str(e)
                }
        
        logger.info(f"ğŸ“¥ Pinè¯¦æƒ…è¡¥å…¨å®Œæˆ: {enhancement_stats}")
        return self._generate_success_result({"enhancement_stats": enhancement_stats})
    
    async def _get_pins_needing_enhancement(self, keyword: str) -> List[Dict]:
        """è·å–éœ€è¦å¢å¼ºçš„Pinåˆ—è¡¨"""
        try:
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # æŸ¥è¯¢ç¼ºå°‘å›¾ç‰‡URLçš„Pinï¼Œæ’é™¤base64ç¼–ç çš„Pin
            with repository._get_session() as session:
                from ..core.database.schema import Pin

                pins = session.query(Pin).filter(
                    # åŸæœ‰æ¡ä»¶ï¼šç¼ºå°‘å›¾ç‰‡URL
                    ((Pin.largest_image_url == None) |
                     (Pin.largest_image_url == '') |
                     (Pin.image_urls == None) |
                     (Pin.image_urls == '')) &
                    # æ–°å¢æ¡ä»¶ï¼šæ’é™¤base64ç¼–ç Pinï¼ˆä»¥UGluå¼€å¤´ï¼‰
                    (~Pin.id.like('UGlu%'))
                ).all()

                return [pin.to_dict() for pin in pins]

        except Exception as e:
            logger.error(f"è·å–éœ€è¦å¢å¼ºçš„Pinå¤±è´¥ {keyword}: {e}")
            return []

    async def _get_pins_batch_with_pagination(self, keyword: str, batch_size: int, offset: int) -> List[Dict]:
        """åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinåˆ—è¡¨

        Args:
            keyword: å…³é”®è¯
            batch_size: æ‰¹æ¬¡å¤§å°
            offset: åç§»é‡

        Returns:
            Pinæ•°æ®åˆ—è¡¨
        """
        try:
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)

            # æŸ¥è¯¢ç¼ºå°‘å›¾ç‰‡URLçš„Pinï¼Œæ”¯æŒåˆ†é¡µï¼Œæ’é™¤base64ç¼–ç çš„Pin
            with repository._get_session() as session:
                from ..core.database.schema import Pin

                pins = session.query(Pin).filter(
                    # åŸæœ‰æ¡ä»¶ï¼šç¼ºå°‘å›¾ç‰‡URL
                    ((Pin.largest_image_url == None) |
                     (Pin.largest_image_url == '') |
                     (Pin.image_urls == None) |
                     (Pin.image_urls == '')) &
                    # æ–°å¢æ¡ä»¶ï¼šæ’é™¤base64ç¼–ç Pinï¼ˆä»¥UGluå¼€å¤´ï¼‰
                    (~Pin.id.like('UGlu%'))
                ).offset(offset).limit(batch_size).all()

                return [pin.to_dict() for pin in pins]

        except Exception as e:
            logger.error(f"åˆ†é¡µè·å–éœ€è¦å¢å¼ºçš„Pinå¤±è´¥ {keyword}: {e}")
            return []

    async def _update_enhanced_pins_batch(self, enhanced_results: List[tuple], keyword: str) -> Dict[str, int]:
        """æ‰¹é‡æ›´æ–°å¢å¼ºåçš„Pinæ•°æ®åˆ°æ•°æ®åº“

        Args:
            enhanced_results: å¢å¼ºç»“æœåˆ—è¡¨ï¼Œæ ¼å¼ä¸º [(pin_id, enhanced_data_or_none), ...]
            keyword: å…³é”®è¯

        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        stats = {"updated": 0, "failed": 0, "skipped": 0}

        # åˆ›å»ºSmartPinEnhancerå®ä¾‹æ¥å¤ç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°é€»è¾‘
        pin_enhancer = SmartPinEnhancer(self.output_dir)

        for pin_id, enhanced_data in enhanced_results:
            try:
                if enhanced_data is None:
                    # è·å–å¤±è´¥
                    stats["failed"] += 1
                    continue

                if not enhanced_data.get('image_urls'):
                    # æ²¡æœ‰è·å–åˆ°å›¾ç‰‡URL
                    stats["skipped"] += 1
                    continue

                # å¤ç”¨ç°æœ‰çš„æ•°æ®åº“æ›´æ–°é€»è¾‘
                success = await pin_enhancer._update_pin_in_database_atomic(enhanced_data, keyword)

                if success:
                    stats["updated"] += 1
                    logger.debug(f"âœ… Pin {pin_id} æ•°æ®åº“æ›´æ–°æˆåŠŸ")
                else:
                    stats["failed"] += 1
                    logger.debug(f"âŒ Pin {pin_id} æ•°æ®åº“æ›´æ–°å¤±è´¥")

            except Exception as e:
                logger.error(f"æ›´æ–°Pin {pin_id} æ—¶å‡ºé”™: {e}")
                stats["failed"] += 1

        return stats

    async def _process_pins_batch_concurrent(self, pins_batch: List[Dict], keyword: str, shared_headers: Dict[str, str]) -> Dict[str, int]:
        """å¹¶å‘å¤„ç†Pinæ‰¹æ¬¡

        Args:
            pins_batch: Pinæ‰¹æ¬¡æ•°æ®
            keyword: å…³é”®è¯
            shared_headers: å…±äº«headers

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        import concurrent.futures
        import threading
        from tqdm import tqdm

        # è¿‡æ»¤å‡ºéœ€è¦å¢å¼ºçš„Pinï¼ˆæ²¡æœ‰å›¾ç‰‡URLçš„ï¼‰
        pins_to_process = []
        for pin in pins_batch:
            if not self._has_valid_image_urls(pin):
                pins_to_process.append(pin)

        if not pins_to_process:
            logger.info("æ‰¹æ¬¡ä¸­æ²¡æœ‰éœ€è¦å¢å¼ºçš„Pin")
            return {"updated": 0, "failed": 0, "skipped": len(pins_batch)}

        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(pins_to_process)} ä¸ªPinï¼Œå¹¶å‘æ•°: {self.max_concurrent}")

        async def process_single_pin_with_browser(pin_data: Dict) -> tuple:
            """ä½¿ç”¨æµè§ˆå™¨å¤„ç†å•ä¸ªPin"""
            try:
                # æ£€æŸ¥å…¨å±€ä¸­æ–­çŠ¶æ€
                from .stage_manager import _global_interrupt_manager
                if _global_interrupt_manager.is_interrupted():
                    raise KeyboardInterrupt("ç”¨æˆ·ä¸­æ–­æ“ä½œ")

                pin_id = pin_data.get('id')

                # ä½¿ç”¨æµè§ˆå™¨è·å–Pinè¯¦æƒ…
                enhanced_data = await fetch_pin_detail_with_browser(pin_id)

                if enhanced_data and enhanced_data.get('image_urls'):
                    # åˆå¹¶Pinæ•°æ®ï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰
                    pin_enhancer = SmartPinEnhancer(self.output_dir)
                    merged_pin = pin_enhancer._merge_pin_data(pin_data, enhanced_data)
                    return (pin_id, merged_pin)
                else:
                    return (pin_id, None)

            except KeyboardInterrupt:
                raise  # é‡æ–°æŠ›å‡ºä¸­æ–­å¼‚å¸¸
            except Exception as e:
                logger.debug(f"å¤„ç†Pin {pin_data.get('id', 'unknown')} å¤±è´¥: {e}")
                return (pin_data.get('id'), None)

        # ä½¿ç”¨å¼‚æ­¥å¹¶å‘å¤„ç†
        enhanced_results = []

        # åˆ›å»ºä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(self.max_concurrent)

        async def process_with_semaphore(pin_data):
            async with semaphore:
                return await process_single_pin_with_browser(pin_data)

        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [process_with_semaphore(pin) for pin in pins_to_process]

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        from tqdm.asyncio import tqdm as atqdm

        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        try:
            enhanced_results = await atqdm.gather(*tasks, desc=f"è·å–{keyword}è¯¦æƒ…")
        except KeyboardInterrupt:
            logger.info("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å¹¶å‘å¤„ç†")
            raise

        # 4. æ‰¹é‡æ›´æ–°æ•°æ®åº“
        update_stats = await self._update_enhanced_pins_batch(enhanced_results, keyword)

        return update_stats

    def _has_valid_image_urls(self, pin: Dict) -> bool:
        """æ£€æŸ¥Pinæ˜¯å¦æœ‰æœ‰æ•ˆçš„å›¾ç‰‡URLï¼ˆå¤ç”¨SmartPinEnhancerçš„é€»è¾‘ï¼‰"""
        # æ£€æŸ¥largest_image_url
        largest_url = pin.get('largest_image_url', '').strip()
        if largest_url and largest_url.startswith('http'):
            return True

        # æ£€æŸ¥image_urlså­—å…¸
        image_urls = pin.get('image_urls', {})
        if isinstance(image_urls, str):
            try:
                import json
                image_urls = json.loads(image_urls)
            except:
                image_urls = {}

        if isinstance(image_urls, dict) and image_urls:
            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æœ‰æ•ˆçš„URL
            for key, url in image_urls.items():
                if url and isinstance(url, str) and url.strip().startswith('http'):
                    return True

        return False

    async def _enhance_pins_batch(self, pins: List[Dict], keyword: str,
                                pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """æ‰¹é‡å¢å¼ºPin - æ”¯æŒå¹¶å‘å’Œå•çº¿ç¨‹æ¨¡å¼"""
        if self.max_concurrent == 1:
            return await self._enhance_pins_sequential(pins, keyword, pin_enhancer)
        else:
            return await self._enhance_pins_concurrent(pins, keyword, pin_enhancer)

    async def _enhance_pins_sequential(self, pins: List[Dict], keyword: str,
                                     pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """å•çº¿ç¨‹é¡ºåºå¢å¼ºPinï¼ˆå›é€€æ–¹æ¡ˆï¼‰"""
        stats = {"checked": len(pins), "enhanced": 0, "failed": 0}

        for pin in pins:
            # æ£€æŸ¥ä¸­æ–­çŠ¶æ€å¹¶åœ¨å¿…è¦æ—¶æŠ›å‡ºKeyboardInterrupt
            self.check_interruption_and_raise()

            try:
                enhanced = await pin_enhancer.enhance_pin_if_needed(pin, keyword)
                if enhanced:
                    stats["enhanced"] += 1

            except Exception as e:
                logger.debug(f"å¢å¼ºPin {pin.get('id', 'unknown')} å¤±è´¥: {e}")
                stats["failed"] += 1

        return stats

    async def _enhance_pins_concurrent(self, pins: List[Dict], keyword: str,
                                     pin_enhancer: SmartPinEnhancer) -> Dict[str, int]:
        """å¤šçº¿ç¨‹å¹¶å‘å¢å¼ºPin"""
        import concurrent.futures
        import threading
        import asyncio
        from tqdm import tqdm

        stats = {"checked": len(pins), "enhanced": 0, "failed": 0}
        stats_lock = threading.Lock()

        def process_single_pin(pin_data: Dict) -> bool:
            """å•çº¿ç¨‹å¤„ç†å•ä¸ªPin"""
            try:
                # æ£€æŸ¥å…¨å±€ä¸­æ–­çŠ¶æ€
                from .stage_manager import _global_interrupt_manager
                if _global_interrupt_manager.is_interrupted():
                    raise KeyboardInterrupt("ç”¨æˆ·ä¸­æ–­æ“ä½œ")

                # åˆ›å»ºçº¿ç¨‹ä¸“ç”¨çš„enhancerå®ä¾‹
                thread_enhancer = SmartPinEnhancer(self.output_dir)

                # åŒæ­¥è°ƒç”¨enhance_pin_if_needed
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                try:
                    enhanced = loop.run_until_complete(
                        thread_enhancer.enhance_pin_if_needed(pin_data, keyword)
                    )
                    return enhanced is not None
                finally:
                    loop.close()

            except KeyboardInterrupt:
                raise  # é‡æ–°æŠ›å‡ºä¸­æ–­å¼‚å¸¸
            except Exception as e:
                logger.debug(f"çº¿ç¨‹å¤„ç†Pin {pin_data.get('id', 'unknown')} å¤±è´¥: {e}")
                return False

        logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† {len(pins)} ä¸ªPinï¼Œå¹¶å‘æ•°: {self.max_concurrent}")

        # ä½¿ç”¨ThreadPoolExecutorè¿›è¡Œå¹¶å‘å¤„ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent) as executor:
            with tqdm(total=len(pins), desc=f"å¤„ç†{keyword}", unit="pin") as pbar:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_pin = {
                    executor.submit(process_single_pin, pin): pin
                    for pin in pins
                }

                # æ”¶é›†ç»“æœ
                for future in concurrent.futures.as_completed(future_to_pin):
                    pin = future_to_pin[future]
                    try:
                        success = future.result()
                        with stats_lock:
                            if success:
                                stats["enhanced"] += 1
                            else:
                                stats["failed"] += 1
                        pbar.update(1)
                    except KeyboardInterrupt:
                        logger.info("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼Œåœæ­¢å¹¶å‘å¤„ç†")
                        # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_pin:
                            f.cancel()
                        raise
                    except Exception as e:
                        logger.error(f"Pin {pin.get('id', 'unknown')} å¤„ç†å¼‚å¸¸: {e}")
                        with stats_lock:
                            stats["failed"] += 1
                        pbar.update(1)

        logger.info(f"å¹¶å‘å¤„ç†å®Œæˆ: å¢å¼º {stats['enhanced']} ä¸ªï¼Œå¤±è´¥ {stats['failed']} ä¸ª")
        return stats

    def _get_thread_safe_headers(self, header_manager: GlobalHeaderManager) -> Dict[str, str]:
        """è·å–çº¿ç¨‹å®‰å…¨çš„Headerså‰¯æœ¬"""
        return header_manager.get_headers()

    def _validate_concurrent_setup(self) -> bool:
        """éªŒè¯å¹¶å‘è®¾ç½®æ˜¯å¦æ­£ç¡®"""
        if self.max_concurrent < 1 or self.max_concurrent > 65536:
            logger.error(f"æ— æ•ˆçš„å¹¶å‘æ•°: {self.max_concurrent}")
            return False

        # æ£€æŸ¥ç³»ç»Ÿèµ„æº
        try:
            import psutil
            available_memory = psutil.virtual_memory().available / (1024**3)  # GB
            if available_memory < 1.0:
                logger.warning(f"å¯ç”¨å†…å­˜è¾ƒä½: {available_memory:.1f}GB")

            # é«˜å¹¶å‘æ—¶çš„å†…å­˜è­¦å‘Š
            if self.max_concurrent > 1000:
                logger.warning(f"ä½¿ç”¨é«˜å¹¶å‘æ•° ({self.max_concurrent})ï¼Œè¯·ç¡®ä¿æœ‰è¶³å¤Ÿçš„ç³»ç»Ÿèµ„æº")
        except ImportError:
            logger.debug("psutilæœªå®‰è£…ï¼Œè·³è¿‡å†…å­˜æ£€æŸ¥")

        return True

    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯Pinå¢å¼ºé˜¶æ®µå®Œæ•´æ€§"""
        # è¿™ä¸ªé˜¶æ®µçš„éªŒè¯æ¯”è¾ƒå¤æ‚ï¼Œæš‚æ—¶è¿”å›True
        # å®é™…åº”ç”¨ä¸­å¯ä»¥æ£€æŸ¥å…³é”®Pinæ˜¯å¦éƒ½æœ‰äº†å›¾ç‰‡URL
        return True
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords


class ImageDownloadStage(StageManager):
    """é˜¶æ®µ4ï¼šå›¾ç‰‡æ–‡ä»¶ä¸‹è½½ - é‡æ„ç‰ˆ

    å®ç°æ¸…æ™°çš„5æ­¥å¾ªç¯é€»è¾‘ï¼š
    1. åˆ›å»ºå·²ä¸‹è½½å›¾ç‰‡piné›†åˆ
    2. ç¿»é¡µæ‰¹é‡è¯»å–å¾…ä¸‹è½½pins
    3. æ£€æŸ¥/è·å–headers
    4. å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡ï¼ˆé‡è¯•æœºåˆ¶+å•ç‚¹é”™è¯¯å®¹å¿ï¼‰
    5. æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¹¶æ›´æ–°å…¨å±€è®¡æ•°
    é‡å¤æ­¥éª¤2-5ç›´åˆ°å®Œæˆ
    """

    def __init__(self, output_dir: str, max_concurrent: int = 15, batch_size: Optional[int] = None):
        super().__init__("å›¾ç‰‡æ–‡ä»¶ä¸‹è½½", output_dir)
        self.max_concurrent = max_concurrent
        # batch_sizeåº”è¯¥ç­‰äºmax_concurrentï¼Œç¡®ä¿æœ€ä¼˜çš„èµ„æºåˆ©ç”¨
        self.batch_size = batch_size if batch_size is not None else max_concurrent
        self.global_header_manager = None

        logger.debug(f"ImageDownloadStageåˆå§‹åŒ–: max_concurrent={self.max_concurrent}, batch_size={self.batch_size}")

    async def _execute_stage(self, target_keyword: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œå›¾ç‰‡æ–‡ä»¶ä¸‹è½½ - 5æ­¥å¾ªç¯é€»è¾‘å®ç°"""
        logger.info("ğŸ“¥ å¼€å§‹å›¾ç‰‡æ–‡ä»¶ä¸‹è½½é˜¶æ®µ - 5æ­¥å¾ªç¯é€»è¾‘")
        logger.info("=" * 60)
        logger.info("æ­¥éª¤1: åˆ›å»ºå·²ä¸‹è½½å›¾ç‰‡piné›†åˆ")
        logger.info("æ­¥éª¤2: ç¿»é¡µæ‰¹é‡è¯»å–å¾…ä¸‹è½½pins")
        logger.info("æ­¥éª¤3: æ£€æŸ¥/è·å–headers")
        logger.info("æ­¥éª¤4: å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡ï¼ˆé‡è¯•+å®¹é”™ï¼‰")
        logger.info("æ­¥éª¤5: æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¹¶æ›´æ–°å…¨å±€è®¡æ•°")
        logger.info("=" * 60)

        # åˆå§‹åŒ–å…¨å±€Headerç®¡ç†å™¨
        from .global_header_manager import GlobalHeaderManager
        self.global_header_manager = GlobalHeaderManager(self.output_dir)

        # å‘ç°éœ€è¦å¤„ç†çš„å…³é”®è¯
        if target_keyword:
            keywords = [target_keyword]
        else:
            keywords = self._discover_all_keywords()

        # å…¨å±€ç»Ÿè®¡
        download_stats = {
            "keywords_processed": 0,
            "total_downloaded": 0,
            "total_failed": 0,
            "total_batches": 0,
            "keyword_details": {}
        }

        # ä¸ºæ¯ä¸ªå…³é”®è¯æ‰§è¡Œ5æ­¥å¾ªç¯é€»è¾‘
        for keyword in keywords:
            self.check_interruption_and_raise()

            logger.info(f"ğŸ¯ å¼€å§‹å¤„ç†å…³é”®è¯: {keyword}")
            keyword_stats = await self._process_keyword_with_5_steps(keyword)

            # æ›´æ–°å…¨å±€ç»Ÿè®¡
            download_stats["keywords_processed"] += 1
            download_stats["total_downloaded"] += keyword_stats["downloaded"]
            download_stats["total_failed"] += keyword_stats["failed"]
            download_stats["total_batches"] += keyword_stats["batches"]
            download_stats["keyword_details"][keyword] = keyword_stats

            logger.info(f"âœ… å…³é”®è¯ {keyword} å®Œæˆ: ä¸‹è½½ {keyword_stats['downloaded']} æˆåŠŸ, {keyword_stats['failed']} å¤±è´¥")

        logger.info(f"ğŸ“¥ å›¾ç‰‡ä¸‹è½½é˜¶æ®µå®Œæˆ: {download_stats}")
        return self._generate_success_result({"download_stats": download_stats})

    async def _process_keyword_with_5_steps(self, keyword: str) -> Dict[str, Any]:
        """ä¸ºå•ä¸ªå…³é”®è¯æ‰§è¡Œ5æ­¥å¾ªç¯é€»è¾‘

        Args:
            keyword: å…³é”®è¯

        Returns:
            å…³é”®è¯å¤„ç†ç»Ÿè®¡ç»“æœ
        """
        from ..core.database.repository import SQLiteRepository
        import os
        from tqdm import tqdm

        # åˆå§‹åŒ–ç»Ÿè®¡
        keyword_stats = {
            "downloaded": 0,
            "failed": 0,
            "batches": 0,
            "total_pins_with_images": 0,
            "already_downloaded": 0
        }

        try:
            # åˆ›å»ºRepository
            repository = SQLiteRepository(keyword=keyword, output_dir=self.output_dir)
            images_dir = os.path.join(self.output_dir, keyword, "images")
            os.makedirs(images_dir, exist_ok=True)

            # ã€æ­¥éª¤1ã€‘åˆ›å»ºå·²ä¸‹è½½å›¾ç‰‡piné›†åˆ
            logger.info(f"ğŸ“‹ æ­¥éª¤1: åˆ›å»ºå·²ä¸‹è½½å›¾ç‰‡piné›†åˆ - {keyword}")
            downloaded_pins_set = self._build_downloaded_pins_set(images_dir)
            keyword_stats["already_downloaded"] = len(downloaded_pins_set)
            logger.info(f"   å·²ä¸‹è½½å›¾ç‰‡: {len(downloaded_pins_set)} ä¸ª")

            # è·å–æ€»æ•°ç”¨äºè¿›åº¦æ¡
            total_pins = self._get_total_pins_with_images(repository, keyword)
            keyword_stats["total_pins_with_images"] = total_pins
            logger.info(f"   æ•°æ®åº“ä¸­æœ‰å›¾ç‰‡URLçš„Pinæ€»æ•°: {total_pins} ä¸ª")

            if total_pins == 0:
                logger.info(f"   å…³é”®è¯ {keyword} æ²¡æœ‰å‘ç°æœ‰å›¾ç‰‡URLçš„Pinï¼Œè·³è¿‡")
                return keyword_stats

            # åˆå§‹åŒ–ç¿»é¡µå‚æ•°
            offset = 0
            batch_count = 0

            # åˆ›å»ºè¿›åº¦æ¡
            with tqdm(total=total_pins, desc=f"ä¸‹è½½ {keyword}", unit="pin") as pbar:
                # æ›´æ–°å·²ä¸‹è½½çš„è¿›åº¦
                pbar.update(len(downloaded_pins_set))

                # ã€ä¸»å¾ªç¯ã€‘é‡å¤æ­¥éª¤2-5ç›´åˆ°å®Œæˆ
                while True:
                    self.check_interruption_and_raise()

                    # ã€æ­¥éª¤2ã€‘ç¿»é¡µæ‰¹é‡è¯»å–å¾…ä¸‹è½½pins
                    logger.debug(f"ğŸ“– æ­¥éª¤2: ç¿»é¡µè¯»å–å¾…ä¸‹è½½pins (offset: {offset}, batch: {self.batch_size})")
                    pins_batch = repository.load_pins_with_images(
                        keyword,
                        limit=self.batch_size,
                        offset=offset
                    )

                    if not pins_batch:
                        logger.info(f"   æ²¡æœ‰æ›´å¤šPinæ•°æ®ï¼Œç¿»é¡µå®Œæˆ")
                        break

                    # è¿‡æ»¤å·²ä¸‹è½½çš„pins
                    missing_pins = [pin for pin in pins_batch if pin['id'] not in downloaded_pins_set]
                    logger.debug(f"   æœ¬æ‰¹æ¬¡: {len(pins_batch)} ä¸ªPinï¼Œå¾…ä¸‹è½½: {len(missing_pins)} ä¸ª")

                    if not missing_pins:
                        logger.debug(f"   æœ¬æ‰¹æ¬¡æ— å¾…ä¸‹è½½Pinï¼Œè·³åˆ°ä¸‹ä¸€æ‰¹æ¬¡")
                        offset += len(pins_batch)
                        continue

                    # ã€æ­¥éª¤3ã€‘æ£€æŸ¥/è·å–headers
                    logger.debug(f"ğŸŒ æ­¥éª¤3: æ£€æŸ¥/è·å–headers")
                    headers_ready = await self._ensure_headers_ready()
                    if not headers_ready:
                        logger.warning(f"   Headersè·å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤headers")

                    # ã€æ­¥éª¤4ã€‘å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡
                    logger.debug(f"â¬‡ï¸ æ­¥éª¤4: å¤šçº¿ç¨‹ä¸‹è½½ {len(missing_pins)} ä¸ªå›¾ç‰‡")
                    batch_results = await self._download_batch_with_retry(missing_pins, keyword, images_dir)

                    # ã€æ­¥éª¤5ã€‘æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¹¶æ›´æ–°å…¨å±€è®¡æ•°
                    logger.debug(f"âœ… æ­¥éª¤5: æ£€æŸ¥æ–‡ä»¶å¹¶æ›´æ–°è®¡æ•°")
                    batch_downloaded, batch_failed = self._verify_and_update_stats(batch_results, downloaded_pins_set)

                    # æ›´æ–°ç»Ÿè®¡
                    keyword_stats["downloaded"] += batch_downloaded
                    keyword_stats["failed"] += batch_failed
                    keyword_stats["batches"] += 1
                    batch_count += 1

                    # æ›´æ–°è¿›åº¦æ¡
                    pbar.update(batch_downloaded)

                    logger.debug(f"   æ‰¹æ¬¡ {batch_count} å®Œæˆ: {batch_downloaded}/{len(missing_pins)} æˆåŠŸ")

                    # æ›´æ–°åç§»é‡
                    offset += len(pins_batch)

                    # å¦‚æœè¿”å›çš„Pinæ•°é‡å°‘äºæ‰¹æ¬¡å¤§å°ï¼Œè¯´æ˜å·²åˆ°æœ«å°¾
                    if len(pins_batch) < self.batch_size:
                        logger.info(f"   å·²å¤„ç†å®Œæ‰€æœ‰Pinï¼Œç¿»é¡µç»“æŸ")
                        break

            logger.info(f"ğŸ¯ å…³é”®è¯ {keyword} å¤„ç†å®Œæˆ: æ€»æ‰¹æ¬¡ {batch_count}, ä¸‹è½½ {keyword_stats['downloaded']} æˆåŠŸ")
            return keyword_stats

        except Exception as e:
            logger.error(f"âŒ å¤„ç†å…³é”®è¯ {keyword} æ—¶å‡ºé”™: {e}")
            keyword_stats["error"] = str(e)
            return keyword_stats

    def _build_downloaded_pins_set(self, images_dir: str) -> set:
        """ã€æ­¥éª¤1ã€‘å»ºç«‹å·²ä¸‹è½½å›¾ç‰‡çš„Pinç´¢å¼•é›†åˆ

        Args:
            images_dir: å›¾ç‰‡ç›®å½•è·¯å¾„

        Returns:
            å·²ä¸‹è½½å›¾ç‰‡çš„Pin IDé›†åˆ
        """
        downloaded_pins = set()

        if not os.path.exists(images_dir):
            logger.debug(f"å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: {images_dir}")
            return downloaded_pins

        try:
            # æ‰«æimagesç›®å½•ï¼Œæå–å·²ä¸‹è½½çš„pin_id
            for filename in os.listdir(images_dir):
                if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.webp')):
                    # ä»æ–‡ä»¶åæå–Pin IDï¼ˆå»æ‰æ‰©å±•åï¼‰
                    pin_id = os.path.splitext(filename)[0]
                    # éªŒè¯æ–‡ä»¶æœ‰æ•ˆæ€§ï¼ˆå¤§å°æ£€æŸ¥ï¼‰
                    file_path = os.path.join(images_dir, filename)
                    if os.path.getsize(file_path) > 1000:  # è‡³å°‘1KB
                        downloaded_pins.add(pin_id)
                    else:
                        logger.debug(f"å‘ç°æ— æ•ˆå›¾ç‰‡æ–‡ä»¶: {filename} (å¤§å°: {os.path.getsize(file_path)} bytes)")

            logger.debug(f"å»ºç«‹å·²ä¸‹è½½ç´¢å¼•: {len(downloaded_pins)} ä¸ªæœ‰æ•ˆæ–‡ä»¶")

        except Exception as e:
            logger.error(f"å»ºç«‹å·²ä¸‹è½½ç´¢å¼•å¤±è´¥: {e}")

        return downloaded_pins

    def _get_total_pins_with_images(self, repository, keyword: str) -> int:
        """è·å–æœ‰å›¾ç‰‡URLçš„Pinæ€»æ•°"""
        try:
            all_pins = repository.load_pins_with_images(keyword)
            return len(all_pins)
        except Exception as e:
            logger.error(f"è·å–Pinæ€»æ•°å¤±è´¥: {e}")
            return 0

    async def _ensure_headers_ready(self) -> bool:
        """ã€æ­¥éª¤3ã€‘ç¡®ä¿Headerså·²å‡†å¤‡å°±ç»ª

        Returns:
            æ˜¯å¦æˆåŠŸè·å–Headers
        """
        try:
            if self.global_header_manager:
                return await self.global_header_manager.ensure_headers_ready()
            else:
                logger.warning("GlobalHeaderManageræœªåˆå§‹åŒ–")
                return False
        except Exception as e:
            logger.error(f"Headerså‡†å¤‡å¤±è´¥: {e}")
            return False

    async def _download_batch_with_retry(self, pins_batch: List[Dict], keyword: str, images_dir: str) -> List[Dict]:
        """ã€æ­¥éª¤4ã€‘å¤šçº¿ç¨‹ä¸‹è½½å›¾ç‰‡æ‰¹æ¬¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶å’Œå•ç‚¹é”™è¯¯å®¹å¿ï¼‰

        Args:
            pins_batch: Pinæ•°æ®æ‰¹æ¬¡
            keyword: å…³é”®è¯
            images_dir: å›¾ç‰‡ç›®å½•

        Returns:
            ä¸‹è½½ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« {'pin_id': str, 'success': bool, 'message': str, 'file_path': str}
        """
        import asyncio
        import concurrent.futures
        from concurrent.futures import ThreadPoolExecutor

        # è·å–headers
        headers = self.global_header_manager.get_headers() if self.global_header_manager else {}

        # åˆ›å»ºä¸‹è½½ä»»åŠ¡
        download_tasks = []
        for pin in pins_batch:
            pin_id = pin.get('id')
            if not pin_id:
                continue

            # æå–å›¾ç‰‡URLs
            image_urls = self._extract_image_urls(pin)
            if not image_urls:
                continue

            # ç”Ÿæˆæ–‡ä»¶è·¯å¾„
            file_path = os.path.join(images_dir, f"{pin_id}.jpg")

            download_tasks.append({
                'pin_id': pin_id,
                'image_urls': image_urls,
                'file_path': file_path,
                'headers': headers.copy()
            })

        # ä½¿ç”¨çº¿ç¨‹æ± è¿›è¡Œå¤šçº¿ç¨‹ä¸‹è½½
        results = []
        max_workers = min(self.max_concurrent, len(download_tasks))

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä¸‹è½½ä»»åŠ¡
            future_to_task = {
                executor.submit(self._download_single_pin_sync, task): task
                for task in download_tasks
            }

            # æ”¶é›†ç»“æœï¼ˆå•ç‚¹é”™è¯¯å®¹å¿ï¼‰
            for future in concurrent.futures.as_completed(future_to_task):
                task = future_to_task[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    # å•ç‚¹é”™è¯¯å®¹å¿ï¼šå•ä¸ªpinä¸‹è½½å¤±è´¥ä¸å½±å“å…¶ä»–pin
                    logger.debug(f"Pin {task['pin_id']} ä¸‹è½½å¼‚å¸¸: {e}")
                    results.append({
                        'pin_id': task['pin_id'],
                        'success': False,
                        'message': f"ä¸‹è½½å¼‚å¸¸: {e}",
                        'file_path': task['file_path']
                    })

        return results

    def _download_single_pin_sync(self, task: Dict) -> Dict:
        """åŒæ­¥ä¸‹è½½å•ä¸ªPinçš„å›¾ç‰‡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

        Args:
            task: ä¸‹è½½ä»»åŠ¡ï¼ŒåŒ…å«pin_id, image_urls, file_path, headers

        Returns:
            ä¸‹è½½ç»“æœå­—å…¸
        """
        pin_id = task['pin_id']
        image_urls = task['image_urls']
        file_path = task['file_path']
        headers = task['headers']

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:
            return {
                'pin_id': pin_id,
                'success': True,
                'message': "æ–‡ä»¶å·²å­˜åœ¨",
                'file_path': file_path
            }

        # å°è¯•ä¸‹è½½æ¯ä¸ªURLï¼ˆé‡è¯•æœºåˆ¶ï¼‰
        max_retries = 3
        for i, url in enumerate(image_urls):
            for retry in range(max_retries):
                try:
                    # ä½¿ç”¨ç°æœ‰çš„ä¸‹è½½å‡½æ•°
                    from ..utils.downloader import download_image
                    success = download_image(url, file_path, headers, timeout=30, max_retries=1)

                    if success and os.path.exists(file_path) and os.path.getsize(file_path) > 1000:
                        return {
                            'pin_id': pin_id,
                            'success': True,
                            'message': f"ä¸‹è½½æˆåŠŸ (URL {i+1}, é‡è¯• {retry+1})",
                            'file_path': file_path
                        }

                except Exception as e:
                    logger.debug(f"Pin {pin_id} URL {i+1} é‡è¯• {retry+1} å¤±è´¥: {e}")

                # é‡è¯•é—´éš”
                if retry < max_retries - 1:
                    import time
                    time.sleep(0.5 * (retry + 1))  # é€’å¢å»¶è¿Ÿ

        return {
            'pin_id': pin_id,
            'success': False,
            'message': f"æ‰€æœ‰URLéƒ½ä¸‹è½½å¤±è´¥ ({len(image_urls)} ä¸ªå°è¯•)",
            'file_path': file_path
        }

    def _verify_and_update_stats(self, batch_results: List[Dict], downloaded_pins_set: set) -> tuple:
        """ã€æ­¥éª¤5ã€‘æ£€æŸ¥æœ¬åœ°æ–‡ä»¶å¹¶æ›´æ–°å…¨å±€è®¡æ•°

        Args:
            batch_results: æ‰¹æ¬¡ä¸‹è½½ç»“æœ
            downloaded_pins_set: å·²ä¸‹è½½pinsé›†åˆï¼ˆä¼šè¢«æ›´æ–°ï¼‰

        Returns:
            (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
        """
        downloaded_count = 0
        failed_count = 0

        for result in batch_results:
            pin_id = result['pin_id']
            success = result['success']
            file_path = result['file_path']

            if success:
                # éªŒè¯æ–‡ä»¶ç¡®å®å­˜åœ¨ä¸”æœ‰æ•ˆ
                if os.path.exists(file_path) and os.path.getsize(file_path) > 1000:
                    downloaded_count += 1
                    downloaded_pins_set.add(pin_id)  # æ›´æ–°å·²ä¸‹è½½é›†åˆ
                    logger.debug(f"âœ… Pin {pin_id} ä¸‹è½½æˆåŠŸå¹¶éªŒè¯")
                else:
                    failed_count += 1
                    logger.debug(f"âŒ Pin {pin_id} ä¸‹è½½æŠ¥å‘ŠæˆåŠŸä½†æ–‡ä»¶æ— æ•ˆ")
            else:
                failed_count += 1
                logger.debug(f"âŒ Pin {pin_id} ä¸‹è½½å¤±è´¥: {result['message']}")

        return downloaded_count, failed_count

    def _extract_image_urls(self, pin: Dict) -> List[str]:
        """ä»Pinæ•°æ®ä¸­æå–å›¾ç‰‡URLs

        Args:
            pin: Pinæ•°æ®å­—å…¸

        Returns:
            å›¾ç‰‡URLåˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
        """
        urls = []

        # ä¼˜å…ˆä½¿ç”¨largest_image_url
        largest_url = pin.get('largest_image_url')
        if largest_url and largest_url.startswith('http'):
            urls.append(largest_url)

        # è§£æimage_urls JSON
        image_urls_json = pin.get('image_urls')
        if image_urls_json:
            try:
                import json
                image_urls_dict = json.loads(image_urls_json) if isinstance(image_urls_json, str) else image_urls_json

                # æŒ‰ä¼˜å…ˆçº§é¡ºåºæå–URL
                size_priorities = ["original", "1200", "736", "564", "474", "236", "170"]
                for size in size_priorities:
                    if size in image_urls_dict:
                        url = image_urls_dict[size]
                        if url and url.startswith('http') and url not in urls:
                            urls.append(url)

            except Exception as e:
                logger.debug(f"è§£æimage_urlså¤±è´¥: {e}")

        return urls

    async def _verify_stage_completion(self) -> bool:
        """éªŒè¯å›¾ç‰‡ä¸‹è½½é˜¶æ®µå®Œæ•´æ€§"""
        # å¯ä»¥æ£€æŸ¥å…³é”®å›¾ç‰‡æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å›True
        return True
    
    def _discover_all_keywords(self) -> List[str]:
        """å‘ç°æ‰€æœ‰å…³é”®è¯"""
        keywords = []
        try:
            if os.path.exists(self.output_dir):
                for item in os.listdir(self.output_dir):
                    item_path = os.path.join(self.output_dir, item)
                    if os.path.isdir(item_path):
                        db_path = os.path.join(item_path, "pinterest.db")
                        if os.path.exists(db_path):
                            keywords.append(item)
        except Exception as e:
            logger.error(f"å‘ç°å…³é”®è¯æ—¶å‡ºé”™: {e}")
        
        return keywords
