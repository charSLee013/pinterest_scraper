#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Pinterestçˆ¬è™«æ ¸å¿ƒæ¨¡å— v3.3

é«˜æ€§èƒ½Pinterestæ•°æ®é‡‡é›†å·¥å…·ï¼Œé›†æˆçœŸå®æµè§ˆå™¨ä¼šè¯åçˆ¬è™«æŠ€æœ¯å’Œ4å€æ€§èƒ½ä¼˜åŒ–ã€‚

æ ¸å¿ƒç‰¹æ€§ï¼š
- å·¥ä¸šçº§åçˆ¬è™«ï¼šçœŸå®æµè§ˆå™¨ä¼šè¯ + æ™ºèƒ½Headersï¼Œ100%çªç ´Pinteresté˜²æŠ¤
- æè‡´æ€§èƒ½ï¼š4å€é€Ÿåº¦æå‡ï¼Œ1.01å¼ /ç§’ä¸‹è½½é€Ÿåº¦ï¼Œ15ä¸ªå¹¶å‘åç¨‹
- æ™ºèƒ½å›é€€ï¼šå¤šå±‚URLå›é€€æœºåˆ¶ï¼Œç¡®ä¿æ¯å¼ å›¾ç‰‡éƒ½èƒ½æˆåŠŸä¸‹è½½
- ç»Ÿä¸€æ¥å£ï¼šæ•°æ®é‡‡é›†å’Œå›¾ç‰‡ä¸‹è½½ä¸€ä½“åŒ–ï¼Œç®€åŒ–ç”¨æˆ·æ“ä½œ

æŠ€æœ¯ä¼˜åŠ¿ï¼š
- çªç ´Pinterestä¼ ç»Ÿ800ä¸ªPiné™åˆ¶
- æ”¯æŒå¤§è§„æ¨¡æ•°æ®é‡‡é›†ï¼ˆ2000+ Pinsï¼‰
- æ™ºèƒ½å»é‡å’Œè´¨é‡ä¿è¯
- æ–­ç‚¹ç»­ä¼ å’Œé”™è¯¯æ¢å¤
- ç°ä»£å¼‚æ­¥æ¶æ„ï¼ŒåŸºäºPatchright
"""

import os
import json
import asyncio
from typing import Dict, List, Optional

from loguru import logger

from .smart_scraper import SmartScraper
from .database.repository import SQLiteRepository
from .download.task_manager import DownloadTaskManager
from .process_manager import ProcessManager
from ..utils import utils


class PinterestScraper:
    """Pinterestçˆ¬è™«ä¸»ç±»

    é«˜æ€§èƒ½Pinterestæ•°æ®é‡‡é›†å·¥å…·ï¼Œé›†æˆçœŸå®æµè§ˆå™¨ä¼šè¯åçˆ¬è™«æŠ€æœ¯ã€‚
    æä¾›ç»Ÿä¸€çš„æ•°æ®é‡‡é›†å’Œå›¾ç‰‡ä¸‹è½½æ¥å£ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®é‡‡é›†ã€‚

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ•°æ®é‡‡é›†ï¼šæ”¯æŒå…³é”®è¯æœç´¢å’ŒURLé‡‡é›†
    - å›¾ç‰‡ä¸‹è½½ï¼šé›†æˆé«˜æ€§èƒ½å¼‚æ­¥ä¸‹è½½å™¨
    - åçˆ¬è™«ï¼šçœŸå®æµè§ˆå™¨ä¼šè¯çªç ´Pinteresté˜²æŠ¤
    - æ€§èƒ½ä¼˜åŒ–ï¼š4å€é€Ÿåº¦æå‡ï¼Œ1.01å¼ /ç§’ä¸‹è½½é€Ÿåº¦

    Example:
        >>> scraper = PinterestScraper(prefer_requests=True)
        >>> pins = await scraper.scrape(query="nature", count=100)
        >>> print(f"é‡‡é›†åˆ° {len(pins)} ä¸ªPinï¼Œå›¾ç‰‡ä¸‹è½½å®Œæˆ")
    """

    def __init__(
        self,
        output_dir: str = "output",
        download_images: bool = True,
        proxy: Optional[str] = None,
        debug: bool = False,
        prefer_requests: bool = False,
        max_concurrent: int = 15
    ):
        """åˆå§‹åŒ–Pinterestçˆ¬è™«

        Args:
            output_dir: è¾“å‡ºç›®å½•
            download_images: æ˜¯å¦ä¸‹è½½å›¾ç‰‡
            proxy: ä»£ç†æœåŠ¡å™¨
            debug: è°ƒè¯•æ¨¡å¼
            prefer_requests: æ˜¯å¦å¯ç”¨æ€§èƒ½æ¨¡å¼ï¼ˆæ¨èï¼Œ4å€é€Ÿåº¦æå‡ï¼‰
            max_concurrent: æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
        """
        self.output_dir = output_dir
        self.download_images = download_images
        self.proxy = proxy
        self.prefer_requests = prefer_requests
        self.debug = debug
        self.max_concurrent = max_concurrent

        # æ³¨æ„ï¼šæ•°æ®åº“å’Œä¸‹è½½ç®¡ç†å™¨ç°åœ¨åœ¨scrapeæ–¹æ³•ä¸­æŒ‰å…³é”®è¯åˆ›å»º
        # è¿™æ ·å¯ä»¥ç¡®ä¿æ¯ä¸ªå…³é”®è¯ä½¿ç”¨ç‹¬ç«‹çš„æ•°æ®åº“æ–‡ä»¶
        self.repository = None
        self.download_manager = None
        self.process_manager = None
        # ä¼ é€’ä»£ç†è®¾ç½®ç»™ä¸‹è½½å™¨
        if proxy:
            self.download_manager.downloader.proxy = proxy

        # æ™ºèƒ½é‡‡é›†å¼•æ“
        self.scraper = SmartScraper(
            proxy=proxy,
            debug=debug
        )

        logger.debug("Pinterestçˆ¬è™«åˆå§‹åŒ–å®Œæˆ")

    async def scrape(
        self,
        query: Optional[str] = None,
        url: Optional[str] = None,
        count: int = 50
    ) -> List[Dict]:
        """ç»Ÿä¸€çš„Pinterestæ•°æ®é‡‡é›†æ¥å£

        Args:
            query: æœç´¢å…³é”®è¯
            url: Pinterest URL
            count: ç›®æ ‡æ•°é‡

        Returns:
            é‡‡é›†åˆ°çš„Pinæ•°æ®åˆ—è¡¨
        """
        if not query and not url:
            logger.error("å¿…é¡»æä¾›queryæˆ–urlå‚æ•°")
            return []

        logger.debug(f"å¼€å§‹Pinterestæ•°æ®é‡‡é›†")
        logger.debug(f"å‚æ•°: query={query}, url={url}, count={count}")

        # è®¾ç½®å·¥ä½œç›®å½•
        work_name = utils.sanitize_filename(query or url.split('/')[-1] or 'scrape')
        work_dirs = utils.setup_directories(self.output_dir, work_name, self.debug)
        work_dir = work_dirs.get('term_root', work_dirs['root'])

        # è·å–è¿›ç¨‹é”ï¼Œé˜²æ­¢å¤šå®ä¾‹åŒæ—¶å¤„ç†ç›¸åŒå…³é”®è¯
        self.process_manager = ProcessManager(work_name, self.output_dir)
        if not self.process_manager.acquire_lock():
            logger.error(f"æ— æ³•å¯åŠ¨é‡‡é›†ä»»åŠ¡ï¼Œæ£€æµ‹åˆ°å…¶ä»–å®ä¾‹æ­£åœ¨å¤„ç†: {work_name}")
            logger.info("è¯·ç­‰å¾…å…¶ä»–å®ä¾‹å®Œæˆï¼Œæˆ–æ£€æŸ¥æ˜¯å¦æœ‰åƒµå°¸è¿›ç¨‹")
            return []

        try:
            # åˆ›å»ºå…³é”®è¯ç‰¹å®šçš„repository
            try:
                self.repository = SQLiteRepository(keyword=work_name, output_dir=self.output_dir)
                # æµ‹è¯•æ•°æ®åº“è¿æ¥
                self.repository.load_pins_by_query(work_name, limit=1)
                logger.debug(f"æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ: {work_name}")
            except Exception as e:
                logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
                # é‡æ–°åˆ›å»ºrepositoryï¼Œè§¦å‘æ•°æ®åº“åˆå§‹åŒ–
                self.repository = SQLiteRepository(keyword=work_name, output_dir=self.output_dir)

            # åˆ›å»ºå…³é”®è¯ç‰¹å®šçš„ä¸‹è½½ç®¡ç†å™¨
            self.download_manager = DownloadTaskManager(
                keyword=work_name,
                output_dir=self.output_dir,
                max_concurrent=self.max_concurrent,
                auto_start=False,
                prefer_requests=self.prefer_requests
            )
            # ä¼ é€’ä»£ç†è®¾ç½®ç»™ä¸‹è½½å™¨
            if self.proxy:
                self.download_manager.downloader.proxy = self.proxy

            # æ£€æŸ¥æ˜¯å¦æœ‰æœªå®Œæˆçš„ä¼šè¯éœ€è¦æ¢å¤
            session_id = await self._check_and_resume_session(work_name, count, work_dir)

            if not session_id:
                # æ£€æŸ¥æ˜¯å¦æœ‰å·²å®Œæˆçš„æ•°æ®ä½†æ•°é‡ä¸è¶³
                existing_pins = self.repository.load_pins_by_query(work_name, limit=None)
                if existing_pins and len(existing_pins) < count:
                    logger.info(f"ğŸ”„ å‘ç°å·²æœ‰æ•°æ®ä½†æ•°é‡ä¸è¶³: {len(existing_pins)}/{count} ä¸ªPin")
                    logger.info(f"ğŸ“ˆ å°†ç»§ç»­é‡‡é›†å‰©ä½™çš„ {count - len(existing_pins)} ä¸ªPin")

                # åˆ›å»ºæ–°çš„é‡‡é›†ä¼šè¯
                session_id = self.repository.create_scraping_session(
                    query=work_name,
                    target_count=count,
                    output_dir=work_dir,
                    download_images=self.download_images
                )

            # ä»æ•°æ®åº“åŠ è½½ç¼“å­˜æ•°æ®
            cached_pins = self.repository.load_pins_by_query(work_name)
            logger.debug(f"ğŸ” æ•°æ®åº“æ£€æŸ¥: å·²æœ‰ {len(cached_pins)} ä¸ªpinsï¼Œç›®æ ‡ {count} ä¸ªpins")

            if len(cached_pins) >= count:
                logger.info(f"âœ… æ•°æ®åº“ä¸­å·²æœ‰ {len(cached_pins)} ä¸ªpinsï¼Œæ»¡è¶³ç›®æ ‡ {count} ä¸ªï¼Œç›´æ¥ä½¿ç”¨")
                # æ›´æ–°ä¼šè¯çŠ¶æ€
                self.repository.update_session_status(session_id, 'completed', len(cached_pins))
                return await self._finalize_results(cached_pins[:count], work_dir, session_id)

            # è®¡ç®—å®é™…éœ€è¦é‡‡é›†çš„æ•°é‡ï¼ˆå¢é‡é‡‡é›†ï¼‰
            cached_count = len(cached_pins)
            remaining_count = count - cached_count

            if cached_count > 0:
                logger.info(f"æ•°æ®åº“ä¸­å·²æœ‰ {cached_count} ä¸ªpinsï¼Œè¿˜éœ€è¦é‡‡é›† {remaining_count} ä¸ª")

            # æ‰§è¡Œæ™ºèƒ½é‡‡é›†ï¼ˆåªé‡‡é›†å‰©ä½™æ•°é‡ï¼‰- å¯ç”¨å®æ—¶ä¿å­˜
            new_pins = await self.scraper.scrape(
                query=query,
                url=url,
                target_count=remaining_count,
                repository=self.repository,
                session_id=session_id
            )

            # å®æ—¶ä¿å­˜å·²å®Œæˆï¼Œæ‰€æœ‰æ•°æ®éƒ½å·²ç›´æ¥å†™å…¥æ•°æ®åº“
            if new_pins:
                logger.debug(f"å®æ—¶ä¿å­˜å®Œæˆ: {len(new_pins)} ä¸ªPin")

            # é‡æ–°ä»æ•°æ®åº“åŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
            all_pins = self.repository.load_pins_by_query(work_name, limit=None)

            # ç¡®ä¿ä¸è¶…è¿‡ç›®æ ‡æ•°é‡
            final_pins = all_pins[:count]

            logger.info(f"æ•°æ®é‡‡é›†å®Œæˆ: {cached_count} + {len(new_pins)} = {len(final_pins)} ä¸ªpins")

            # æ›´æ–°ä¼šè¯çŠ¶æ€
            self.repository.update_session_status(session_id, 'completed', len(final_pins))

            return await self._finalize_results(final_pins, work_dir, session_id)

        except KeyboardInterrupt:
            logger.warning("æ£€æµ‹åˆ°ç”¨æˆ·ä¸­æ–­ï¼Œæ•°æ®å·²å®æ—¶ä¿å­˜åˆ°æ•°æ®åº“...")
            # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºä¸­æ–­
            if hasattr(self, 'repository') and self.repository and 'session_id' in locals():
                saved_count = len(self.repository.load_pins_by_query(work_name))
                self.repository.update_session_status(session_id, 'interrupted', saved_count)
                logger.info(f"ä¼šè¯çŠ¶æ€å·²æ›´æ–°ä¸ºä¸­æ–­ï¼Œå·²ä¿å­˜ {saved_count} ä¸ªPin")
            raise
        except Exception as e:
            logger.error(f"é‡‡é›†è¿‡ç¨‹å‡ºé”™: {e}")
            # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºå¤±è´¥
            if hasattr(self, 'repository') and self.repository and 'session_id' in locals():
                self.repository.update_session_status(session_id, 'failed', 0)
            raise
        finally:
            # ç¡®ä¿é‡Šæ”¾è¿›ç¨‹é”
            if self.process_manager:
                self.process_manager.release_lock()

    # æ³¨æ„ï¼š_load_cache å’Œ _merge_pins æ–¹æ³•å·²è¢«æ•°æ®åº“Repositoryæ›¿ä»£
    # æ•°æ®åº“è‡ªåŠ¨å¤„ç†ç¼“å­˜å’Œå»é‡é€»è¾‘

    async def _finalize_results(self, pins: List[Dict], work_dir: str, session_id: str) -> List[Dict]:
        """å®Œæˆç»“æœå¤„ç†ï¼šä¿å­˜æ•°æ®å’Œè°ƒåº¦å¼‚æ­¥ä¸‹è½½"""
        if not pins:
            logger.warning("æ²¡æœ‰è·å–åˆ°ä»»ä½•Pinæ•°æ®")
            return []

        # ä¿å­˜JSONæ•°æ®ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        self._save_pins_json(pins, work_dir)

        # è°ƒåº¦å¼‚æ­¥å›¾ç‰‡ä¸‹è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.download_images:
            await self._schedule_async_downloads(pins, work_dir)

        # è¾“å‡ºç®€å•ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—
        self._log_simple_stats(pins)

        logger.info(f"é‡‡é›†å®Œæˆ: {len(pins)} ä¸ªPin")
        return pins

    def _save_pins_json(self, pins: List[Dict], work_dir: str):
        """ä¿å­˜Pinæ•°æ®ä¸ºJSONæ–‡ä»¶"""
        json_file = os.path.join(work_dir, "pins.json")
        try:
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(pins, f, ensure_ascii=False, indent=2)
            logger.info(f"Pinæ•°æ®å·²ä¿å­˜åˆ°: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")

    async def _schedule_async_downloads(self, pins: List[Dict], work_dir: str):
        """è°ƒåº¦å¼‚æ­¥å›¾ç‰‡ä¸‹è½½"""
        if not pins:
            return

        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        images_dir = os.path.join(work_dir, "images")
        os.makedirs(images_dir, exist_ok=True)

        # ç»Ÿè®¡æœ‰å›¾ç‰‡URLçš„Pinæ•°é‡
        pins_with_images = [pin for pin in pins if pin.get('largest_image_url')]
        logger.info(f"å‡†å¤‡ä¸‹è½½: {len(pins_with_images)}/{len(pins)} ä¸ªPinæœ‰å›¾ç‰‡URL")

        # å¯åŠ¨å¼‚æ­¥ä¸‹è½½ä»»åŠ¡
        try:
            # å¯åŠ¨ä¸‹è½½ç®¡ç†å™¨
            await self.download_manager.start()

            # è°ƒåº¦ä¸‹è½½ä»»åŠ¡
            scheduled_count = await self.download_manager.schedule_pin_downloads(pins, work_dir)
            logger.info(f"å·²è°ƒåº¦ {scheduled_count} ä¸ªä¸‹è½½ä»»åŠ¡")

            # ä¸‹è½½ä»»åŠ¡å°†åœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œä¸»ç¨‹åºå¯ä»¥é€šè¿‡wait_for_downloads_completionç­‰å¾…å®Œæˆ

        except Exception as e:
            logger.error(f"è°ƒåº¦ä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}")
            logger.warning("ä¸‹è½½ä»»åŠ¡è°ƒåº¦å¤±è´¥")

    def _fallback_sync_download(self, pins: List[Dict], work_dir: str):
        """å›é€€åˆ°åŒæ­¥ä¸‹è½½ï¼ˆå…¼å®¹æ€§ä¿éšœï¼‰"""
        logger.warning("å›é€€åˆ°åŒæ­¥ä¸‹è½½æ¨¡å¼")

        # åˆ›å»ºå›¾ç‰‡ç›®å½•
        images_dir = os.path.join(work_dir, "images")
        os.makedirs(images_dir, exist_ok=True)

        # å‡†å¤‡ä¸‹è½½ä»»åŠ¡
        download_tasks = []
        for pin in pins:
            image_url = pin.get('largest_image_url') or pin.get('image_urls', {}).get('original')
            if image_url:
                pin_id = pin.get('id', 'unknown')
                filename = f"{pin_id}.jpg"
                download_tasks.append({
                    'url': image_url,
                    'path': os.path.join(images_dir, filename),
                    'pin_id': pin_id
                })

        if download_tasks:
            # ä½¿ç”¨åŸæœ‰çš„ä¸‹è½½å™¨
            from ..utils import downloader
            success_count = downloader.download_images_batch(download_tasks)
            logger.info(f"åŒæ­¥å›¾ç‰‡ä¸‹è½½: {success_count}/{len(download_tasks)}")

            # æ›´æ–°Pinæ•°æ®ä¸­çš„ä¸‹è½½çŠ¶æ€
            for pin in pins:
                pin_id = pin.get('id')
                if pin_id:
                    image_path = os.path.join(images_dir, f"{pin_id}.jpg")
                    pin['downloaded'] = os.path.exists(image_path)
                    if pin['downloaded']:
                        pin['download_path'] = image_path

    def _log_simple_stats(self, pins: List[Dict]):
        """è¾“å‡ºç®€å•ç»Ÿè®¡ä¿¡æ¯åˆ°æ—¥å¿—"""
        total_pins = len(pins)
        downloaded_images = sum(1 for pin in pins if pin.get('downloaded', False))
        unique_creators = len(set(pin.get('creator', {}).get('name', 'Unknown') for pin in pins))

        logger.debug(f"ç»Ÿè®¡: {total_pins} pins, {downloaded_images} å›¾ç‰‡, {unique_creators} åˆ›ä½œè€…")

    def get_stats(self) -> Dict:
        """è·å–é‡‡é›†å™¨ç»Ÿè®¡ä¿¡æ¯"""
        stats = self.scraper.get_stats()

        # æ·»åŠ ä¸‹è½½ç»Ÿè®¡ä¿¡æ¯
        if hasattr(self.download_manager, 'get_download_stats'):
            download_stats = self.download_manager.get_download_stats()
            stats['download_stats'] = download_stats

        return stats

    async def wait_for_downloads_completion(self, timeout: int = 3600):
        """ç­‰å¾…æ‰€æœ‰ä¸‹è½½ä»»åŠ¡å®Œæˆ

        Args:
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
        """
        if not hasattr(self, 'download_manager') or not self.download_manager:
            logger.warning("ä¸‹è½½ç®¡ç†å™¨æœªåˆå§‹åŒ–")
            return

        try:
            await self.download_manager.wait_for_completion(timeout=timeout)
        except Exception as e:
            logger.error(f"ç­‰å¾…ä¸‹è½½å®Œæˆæ—¶å‡ºé”™: {e}")
            raise

    async def _check_and_resume_session(self, work_name: str, count: int, work_dir: str) -> Optional[str]:
        """æ£€æŸ¥å¹¶æ¢å¤æœªå®Œæˆçš„ä¼šè¯

        Args:
            work_name: å·¥ä½œåç§°
            count: ç›®æ ‡æ•°é‡
            work_dir: å·¥ä½œç›®å½•

        Returns:
            æ¢å¤çš„ä¼šè¯IDï¼Œå¦‚æœæ²¡æœ‰æ¢å¤åˆ™è¿”å›None
        """
        try:
            # æŸ¥è¯¢æœªå®Œæˆçš„ä¼šè¯
            incomplete_sessions = self.repository.get_incomplete_sessions(work_name)

            if not incomplete_sessions:
                return None

            # è·å–æœ€æ–°çš„æœªå®Œæˆä¼šè¯
            latest_session = incomplete_sessions[0]
            cached_pins = self.repository.load_pins_by_query(work_name)
            cached_count = len(cached_pins)

            if cached_count == 0:
                logger.info("ğŸ“ å‘ç°æœªå®Œæˆä¼šè¯ä½†æ— ç¼“å­˜æ•°æ®ï¼Œåˆ›å»ºæ–°ä¼šè¯")
                return None

            # è‡ªåŠ¨ä½¿ç”¨å·²æœ‰æ•°æ®ï¼Œæ— éœ€ç”¨æˆ·ç¡®è®¤
            logger.warning(f"ğŸ”„ å‘ç°æœªå®Œæˆä»»åŠ¡: {work_name}")
            logger.info(f"ğŸ“Š ä¸Šæ¬¡è¿›åº¦: {cached_count}/{latest_session['target_count']} ä¸ªPin (å·²å®Œæˆ {cached_count/latest_session['target_count']*100:.1f}%)")
            logger.info(f"ğŸ“… ä¼šè¯çŠ¶æ€: {latest_session['status']}")
            logger.info(f"ğŸ¯ æœ¬æ¬¡ç›®æ ‡: {count} ä¸ªPin")

            if cached_count >= count:
                logger.info(f"âœ… å·²æœ‰æ•°æ® ({cached_count} ä¸ªPin) æ»¡è¶³æœ¬æ¬¡ç›®æ ‡ ({count} ä¸ªPin)ï¼Œç›´æ¥ä½¿ç”¨ç°æœ‰æ•°æ®")
                remaining_needed = 0
            else:
                remaining_needed = count - cached_count
                logger.info(f"ğŸ“ˆ è‡ªåŠ¨ç»§ç»­é‡‡é›†å‰©ä½™çš„ {remaining_needed} ä¸ªPin")

            # è‡ªåŠ¨æ¢å¤ä¼šè¯ï¼Œæ— éœ€ç”¨æˆ·ç¡®è®¤
            # æ¢å¤ä¼šè¯
            session_id = latest_session['id']
            success = self.repository.resume_session(session_id)

            if success:
                logger.info(f"âœ… æˆåŠŸæ¢å¤ä¼šè¯: {session_id}")
                if remaining_needed > 0:
                    logger.info(f"ğŸš€ å°†ä» {cached_count} ä¸ªPinç»§ç»­é‡‡é›†åˆ° {count} ä¸ªPin (è¿˜éœ€ {remaining_needed} ä¸ª)")
                else:
                    logger.info(f"ğŸ‰ å·²æœ‰æ•°æ®æ»¡è¶³éœ€æ±‚ï¼Œç›´æ¥ä½¿ç”¨ {cached_count} ä¸ªPin")
                return session_id
            else:
                logger.error("âŒ æ¢å¤ä¼šè¯å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°ä¼šè¯")
                return None

        except Exception as e:
            logger.error(f"æ£€æŸ¥ä¼šè¯æ¢å¤æ—¶å‡ºé”™: {e}")
            return None

    async def close(self):
        """å…³é—­çˆ¬è™«ï¼Œæ¸…ç†èµ„æº"""
        try:
            logger.debug("å¼€å§‹æ¸…ç†Pinterestçˆ¬è™«èµ„æº...")

            # åœæ­¢ä¸‹è½½ç®¡ç†å™¨
            if hasattr(self, 'download_manager') and self.download_manager:
                try:
                    await self.download_manager.stop()
                    logger.debug("ä¸‹è½½ç®¡ç†å™¨å·²åœæ­¢")
                except Exception as e:
                    logger.warning(f"åœæ­¢ä¸‹è½½ç®¡ç†å™¨æ—¶å‡ºé”™: {e}")

            # å…³é—­æ™ºèƒ½é‡‡é›†å™¨ä¸­çš„æµè§ˆå™¨èµ„æº
            if hasattr(self, 'scraper') and self.scraper:
                try:
                    await self.scraper.close()
                    logger.debug("æ™ºèƒ½é‡‡é›†å™¨å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­æ™ºèƒ½é‡‡é›†å™¨æ—¶å‡ºé”™: {e}")

            # é‡Šæ”¾è¿›ç¨‹é”
            if hasattr(self, 'process_manager') and self.process_manager:
                try:
                    self.process_manager.release_lock()
                    logger.debug("è¿›ç¨‹é”å·²é‡Šæ”¾")
                except Exception as e:
                    logger.warning(f"é‡Šæ”¾è¿›ç¨‹é”æ—¶å‡ºé”™: {e}")

            # å…³é—­æ•°æ®åº“è¿æ¥
            if hasattr(self, 'repository') and self.repository:
                try:
                    # å¦‚æœrepositoryæœ‰closeæ–¹æ³•ï¼Œè°ƒç”¨å®ƒ
                    if hasattr(self.repository, 'close'):
                        await self.repository.close()
                    logger.debug("æ•°æ®åº“è¿æ¥å·²å…³é—­")
                except Exception as e:
                    logger.warning(f"å…³é—­æ•°æ®åº“è¿æ¥æ—¶å‡ºé”™: {e}")

            logger.debug("Pinterestçˆ¬è™«èµ„æºæ¸…ç†å®Œæˆ")
        except Exception as e:
            logger.error(f"å…³é—­çˆ¬è™«æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def __del__(self):
        """ææ„å‡½æ•°ï¼Œç¡®ä¿èµ„æºæ¸…ç†"""
        # æ³¨æ„ï¼šåœ¨ææ„å‡½æ•°ä¸­ä¸èƒ½ä½¿ç”¨async/await
        # è¿™é‡Œåªæ˜¯è®°å½•è­¦å‘Šï¼Œå®é™…æ¸…ç†åº”è¯¥é€šè¿‡æ˜¾å¼è°ƒç”¨close()æ–¹æ³•
        if hasattr(self, 'download_manager') and self.download_manager and hasattr(self.download_manager, 'started') and self.download_manager.started:
            logger.warning("PinterestScraper æœªæ­£ç¡®å…³é—­ï¼Œå¯èƒ½å¯¼è‡´èµ„æºæ³„æ¼ã€‚è¯·è°ƒç”¨ await scraper.close()")